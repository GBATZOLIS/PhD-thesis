@book{silverman1986density,
  title={Density Estimation for Statistics and Data Analysis},
  author={Silverman, B. W.},
  year={1986},
  publisher={Chapman and Hall}
}

@book{wasserman2006all,
  title={All of Nonparametric Statistics},
  author={Wasserman, L.},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@inproceedings{zhou2022ibot,
  title={iBOT: Image BERT Pre-training with Online Tokenizer},
  author={Zhou, Dingzhou and Wang, Zhaohan and Wang, Luyang and Han, Junwei and Dai, Xiyang and Shen, Yikang and Feng, Jiashi},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=GsI8LxU5Dn}
}

@inproceedings{gidaris2018unsupervised,
  title={Unsupervised representation learning by predicting image rotations},
  author={Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{liu2019roberta,
  title={RoBERTa: A robustly optimized BERT pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{noroozi2016unsupervised,
  title={Unsupervised learning of visual representations by solving jigsaw puzzles},
  author={Noroozi, Mehdi and Favaro, Paolo},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={69--84},
  year={2016},
  organization={Springer}
}

@inproceedings{misra2016shuffle,
  title={Shuffle and learn: unsupervised learning using temporal order verification},
  author={Misra, Ishan and Zitnick, C Lawrence and Hebert, Martial},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={527--544},
  year={2016},
  organization={Springer}
}


@incollection{bickel2007local,
  title={Local polynomial regression on unknown manifolds},
  author={Bickel, P. J. and Li, B.},
  booktitle={Complex Datasets and Inverse Problems},
  pages={177--186},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

@techreport{cayton2005algorithms,
  title={Algorithms for manifold learning},
  author={Cayton, L.},
  institution={University of California at San Diego},
  year={2005},
  number={CS2005-0923}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@article{poggio2017theory,
  title={Why and when can deep--but not shallow--networks avoid the curse of dimensionality: A review},
  author={Poggio, T. and Mhaskar, H. and Rosasco, L. and Miranda, B. and Liao, Q.},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT Press}
}

@book{scott2015multivariate,
  title={Multivariate Density Estimation: Theory, Practice, and Visualization},
  author={Scott, D. W.},
  year={2015},
  publisher={John Wiley \& Sons}
}

@inproceedings{cross-domaindisentanglement,
 author = {Gonzalez-Garcia, Abel and van de Weijer, Joost and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {1287--1298},
 publisher = {Curran Associates, Inc.},
 title = {Image-to-image translation for cross-domain disentanglement},
 url = {https://proceedings.neurips.cc/paper/2018/file/dc6a70712a252123c40d2adba6a11d84-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{HCFLOW,
  title={Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling},
  author={Liang, Jingyun and Lugmayr, Andreas and Zhang, Kai and Danelljan, Martin and Van Gool, Luc and Timofte, Radu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4076--4085},
  year={2021}
}

@inproceedings{dual-glow,
author = {Sun, Haoliang and Mehta, Ronak and Zhou, Hao H. and Huang, Zhichun and Johnson, Sterling C. and Prabhakaran, Vivek and Singh, Vikas},
title = {DUAL-GLOW: Conditional Flow-Based Generative Model for Modality Transfer},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{SRFLOW,
author="Lugmayr, Andreas
and Danelljan, Martin
and Van Gool, Luc
and Timofte, Radu",
title={SRFlow: Learning the Super-Resolution Space with Normalizing Flow},
booktitle="Computer Vision -- ECCV 2020",
year="2020"
}

@inproceedings{ardizzone2019guided,
  title={Guided Image Generation with Conditional Invertible Neural Networks},
  author={Lynton Ardizzone and Jakob Kruse and Carsten Rother and Ullrich K{\"o}the},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={2395--2406},
  year={2019}
}

@misc{BRGM,
  title={Bayesian Image Reconstruction using Deep Generative Models},
  author={Razvan V. Marinescu and Daniel Moyer and Polina Golland},
  year={2021},
  eprint={2012.04567},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@inproceedings{GLOW,
 author = {Kingma, Durk P and Dhariwal, Prafulla},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {10215--10224},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 volume = {31},
 year = {2018}
}

@inproceedings{onken2021ot,
  title={Ot-flow: Fast and accurate continuous normalizing flows via optimal transport},
  author={Onken, Derek and Fung, Samy Wu and Li, Xingjian and Ruthotto, Lars},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={9223--9232},
  year={2021}
}

@inproceedings{Nice2014,
  author    = {Laurent Dinh and David Krueger and Yoshua Bengio},
  editor    = {Yoshua Bengio and Yann LeCun},
  title     = {{NICE:} Non-linear Independent Components Estimation},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015},
  year = 2015
}

@inproceedings{RealNVP2016,
  author    = {Laurent Dinh and Jascha Sohl{-}Dickstein and Samy Bengio},
  title     = {Density estimation using Real {NVP}},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017},
  year = 2017
}

@inproceedings{grover2020alignflow,
  title={Alignflow: Cycle consistent learning from multiple domains via normalizing flows},
  author={Grover, Aditya and Chute, Christopher and Shu, Rui and Cao, Zhangjie and Ermon, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={4028--4035},
  year={2020}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}



@inproceedings{PRRN,
  title = {Pixel Recurrent Neural Networks},
  author = {Oord, Aaron Van and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  pages = {1747--1756},
  year = {2016},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = {48}
}

@inproceedings{NIPS2016_ddeebdee,
 author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 volume = {29},
 year = {2016}
}

@inproceedings{germain2015made,
  title={Made: Masked autoencoder for distribution estimation},
  author={Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle={International Conference on Machine Learning},
  pages={881--889},
  year={2015},
  organization={PMLR}
}

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12873--12883},
  year={2021}
}

@article{batzolis2022non,
  title={Non-uniform diffusion models},
  author={Batzolis, Georgios and Stanczuk, Jan and Sch{\"o}nlieb, Carola-Bibiane and Etmann, Christian},
  journal={arXiv preprint arXiv:2207.09786},
  year={2022}
}

@article{batzolis2024caflow,
  title={CAFLOW: Conditional Autoregressive Flows},
  author={Batzolis, Georgios and Carioni, Marcello and Etmann, Christian and Afyouni, Soroosh and Kourtzi, Zoe and Sch{\"o}nlieb, Carola-Bibiane},
  journal={Foundations of Data Science},
  volume={6},
  number={4},
  pages={553--583},
  year={2024},
  month={December},
  doi={10.3934/fods.2024028},
  abstract={We introduce CAFLOW, a new diverse image-to-image translation model that simultaneously leverages the power of autoregressive modeling and the modeling efficiency of conditional normalizing flows. We transform the conditioning image into a sequence of latent encodings using a multiscale normalizing flow and repeat the process for the conditioned image. We model the conditional distribution of the latent encodings by modeling the autoregressive distributions with an efficient multi-scale normalizing flow, where each conditioning factor affects image synthesis at its respective resolution scale. Our proposed framework performs well on a range of image-to-image translation tasks. It outperforms former designs of conditional flows because of its expressive autoregressive structure.},
  keywords={Generative modeling, conditional normalizing flows, autoregressive modeling, conditional likelihood estimation, image-to-image translation},
  MSC={68T07, 68U10},
  note={The first author is supported by GSK. Early access: June 2024.},
  publisher={AIMS Press}
}


@article{batzolis2023variational,
  title={Variational Diffusion Auto-encoder: Latent Space Extraction from Pre-trained Diffusion Models},
  author={Batzolis, Georgios and Stanczuk, Jan and Sch{\"o}nlieb, Carola-Bibiane},
  journal={arXiv preprint arXiv:2304.12141},
  year={2023}
}

@InProceedings{pmlr-v235-stanczuk24a,
  title = 	 {Diffusion Models Encode the Intrinsic Dimension of Data Manifolds},
  author =       {Stanczuk, Jan Pawel and Batzolis, Georgios and Deveney, Teo and Sch\"{o}nlieb, Carola-Bibiane},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {46412--46440},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/stanczuk24a/stanczuk24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/stanczuk24a.html},
  abstract = 	 {In this work, we provide a mathematical proof that diffusion models encode data manifolds by approximating their normal bundles. Based on this observation we propose a novel method for extracting the intrinsic dimension of the data manifold from a trained diffusion model. Our insights are based on the fact that a diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, at low noise levels, the diffusion model provides us with an approximation of the manifold’s normal bundle, allowing for an estimation of the manifold’s intrinsic dimension. To the best of our knowledge our method is the first estimator of intrinsic dimension based on diffusion models and it outperforms well established estimators in controlled experiments on both Euclidean and image data.}
}


@inproceedings{larochelle2011neural,
  title={The neural autoregressive distribution estimator},
  author={Larochelle, Hugo and Murray, Iain},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={29--37},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4401--4410},
  year={2019}
}

@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of StyleGAN},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={8110--8119},
  year={2020}
}

@article{yu2015lsun,
  title={Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop},
  author={Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  journal={arXiv preprint arXiv:1506.03365},
  year={2015}
}

@inproceedings{viazovetskyi2020stylegan2,
  title={Stylegan2 distillation for feed-forward image manipulation},
  author={Viazovetskyi, Yuri and Ivashkin, Vladimir and Kashin, Evgeny},
  booktitle={European Conference on Computer Vision},
  pages={170--186},
  year={2020},
  organization={Springer}
}

@inproceedings{abdal2019image2stylegan,
  title={Image2stylegan: How to embed images into the stylegan latent space?},
  author={Abdal, Rameen and Qin, Yipeng and Wonka, Peter},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4432--4441},
  year={2019}
}

@article{pan2010domain,
  title={Domain adaptation via transfer component analysis},
  author={Pan, Sinno Jialin and Tsang, Ivor W and Kwok, James T and Yang, Qiang},
  journal={IEEE Transactions on Neural Networks},
  volume={22},
  number={2},
  pages={199--210},
  year={2010},
  publisher={IEEE}
}

@inproceedings{autoregressive_flows,
  title = {Neural Autoregressive Flows},
  author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {2078--2087},
  year = {2018},
  volume = {80}
}


@inproceedings{GANs,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Generative Adversarial Nets},
 volume = {27},
 year = {2014}
}

@inproceedings{batchnormalization,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = {Ioffe, Sergey and Szegedy, Christian},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages = {448--456},
  year = {2015},
  volume = {37}
}

@inproceedings{neuralODEs,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Neural Ordinary Differential Equations},
 volume = {31},
 year = {2018}
}

@inproceedings{HPNAF,
  author={Oh, G. and Valois, J.-S.},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={HCNAF: Hyper-Conditioned Neural Autoregressive Flow and its Application for Probabilistic Occupancy Map Forecasting}, 
  year={2020}
}

@misc{cflows,
title={Learning Likelihoods with Conditional Normalizing Flows},
author={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},
year={2020},
url={https://openreview.net/forum?id=rJg3zxBYwH}
}

@article{cGLOW,
  title={Structured Output Learning with Conditional Generative Flows},
  author={Lu, Y. and Huang, B.},
  journal={AAAI},
  year={2020}
}

@inproceedings{WAVELET-FLOW,
  title={{Wavelet Flow: Fast Training of High Resolution Normalizing Flows}},
  author={Yu, Jason J. and Derpanis, Konstantinos and Brubaker, Marcus A.},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein {GAN}},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},
  year={2017},
  volume={70},
  pages={214--223},
  publisher={PMLR}
}

@inproceedings{rezende2015variational,
  title={Variational inference with normalizing flows},
  author={Rezende, Danilo and Mohamed, Shakir},
  booktitle={International Conference on Machine Learning},
  pages={1530--1538},
  year={2015},
  organization={PMLR}
}

@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}

@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@inproceedings{colorGAN,
  author={Blanch, Marc Górriz and Mrak, Marta and Smeaton, Alan F. and O'Connor, Noel E.},
  booktitle={2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)}, 
  title={End-to-End Conditional GAN-based Architectures for Image Colourisation}, 
  year={2019},
  pages={1--6}
}

@inproceedings{WGAN,
  title = {{W}asserstein Generative Adversarial Networks},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages = {214--223},
  year = {2017},
  volume = {70}
}

@inproceedings{cycada,
  title = {{C}y{CADA}: Cycle-Consistent Adversarial Domain Adaptation},
  author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {1989--1998},
  year = {2018},
  volume = {80}
}

@article{representationsurvey,
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Representation Learning: A Review and New Perspectives}, 
  year={2013},
  volume={35},
  number={8},
  pages={1798--1828}
}

@inproceedings{ESRGAN,
author="Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Loy, Chen Change",
editor="Leal-Taix{\'e}, Laura and Roth, Stefan",
title="ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
booktitle="Computer Vision -- ECCV 2018 Workshops",
year="2019",
pages="63--79"
}

@inproceedings{Pumarola2020,
  author={Pumarola, Albert and Popov, Stefan and Moreno-Noguer, Francesc and Ferrari, Vittorio},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds}, 
  year={2020},
  pages={7946--7955}
}

@inproceedings{Generativecontexualattention,
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Generative Image Inpainting with Contextual Attention}, 
  year={2018},
  pages={5505--5514}
}

@inproceedings{li2019feedback,
  title={Feedback network for image super-resolution},
  author={Li, Zhen and Yang, Jinglei and Liu, Zheng and Yang, Xiaomin and Jeon, Gwanggil and Wu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3867--3876},
  year={2019}
}

@inproceedings{liu2015deep,
  title={Deep learning face attributes in the wild},
  author={Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3730--3738},
  year={2015}
}

@article{scorebased,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@inproceedings{yan2016attribute2image,
  title={Attribute2image: Conditional image generation from visual attributes},
  author={Yan, Xinchen and Yang, Jimei and Sohn, Kihyuk and Lee, Honglak},
  booktitle={European Conference on Computer Vision},
  pages={776--791},
  year={2016},
  organization={Springer}
}

@inproceedings{du2019implicit,
  title={Implicit Generation and Modeling with Energy-Based Models},
  author={Du, Yilun and Mordatch, Igor},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{vanwavenet2016,
  title={WaveNet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}


@inproceedings{VAE,
  author = {Kingma, Diederik P. and Welling, Max},
  editor = {Bengio, Yoshua and LeCun, Yann},
  title = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014},
  year = {2014}
}

@inproceedings{coupled,
  author = {Liu, Ming-Yu and Tuzel, Oncel},
  booktitle = {Advances in Neural Information Processing Systems},
  title = {Coupled Generative Adversarial Networks},
  volume = {29},
  year = {2016}
}

@article{Rozantsevbeyond,
  author={Rozantsev, Artem and Salzmann, Mathieu and Fua, Pascal},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Beyond Sharing Weights for Deep Domain Adaptation}, 
  year={2019},
  volume={41},
  number={4},
  pages={801--814}
}

@inproceedings{Sunfrustratingly,
  author = {Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  title = {Return of Frustratingly Easy Domain Adaptation},
  year = {2016},
  publisher = {AAAI Press},
  booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  pages = {2058--2065}
}

@article{wu2020stochastic,
  title={Stochastic normalizing flows},
  author={Wu, Hao and K{\"o}hler, Jonas and No{\'e}, Frank},
  journal={arXiv preprint arXiv:2002.06707},
  year={2020}
}

@inproceedings{ho2019flow++,
  title={Flow++: Improving flow-based generative models with variational dequantization and architecture design},
  author={Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={2722--2730},
  year={2019},
  organization={PMLR}
}

@inproceedings{verine2023expressivity,
  title={On the expressivity of bi-Lipschitz normalizing flows},
  author={Verine, Alexandre and Negrevergne, Benjamin and Chevaleyre, Yann and Rossi, Fabrice},
  booktitle={Asian Conference on Machine Learning},
  pages={1054--1069},
  year={2023},
  organization={PMLR}
}

@inproceedings{behrmann2021understanding,
  title={Understanding and mitigating exploding inverses in invertible neural networks},
  author={Behrmann, Jens and Vicol, Paul and Wang, Kuan-Chieh and Grosse, Roger and Jacobsen, J{\"o}rn-Henrik},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1792--1800},
  year={2021},
  organization={PMLR}
}

% General
@book{oksendal2003sde,
      author = {Oksendal, Bernt},
      title = {Stochastic Differential Equations (5th Ed.): An Introduction with Applications},
      year = {2003},
      publisher = {Springer-Verlag},
      address = {Heidelberg}
}

@misc{leonard2013properties,
      title={Some properties of path measures}, 
      author={Christian Léonard},
      year={2013},
      eprint={1308.0217},
      archivePrefix={arXiv},
      primaryClass={math.PR}
}

@inproceedings{tian2024visual,
  title={Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  author={Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://arxiv.org/abs/2404.02905}
}


@misc{daras2021intermediate,
      title={Intermediate Layer Optimization for Inverse Problems using Deep Generative Models}, 
      author={Giannis Daras and Joseph Dean and Ajil Jalal and Alexandros G. Dimakis},
      year={2021},
      eprint={2102.07364},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{marinescu2021bayesian_reconstruction_gans,
      title={Bayesian Image Reconstruction using Deep Generative Models}, 
      author={Razvan V Marinescu and Daniel Moyer and Polina Golland},
      year={2021},
      eprint={2012.04567},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gabbay2019style__inversion_image_enhancement,
      title={Style Generator Inversion for Image Enhancement and Animation}, 
      author={Aviv Gabbay and Yedid Hoshen},
      year={2019},
      eprint={1906.11880},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wei2021gan_inversion,
      title={A Simple Baseline for StyleGAN Inversion}, 
      author={Tianyi Wei and Dongdong Chen and Wenbo Zhou and Jing Liao and Weiming Zhang and Lu Yuan and Gang Hua and Nenghai Yu},
      year={2021},
      eprint={2104.07661},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{vincent2011connection,
  author={Vincent, Pascal},
  journal={Neural Computation}, 
  title={A Connection Between Score Matching and Denoising Autoencoders}, 
  year={2011},
  volume={23},
  number={7},
  pages={1661-1674},
  doi={10.1162/NECO_a_00142}}

@misc{wang2020survey,
      title={Deep Learning for Image Super-resolution: A Survey}, 
      author={Zhihao Wang and Jian Chen and Steven C. H. Hoi},
      year={2020},
      eprint={1902.06068},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{daubechies1992wavelets,
author = {Daubechies, Ingrid},
title = {Ten Lectures on Wavelets},
year = {1992},
isbn = {0898712742},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA}
}

@misc{kingma2014autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2014},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{liu2023i2sb,
      title={I$^2$SB: Image-to-Image Schr\"odinger Bridge}, 
      author={Guan-Horng Liu and Arash Vahdat and De-An Huang and Evangelos A. Theodorou and Weili Nie and Anima Anandkumar},
      year={2023},
      eprint={2302.05872},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{donahue2019adversarial_audio,
      title={Adversarial Audio Synthesis}, 
      author={Chris Donahue and Julian McAuley and Miller Puckette},
      year={2019},
      eprint={1802.04208},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{zhu2020cycle_gan,
      title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}, 
      author={Jun-Yan Zhu and Taesung Park and Phillip Isola and Alexei A. Efros},
      year={2020},
      eprint={1703.10593},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{tang2021attentiongan,
      title={AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks}, 
      author={Hao Tang and Hong Liu and Dan Xu and Philip H. S. Torr and Nicu Sebe},
      year={2021},
      eprint={1911.11897},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{grathwohl2020your,
  title={Your classifier is secretly an energy-based model and you should treat it like one},
  author={Grathwohl, Will and Wang, Kevin C. and Jacobsen, J{\"o}rn-Henrik and Duvenaud, David and Swersky, Kevin and Norouzi, Mohammad},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{bengio2005autoregressive,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}

@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{dinh2016density,
  title={Density estimation using Real NVP},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{kobyzev2020normalizing,
  title={Normalizing Flows: An Introduction and Review of Current Methods},
  author={Kobyzev, Ivan and Prince, Simon JD and Brubaker, Marcus A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={43},
  number={11},
  pages={3964--3979},
  year={2021}
}


@article{papamakarios2019normalizing,
  title={Normalizing flows for probabilistic modeling and inference},
  author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={57},
  pages={1--64},
  year={2021}
}

@misc{papamakarios2021normalizing,
      title={Normalizing Flows for Probabilistic Modeling and Inference}, 
      author={George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
      year={2021},
      eprint={1912.02762},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

% Data

@inproceedings{2015celeba,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}

@InProceedings{yu2014sketch2shoe,
  author = {A. Yu and K. Grauman},
  title = {Fine-Grained Visual Comparisons with Local Learning},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  month = {Jun},
  year = {2014}
}

@misc{isola2018pix2pix,
      title={Image-to-Image Translation with Conditional Adversarial Networks}, 
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{xie2015edges,
      title={Holistically-Nested Edge Detection}, 
      author={Saining Xie and Zhuowen Tu},
      year={2015},
      eprint={1504.06375},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{2017div2k,
	author = {Agustsson, Eirikur and Timofte, Radu},
	title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	month = {July},
	year = {2017}
} 

% Metrics
@inproceedings{zhang2018lpips,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={CVPR},
  year={2018}
}

@misc{heusel2018fid,
      title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}, 
      author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
      year={2018},
      eprint={1706.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@ARTICLE{zhou2004psnr+ssim,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  doi={10.1109/TIP.2003.819861}}

%Score 

@article{hyvarinen2005score_original,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695-709},
  url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
}

@misc{song2020generative_score,
      title={Generative Modeling by Estimating Gradients of the Data Distribution}, 
      author={Yang Song and Stefano Ermon},
      year={2020},
      eprint={1907.05600},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tashiro2021csdi,
      title={CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation}, 
      author={Yusuke Tashiro and Jiaming Song and Yang Song and Stefano Ermon},
      year={2021},
      eprint={2107.03502},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Diffusion

@misc{sohldickstein2015diffusion_original,
      title={Deep Unsupervised Learning using Nonequilibrium Thermodynamics}, 
      author={Jascha Sohl-Dickstein and Eric A. Weiss and Niru Maheswaranathan and Surya Ganguli},
      year={2015},
      eprint={1503.03585},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{ho2020denoising,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dhariwal2021diffusion_beats_gans,
      title={Diffusion Models Beat GANs on Image Synthesis}, 
      author={Prafulla Dhariwal and Alex Nichol},
      year={2021},
      eprint={2105.05233},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% SDE

@article{song2023consistency,
  title={Consistency models},
  author={Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2303.01469},
  year={2023}
}

@article{salimans2022progressive,
  title={Progressive distillation for fast sampling of diffusion models},
  author={Salimans, Tim and Ho, Jonathan},
  journal={arXiv preprint arXiv:2202.00512},
  year={2022}
}

@article{song2020denoising,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@misc{song2021sde,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{song2021maximum,
      title={Maximum Likelihood Training of Score-Based Diffusion Models}, 
      author={Yang Song and Conor Durkan and Iain Murray and Stefano Ermon},
      year={2021},
      eprint={2101.09258},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{anderson1982reverse_time_sde,
title = {Reverse-time diffusion equation models},
journal = {Stochastic Processes and their Applications},
volume = {12},
number = {3},
pages = {313-326},
year = {1982},
issn = {0304-4149},
doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
author = {Brian D.O. Anderson}
}

% Super resolution

@misc{saharia2021sr3,
      title={Image Super-Resolution via Iterative Refinement}, 
      author={Chitwan Saharia and Jonathan Ho and William Chan and Tim Salimans and David J. Fleet and Mohammad Norouzi},
      year={2021},
      eprint={2104.07636},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@misc{liang2021hrflow,
      title={Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling}, 
      author={Jingyun Liang and Andreas Lugmayr and Kai Zhang and Martin Danelljan and Luc Van Gool and Radu Timofte},
      year={2021},
      eprint={2108.05301},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@ARTICLE{keyes1981bicubic,
  author={Keys, R.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Cubic convolution interpolation for digital image processing}, 
  year={1981},
  volume={29},
  number={6},
  pages={1153-1160},
  doi={10.1109/TASSP.1981.1163711}}

%IP 
@article{arridge2019ip, 
         title={Solving inverse problems using data-driven models}, 
         volume={28}, 
         DOI={10.1017/S0962492919000059}, 
         journal={Acta Numerica}, 
         publisher={Cambridge University Press}, 
         author={Arridge, Simon and Maass, Peter and Öktem, Ozan and Schönlieb, Carola-Bibiane}, 
         year={2019}, 
         pages={1–174}}

@inproceedings{muller2012ip,
  title={Linear and Nonlinear Inverse Problems with Practical Applications},
  author={Jennifer L. Mueller and Samuli Siltanen},
  booktitle={Computational science and engineering},
  year={2012}
}

% Stats
@incollection{whitney1994estimation,
  title = {Chapter 36 Large sample estimation and hypothesis testing},
  series = {Handbook of Econometrics},
  publisher = {Elsevier},
  volume = {4},
  pages = {2111-2245},
  year = {1994},
  issn = {1573-4412},
  doi = {https://doi.org/10.1016/S1573-4412(05)80005-4},
  url = {https://www.sciencedirect.com/science/article/pii/S1573441205800054},
  author = {Whitney K. Newey and Daniel McFadden}
}


@misc{rombach2022stable_diffusion,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2022},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{pettis_nn_dim_estimator,
  author={Pettis, Karl W. and Bailey, Thomas A. and Jain, Anil K. and Dubes, Richard C.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={An Intrinsic Dimensionality Estimator from Near-Neighbor Information}, 
  year={1979},
  volume={PAMI-1},
  number={1},
  pages={25-37},
  doi={10.1109/TPAMI.1979.4766873}}


@misc{kong2020diffWave,
  doi = {10.48550/ARXIV.2009.09761},
  
  url = {https://arxiv.org/abs/2009.09761},
  
  author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  
  keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), Machine Learning (stat.ML), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DiffWave: A Versatile Diffusion Model for Audio Synthesis},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@misc{manifold_hypothesis,
  doi = {10.48550/ARXIV.1310.0425},
  url = {https://arxiv.org/abs/1310.0425},
  author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  title = {Testing the Manifold Hypothesis},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{pca,
  author       = {Pearson, Karl},
  title        = {{LIII. On lines and planes of closest fit to 
                   systems of points in space}},
  month        = nov,
  year         = 1901,
  publisher    = {Zenodo},
  doi          = {10.1080/14786440109462720},
  url          = {https://doi.org/10.1080/14786440109462720}
}

@article{ppca,
  added-at = {2017-11-28T07:11:13.000+0100},
  author = {Bishop, C M and Tipping, M E},
  biburl = {https://www.bibsonomy.org/bibtex/25bcaea0a135fee41b805037c9b434388/lenalauber},
  description = {Bishop_Tipping_1999_Probabilistic_PCA.pdf},
  interhash = {2afbf998e9e208a2f74687e586e1e841},
  intrahash = {5bcaea0a135fee41b805037c9b434388},
  keywords = {final thema:representationlearning},
  note = {PPCA},
  timestamp = {2017-11-28T07:11:13.000+0100},
  title = {Probabilistic Principal Component Analysis},
  year = 2001
}

@misc{fan_local_pca,
  doi = {10.48550/ARXIV.1002.2050},
  url = {https://arxiv.org/abs/1002.2050},
  author = {Fan, Mingyu and Gu, Nannan and Qiao, Hong and Zhang, Bo},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Intrinsic dimension estimation of data by principal component analysis},
  publisher = {arXiv},
  year = {2010},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{haro_mle,
  author    = {Gloria Haro and
               Gregory Randall and
               Guillermo Sapiro},
  title     = {Translated Poisson Mixture Model for Stratification Learning},
  journal   = {Int. J. Comput. Vis.},
  volume    = {80},
  number    = {3},
  pages     = {358--374},
  year      = {2008}
}


@article{lle,
  added-at = {2012-07-13T11:59:39.000+0200},
  author = {Roweis, Sam T. and Saul, Lawrence K.},
  biburl = {https://www.bibsonomy.org/bibtex/2cccdcfb847911c5899f3f35f9f1b5de5/jabreftest},
  doi = {10.1126/science.290.5500.2323},
  eprint = {http://www.sciencemag.org/cgi/reprint/290/5500/2323.pdf},
  file = {Roweis2000.pdf:2000/Roweis2000.pdf:PDF},
  groups = {public},
  interhash = {e4ae1bddd3d395677778454060783c55},
  intrahash = {cccdcfb847911c5899f3f35f9f1b5de5},
  journal = {Science},
  keywords = {},
  number = 5500,
  pages = {2323-2326},
  timestamp = {2012-07-13T11:59:39.000+0200},
  title = {{Nonlinear Dimensionality Reduction by Locally Linear Embedding}},
  url = {http://www.sciencemag.org/cgi/content/abstract/290/5500/2323},
  username = {jabreftest},
  volume = 290,
  year = 2000
}


@article{
IsoMap,
author = {Joshua B. Tenenbaum  and Vin de Silva  and John C. Langford },
title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
journal = {Science},
volume = {290},
number = {5500},
pages = {2319-2323},
year = {2000},
doi = {10.1126/science.290.5500.2319},
URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
}



@inproceedings{tsne,
 author = {Hinton, Geoffrey E and Roweis, Sam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Becker and S. Thrun and K. Obermayer},
 pages = {},
 publisher = {MIT Press},
 title = {Stochastic Neighbor Embedding},
 url = {https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
 volume = {15},
 year = {2002}
}




@misc{gan,
  doi = {10.48550/ARXIV.1406.2661},
  
  url = {https://arxiv.org/abs/1406.2661},
  
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Generative Adversarial Networks},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{diffusion_models,
  doi = {10.48550/ARXIV.1503.03585},
  
  url = {https://arxiv.org/abs/1503.03585},
  
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  
  keywords = {Machine Learning (cs.LG), Disordered Systems and Neural Networks (cond-mat.dis-nn), Neurons and Cognition (q-bio.NC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Biological sciences, FOS: Biological sciences},
  
  title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ddpm,
  doi = {10.48550/ARXIV.2006.11239},
  
  url = {https://arxiv.org/abs/2006.11239},
  
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Denoising Diffusion Probabilistic Models},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{score_matching,
  author  = {Aapo Hyv{{\"a}}rinen},
  title   = {Estimation of Non-Normalized Statistical Models by Score Matching},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {24},
  pages   = {695--709},
  url     = {http://jmlr.org/papers/v6/hyvarinen05a.html}
}

% --------------------RELATED WORK----------------
@ARTICLE{Karhunen-Loeve,
  author={Fukunaga, K. and Olsen, D.R.},
  journal={IEEE Transactions on Computers}, 
  title={An Algorithm for Finding Intrinsic Dimensionality of Data}, 
  year={1971},
  volume={C-20},
  number={2},
  pages={176-183},
  doi={10.1109/T-C.1971.223208}}

@inproceedings{auto_ppca,
 author = {Minka, Thomas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Automatic Choice of Dimensionality for PCA},
 url = {https://proceedings.neurips.cc/paper/2000/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf},
 volume = {13},
 year = {2000}
}

@inproceedings{packing_number,
 author = {K\'{e}gl, Bal\'{a}zs},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Becker and S. Thrun and K. Obermayer},
 pages = {},
 publisher = {MIT Press},
 title = {Intrinsic Dimension Estimation Using Packing Numbers},
 url = {https://proceedings.neurips.cc/paper/2002/file/1177967c7957072da3dc1db4ceb30e7a-Paper.pdf},
 volume = {15},
 year = {2002}
}

@ARTICLE{fractal_dim,
  author={Camastra, F. and Vinciarelli, A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Estimating the intrinsic dimension of data with a fractal-based method}, 
  year={2002},
  volume={24},
  number={10},
  pages={1404-1407},
  doi={10.1109/TPAMI.2002.1039212}}

@inproceedings{dim_MLE,
 author = {Levina, Elizaveta and Bickel, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Maximum Likelihood Estimation of Intrinsic Dimension},
 url = {https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf},
 volume = {17},
 year = {2004}
}

@misc{R_intrinsic_dim,
title = {intrinsicDimension: Intrinsic Dimension Estimation},
author = {Kerstin Johnsson},
year = {2016},
url = {https://cran.r-project.org/web/packages/intrinsicDimension/},
}

@article{sklearn,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

%--------------------------------------------------
%----------------Datasets----------------
@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@article{cifar,
author = {Krizhevsky, Alex},
year = {2012},
month = {05},
pages = {},
title = {Learning Multiple Layers of Features from Tiny Images},
journal = {University of Toronto}
}

@INPROCEEDINGS{imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}}

  @article{mammoth,
  author  = {Yingfan Wang and Haiyang Huang and Cynthia Rudin and Yaron Shaposhnik},
  title   = {Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {201},
  pages   = {1--73},
  url     = {http://jmlr.org/papers/v22/20-1061.html}
}


@article{koehler2022statistical,
  title={Statistical Efficiency of Score Matching: The View from Isoperimetry},
  author={Koehler, Frederic and Heckett, Alexander and Risteski, Andrej},
  journal={arXiv preprint arXiv:2210.00726},
  year={2022}
}


@article{pope2021intrinsic,
  title={The intrinsic dimension of images and its impact on learning},
  author={Pope, Phillip and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
  journal={arXiv preprint arXiv:2104.08894},
  year={2021}
}

@inproceedings{karras2019stylegan,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}

@inproceedings{karras2020stylegan2,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}

@inproceedings{arjovsky2017principled_gans,
  author       = {Mart{\'{\i}}n Arjovsky and
                  L{\'{e}}on Bottou},
  title        = {Towards Principled Methods for Training Generative Adversarial Networks},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=Hk4\_qw5xe},
  timestamp    = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ArjovskyB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{kingmaVDM,
  doi = {10.48550/ARXIV.2107.00630},
  
  url = {https://arxiv.org/abs/2107.00630},
  
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Variational Diffusion Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@book{lee2019_riemman,
  title={Introduction to Riemannian Manifolds},
  author={Lee, J.M.},
  isbn={9783319917542},
  series={Graduate Texts in Mathematics},
  url={https://books.google.co.uk/books?id=UIPltQEACAAJ},
  year={2019},
  publisher={Springer International Publishing}
}

@book{lee2003_smooth,
  title={Introduction to Smooth Manifolds},
  author={Lee, J.M.},
  isbn={9780387954486},
  lccn={2002070454},
  series={Graduate Texts in Mathematics},
  url={https://books.google.co.uk/books?id=eqfgZtjQceYC},
  year={2003},
  publisher={Springer}
}

@book{palais1988critical,
  title={Critical Point Theory and Submanifold Geometry},
  author={Palais, Richard S. and Terng, C.},
  isbn={9783540503996},
  lccn={88032694},
  series={Critical Point Theory and Submanifold Geometry},
  url={https://books.google.co.uk/books?id=ViSHzQEACAAJ},
  year={1988},
  publisher={Springer}
}

@book{nicolaescu2011morse_theory,
  title={An Invitation to Morse Theory},
  author={Nicolaescu, L.},
  isbn={9781461411055},
  series={Universitext},
  url={https://books.google.co.uk/books?id=nCgvt2MY4QAC},
  year={2011},
  publisher={Springer New York}
}

@misc{pidstrigach2022manifold_jakiw,
      title={Score-Based Generative Models Detect Manifolds}, 
      author={Jakiw Pidstrigach},
      year={2022},
      eprint={2206.01018},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{chen2023manifold_linear,
      title={Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data}, 
      author={Minshuo Chen and Kaixuan Huang and Tuo Zhao and Mengdi Wang},
      year={2023},
      eprint={2302.07194},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oko2023diffusion_mini_max,
      title={Diffusion Models are Minimax Optimal Distribution Estimators}, 
      author={Kazusato Oko and Shunta Akiyama and Taiji Suzuki},
      year={2023},
      eprint={2303.01861},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{tempczyk2022lidl,
      title={LIDL: Local Intrinsic Dimension Estimation Using Approximate Likelihood}, 
      author={Piotr Tempczyk and Rafał Michaluk and Łukasz Garncarek and Przemysław Spurek and Jacek Tabor and Adam Goliński},
      year={2022},
      eprint={2206.14882},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{brehmer2020flows,
  title={Flows for simultaneous manifold learning and density estimation},
  author={Brehmer, Johann and Cranmer, Kyle},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={442--453},
  year={2020}
}

@inproceedings{horvat2022nfid,
 author = {Horvat, Christian and Pfister, Jean-Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {12225--12236},
 publisher = {Curran Associates, Inc.},
 title = {Intrinsic dimensionality estimation using Normalizing Flows},
 volume = {35},
 year = {2022}
}


@article{campadelli2015intrinsic,
  title={Intrinsic dimension estimation: Relevant techniques and a benchmark framework},
  author={Campadelli, Paola and Casiraghi, Elena and Ceruti, Claudio and Rozza, Alessandro},
  journal={Mathematical Problems in Engineering},
  volume={2015},
  pages={1--21},
  year={2015},
  publisher={Hindawi Limited}
}

@article{kramer1991nonlinear,
  title={Nonlinear principal component analysis using autoassociative neural networks},
  author={Kramer, Mark A},
  journal={AIChE journal},
  volume={37},
  number={2},
  pages={233--243},
  year={1991},
  publisher={Wiley Online Library}
}

@article{goyal2022inductive,
  title={Inductive biases for deep learning of higher-level cognition},
  author={Goyal, Anirudh and Bengio, Yoshua},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2266},
  pages={20210068},
  year={2022},
  publisher={The Royal Society}
}

@article{weed2019sharp,
  title={Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance},
  author={Weed, Jonathan and Bach, Francis},
  year={2019}
}


@article{kpotufe2011knn,
  title={k-NN regression adapts to local intrinsic dimension},
  author={Kpotufe, Samory},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{kim2019kde,
  title={Uniform convergence rate of the kernel density estimator adaptive to intrinsic volume dimension},
  author={Kim, Jisu and Shin, Jaehyeok and Rinaldo, Alessandro and Wasserman, Larry},
  booktitle={International Conference on Machine Learning},
  pages={3398--3407},
  year={2019},
  organization={PMLR}
}

@inproceedings{behrmann2021understandin,
  title={Understanding and mitigating exploding inverses in invertible neural networks},
  author={Behrmann, Jens and Vicol, Paul and Wang, Kuan-Chieh and Grosse, Roger and Jacobsen, J{\"o}rn-Henrik},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1792--1800},
  year={2021},
  organization={PMLR}
}

@inproceedings{jaini2020tails,
  title={Tails of Lipschitz triangular flows},
  author={Jaini, Priyank and Kobyzev, Ivan and Yu, Yaoliang and Brubaker, Marcus},
  booktitle={International Conference on Machine Learning},
  pages={4673--4681},
  year={2020},
  organization={PMLR}
}

@inproceedings{cornish2020relaxing,
  title={Relaxing bijectivity constraints with continuously indexed normalising flows},
  author={Cornish, Rob and Caterini, Anthony and Deligiannidis, George and Doucet, Arnaud},
  booktitle={International conference on machine learning},
  pages={2133--2143},
  year={2020},
  organization={PMLR}
}

@article{laszkiewicz2021copula,
  title={Copula-based normalizing flows},
  author={Laszkiewicz, Mike and Lederer, Johannes and Fischer, Asja},
  journal={arXiv preprint arXiv:2107.07352},
  year={2021}
}

@misc{batzolis2022non_uniform,
  doi = {10.48550/ARXIV.2207.09786},
  url = {https://arxiv.org/abs/2207.09786},
  author = {Batzolis, Georgios and Stanczuk, Jan and Schönlieb, Carola-Bibiane and Etmann, Christian},
  title = {Non-Uniform Diffusion Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{stanczuk2021wasserstein,
  doi = {10.48550/ARXIV.2103.01678},
  url = {https://arxiv.org/abs/2103.01678},
  author = {Stanczuk, Jan and Etmann, Christian and Kreusser, Lisa Maria and Schönlieb, Carola-Bibiane},
  title = {Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{zaho2017understanding_vaes,
  doi = {10.48550/ARXIV.1702.08658},
  url = {https://arxiv.org/abs/1702.08658},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  title = {Towards Deeper Understanding of Variational Autoencoding Models},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{draxler2024free,
  title={Free-form flows: Make any architecture a normalizing flow},
  author={Draxler, Felix and Sorrenson, Peter and Zimmermann, Lea and Rousselot, Armand and K{\"o}the, Ullrich},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2197--2205},
  year={2024},
  organization={PMLR}
}

@misc{preechakul2022diffusion_decoder,
  title={Diffusion Autoencoders: Toward a Meaningful and Decodable Representation},
  author={Konpat Preechakul and Nattanat Chatthee and Suttisak Wizadwongsa and Supasorn Suwajanakorn},
  year={2022},
  eprint={2111.15640},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@misc{yang2023ldiffusion_decoder_compression,
  title={Lossy Image Compression with Conditional Diffusion Models},
  author={Ruihan Yang and Stephan Mandt},
  year={2023},
  eprint={2209.06950},
  archivePrefix={arXiv},
  primaryClass={eess.IV}
}

@article{koehler2022statistical,
  title={Statistical Efficiency of Score Matching: The View from Isoperimetry},
  author={Koehler, Frederic and Heckett, Alexander and Risteski, Andrej},
  journal={arXiv preprint arXiv:2210.00726},
  year={2022}
}

@misc{pca,
  author = {Pearson, Karl},
  title = {{LIII. On lines and planes of closest fit to systems of points in space}},
  year = {1901},
  publisher = {Zenodo},
  doi = {10.1080/14786440109462720},
  url = {https://doi.org/10.1080/14786440109462720}
}

@inproceedings{liu2015celeba,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015}
}

@inproceedings{higgins2016beta_vae,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Irina Higgins and Lo{\"i}c Matthey and Arka Pal and Christopher P. Burgess and Xavier Glorot and Matthew M. Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@misc{rybkin2021sigma_vae,
  title={Simple and Effective VAE Training with Calibrated Decoders},
  author={Oleh Rybkin and Kostas Daniilidis and Sergey Levine},
  year={2021},
  eprint={2006.13202},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{rezende2014vae2,
  title={Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
  author={Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
  year={2014},
  eprint={1401.4082},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

%references for Score-Based pullback Riemannian Geometry

@inproceedings{dinh2017density,
  title={Density estimation using Real {NVP}},
  author={Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=HkpbnH9lx}
}

@article{huang2022riemannian,
  title={Riemannian diffusion models},
  author={Huang, Chin-Wei and Aghajohari, Milad and Bose, Joey and Panangaden, Prakash and Courville, Aaron C},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2750--2761},
  year={2022}
}

@misc{kapusniak2024metricflowmatchingsmooth,
      title={Metric Flow Matching for Smooth Interpolations on the Data Manifold}, 
      author={Kacper Kapusniak and Peter Potaptchik and Teodora Reu and Leo Zhang and Alexander Tong and Michael Bronstein and Avishek Joey Bose and Francesco Di Giovanni},
      year={2024},
      eprint={2405.14780},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14780}, 
}
@misc{sorrenson2024learningdistancesdatanormalizing,
      title={Learning Distances from Data with Normalizing Flows and Score Matching}, 
      author={Peter Sorrenson and Daniel Behrend-Uriarte and Christoph Schnörr and Ullrich Köthe},
      year={2024},
      eprint={2407.09297},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.09297}, 
}
@misc{nesterov2022learning,
      title={Learning Invariances with Generalised Input-Convex Neural Networks}, 
      author={Vitali Nesterov and Fabricio Arend Torres and Monika Nagy-Huber and Maxim Samarin and Volker Roth},
      year={2022},
      eprint={2204.07009},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{INRZ19jla,
title={Modewise Operators, the Tensor Restricted Isometry Property, and Low-Rank Tensor Recovery},
author={M. A. Iwen and D. Needell and M. Perlmutter and E. Rebrova},
note={Submitted},
year={2021}}

@article{tensorCUR21,
title={Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions},
author={H. Cai and K. Hamm and L. Huang and D. Needell},
journal={Journal of Machine Learning Research},
volume={22},
number={185},
pages={1--36},
year={2021}}

@article{INRZ19jl,
title={Lower Memory Oblivious (Tensor) Subspace Embeddings with Fewer Random Bits: Modewise Methods for Least Squares},
author={M. A. Iwen and D. Needell and E. Rebrova and and A. Zare},
journal={SIAM Journal on Matrix Analysis and Applications},
volume={42},
number={1},
pages={376-416},
year={2020}}

@article{huang2014robust,
  title={Robust manifold nonnegative matrix factorization},
  author={Huang, Jin and Nie, Feiping and Huang, Heng and Ding, Chris},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={8},
  number={3},
  pages={1--21},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@incollection{holler2020non,
  title={Non-smooth variational regularization for processing manifold-valued data},
  author={Holler, Martin and Weinmann, Andreas},
  booktitle={Handbook of Variational Methods for Nonlinear Geometric Data},
  pages={51--93},
  year={2020},
  publisher={Springer}
}

@article{tan2023data,
  title={Data-driven mirror descent with input-convex neural networks},
  author={Tan, Hong Ye and Mukherjee, Subhadip and Tang, Junqi and Sch{\"o}nlieb, Carola-Bibiane},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={2},
  pages={558--587},
  year={2023},
  publisher={SIAM}
}

@article{leong2022optimal,
  title={Optimal regularization for a data source},
  author={Leong, Oscar and O'Reilly, Eliza and Soh, Yong Sheng and Chandrasekaran, Venkat},
  journal={arXiv preprint arXiv:2212.13597},
  year={2022}
}

@article{diepeveen2024pulling,
  title={Pulling back symmetric Riemannian geometry for data analysis},
  author={Diepeveen, Willem},
  journal={arXiv preprint arXiv:2403.06612},
  year={2024}
}

@article{wang2022accelerated,
  title={Accelerated information gradient flow},
  author={Wang, Yifei and Li, Wuchen},
  journal={Journal of Scientific Computing},
  volume={90},
  pages={1--47},
  year={2022},
  publisher={Springer}
}

@article{mukherjee2020learned,
  title={Learned convex regularizers for inverse problems},
  author={Mukherjee, Subhadip and Dittmer, S{\"o}ren and Shumaylov, Zakhar and Lunz, Sebastian and {\"O}ktem, Ozan and Sch{\"o}nlieb, Carola-Bibiane},
  journal={arXiv preprint arXiv:2008.02839},
  year={2020}
}

@article{stanczuk2022your,
  title={Your diffusion model secretly knows the dimension of the data manifold},
  author={Stanczuk, Jan and Batzolis, Georgios and Deveney, Teo and Sch{\"o}nlieb, Carola-Bibiane},
  journal={arXiv preprint arXiv:2212.12611},
  year={2022}
}

@article{shumaylov2024weakly,
  title={Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation},
  author={Shumaylov, Zakhar and Budd, Jeremy and Mukherjee, Subhadip and Sch{\"o}nlieb, Carola-Bibiane},
  journal={arXiv preprint arXiv:2402.01052},
  year={2024}
}

@article{herzog2023manifold,
  title={A manifold of planar triangular meshes with complete Riemannian metric},
  author={Herzog, Roland and Loayza-Romero, Estefan{\'\i}a},
  journal={Mathematics of Computation},
  volume={92},
  number={339},
  pages={1--50},
  year={2023}
}

@article{durkan2019neural,
  title={Neural spline flows},
  author={Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yang2018geodesic,
  title={Geodesic clustering in deep generative models},
  author={Yang, Tao and Arvanitidis, Georgios and Fu, Dongmei and Li, Xiaogang and Hauberg, S{\o}ren},
  journal={arXiv preprint arXiv:1809.04747},
  year={2018}
}

@inproceedings{izmailov2020semi,
  title={Semi-supervised learning with normalizing flows},
  author={Izmailov, Pavel and Kirichenko, Polina and Finzi, Marc and Wilson, Andrew Gordon},
  booktitle={International conference on machine learning},
  pages={4615--4630},
  year={2020},
  organization={PMLR}
}

@Article{salman2018deep,
  author     = {Salman, Hadi and Yadollahpour, Payman and Fletcher, Tom and Batmanghelich, Kayhan},
  journal    = {arXiv preprint arXiv:1810.03256},
  title      = {Deep diffeomorphic normalizing flows},
  year       = {2018},
  comment    = {Contribution:
- "We introduce a new type of NF, called Deep Diffeomorphic Normalizing Flow (DDNF)."

Comments:
- scalability might also be an issue for me
- Deep learning for the flow field

Related to my work:
- Cryo-EM:
	- Background deep learning for learning difformations},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/68 - Salman - Deep Diffeomorphic Normalizing Flows.pdf:PDF},
  groups     = {Cryo-EM, Has No Tree File, Spatiotemporal SPA, SPA Background papers, Riemannian manifold learning with extras, General interest, Ideas I-xyz, RML Research},
  id         = {68},
  readstatus = {skimmed},
}

@Book{sakai1996riemannian,
  author     = {Sakai, Takashi},
  publisher  = {American Mathematical Soc.},
  title      = {Riemannian geometry},
  year       = {1996},
  volume     = {149},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/315 - Sakai - Riemannian Geometry.pdf:PDF},
  groups     = {Cryo-EM, RML Motivation Riemannian geometry},
  id         = {315},
  readstatus = {skimmed},
}

@Book{carmo1992riemannian,
  author     = {Carmo, Manfredo Perdigao do},
  publisher  = {Birkh{\"a}user},
  title      = {Riemannian geometry},
  year       = {1992},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/316 - Do Carmo - Riemannian Geometry.pdf:PDF},
  groups     = {Cryo-EM, Riemannian geometry for efficient protein conformation analysis, RML Motivation Riemannian geometry},
  id         = {316},
  readstatus = {skimmed},
}

@Article{diepeveen2021inexact,
  author     = {Diepeveen, Willem and Lellmann, Jan},
  journal    = {SIAM Journal on Imaging Sciences},
  title      = {An Inexact Semismooth Newton Method on Riemannian Manifolds with Application to Duality-Based Total Variation Denoising},
  year       = {2021},
  number     = {4},
  pages      = {1565--1600},
  volume     = {14},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/361 - Diepeveen - An Inexact Semismooth Newton Method on Riemannian Manifolds with Application to Duality-Based Total Variation Denoising.pdf:PDF},
  groups     = {Manifold-valued Bayesian IPs, In Article, Lifting for SPA, Own work, RML Motivation symmetric spaces},
  id         = {361},
  publisher  = {SIAM},
  readstatus = {skimmed},
}

@Article{chan2001total,
  author         = {Chan, Tony F and Kang, Sung Ha and Shen, Jianhong},
  journal        = {Journal of Visual Communication and Image Representation},
  title          = {Total variation denoising and enhancement of color images based on the CB and HSV color models},
  year           = {2001},
  number         = {4},
  pages          = {422--435},
  volume         = {12},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/385 - Chan - Total variation denoising and enhancement of color images based on the CB and HSV color models.pdf:PDF},
  groups         = {Manifold-valued Bayesian IPs, Manifold-valued tensor decompositions and approximations, MvTD motivation, RML Motivation symmetric spaces},
  id             = {385},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@Article{massonnet1998radar,
  author         = {Massonnet, Didier and Feigl, Kurt L},
  journal        = {Reviews of geophysics},
  title          = {Radar interferometry and its application to changes in the Earth's surface},
  year           = {1998},
  number         = {4},
  pages          = {441--500},
  volume         = {36},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/386 - Massonnet - Radar interferometry and its application to changes in the Earth's surface.pdf:PDF},
  groups         = {Manifold-valued Bayesian IPs, Manifold-valued tensor decompositions and approximations, MvTD motivation, Highly cited, RML Motivation symmetric spaces},
  id             = {386},
  publisher      = {Wiley Online Library},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@Article{basser1994mr,
  author         = {Basser, Peter J and Mattiello, James and LeBihan, Denis},
  journal        = {Biophysical journal},
  title          = {MR diffusion tensor spectroscopy and imaging},
  year           = {1994},
  number         = {1},
  pages          = {259--267},
  volume         = {66},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/387 - Basser - MR diffusion tensor spectroscopy and imaging.pdf:PDF},
  groups         = {Manifold-valued Bayesian IPs, Manifold-valued tensor decompositions and approximations, MvTD motivation, Highly cited, RML Motivation symmetric spaces},
  id             = {387},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@Article{adams1993orientation,
  author         = {Adams, Brent L and Wright, Stuart I and Kunze, Karsten},
  journal        = {Metallurgical Transactions A},
  title          = {Orientation imaging: the emergence of a new microscopy},
  year           = {1993},
  number         = {4},
  pages          = {819--831},
  volume         = {24},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/388 - Adams - Orientation imaging the emergence of a new microscopy.pdf:PDF},
  groups         = {Manifold-valued Bayesian IPs, Manifold-valued tensor decompositions and approximations, MvTD motivation, Highly cited, RML Motivation symmetric spaces},
  id             = {388},
  publisher      = {Springer},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@InProceedings{arvanitidis2019fast,
  author       = {Arvanitidis, Georgios and Hauberg, Soren and Hennig, Philipp and Schober, Michael},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  title        = {Fast and robust shortest paths on manifolds learned from data},
  year         = {2019},
  organization = {PMLR},
  pages        = {1506--1515},
  comment      = {Related to my work:
- MbPD:
	- different approach to ours
		- however, ours might not be always well-defined
		- theirs might not converge either
- Metric learning:
	- Approximate shortest paths},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/417 - Arvanitidis - Fast and Robust Shortest Paths on Manifolds Learned from Data.pdf:PDF},
  groups       = {Cryo-EM, Manifold-guided protein deformations, Motivation, Riemannian geometry for efficient protein conformation analysis, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id           = {417},
  ranking      = {rank3},
  readstatus   = {skimmed},
}

@Article{arvanitidis2017latent,
  author     = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  journal    = {arXiv preprint arXiv:1710.11379},
  title      = {Latent space oddity: on the curvature of deep generative models},
  year       = {2017},
  comment    = {Main ideas:
- Propose to use VAE to pull back Riemannian geometry onto the latent space

Comments:
- So the canonical way of training a VAE is not sufficient
- [418], [700] and [701] worked on this simultaneously

Related to my work:
- Metric learning:
	- Riemannian manifold learning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/418 - Arvanitidis - Latent space oddity on the curvature of deep generative models.pdf:PDF},
  groups     = {Cryo-EM, Manifold-guided protein deformations, Related work, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML Motivation Riemannian geometry, RML RW chart based},
  id         = {418},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{arvanitidis2020geometrically,
  author     = {Arvanitidis, Georgios and Hauberg, S{\o}ren and Sch{\"o}lkopf, Bernhard},
  journal    = {arXiv preprint arXiv:2008.00565},
  title      = {Geometrically enriched latent spaces},
  year       = {2020},
  comment    = {Main ideas:
- Use ambient space additional metrics to enforce domain knowledge

Related to my work:
- Metric learning:
	- Riemannian manifold learning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/419 - Arvanitidis - Geometrically Enriched Latent Spaces.pdf:PDF},
  groups     = {Cryo-EM, Manifold-guided protein deformations, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {419},
  readstatus = {skimmed},
}

@Article{arvanitidis2021prior,
  author     = {Arvanitidis, Georgios and Georgiev, Bogdan and Sch{\"o}lkopf, Bernhard},
  journal    = {arXiv preprint arXiv:2103.05290},
  title      = {A prior-based approximate latent riemannian metric},
  year       = {2021},
  comment    = {Main ideas:
- Use a metric with only diagonal components to make pull-back geometry from VAE more stable

Related to my work:
- Metric learning:
	- Riemannian manifold learning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/420 - Arvanitidis - A prior-based approximate latent riemannian metric.pdf:PDF},
  groups     = {Cryo-EM, Manifold-guided protein deformations, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {420},
  readstatus = {skimmed},
}

@Article{arvanitidis2016locally,
  author     = {Arvanitidis, Georgios and Hansen, Lars K and Hauberg, S{\o}ren},
  journal    = {Advances in Neural Information Processing Systems},
  title      = {A locally adaptive normal distribution},
  year       = {2016},
  volume     = {29},
  comment    = {[423]:  "To this end,Arvanitidis et al. (2016) proposed a maximum likelihoodestimation scheme based on a data-induced metric to learnthe parameters of a Locally Adaptive Normal Distribution(LAND), illustrated in Fig. 1."

Related to my work:
- Metric learning:
	- Statistical models from Riemannian manifold learning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/422 - Arvanitidis - A Locally Adaptive Normal Distribution.pdf:PDF},
  groups     = {Cryo-EM, Manifold-guided protein deformations, Riemannian manifold learning with extras, RML Related work Finding a Riemannian geometry on all of space},
  id         = {422},
  readstatus = {skimmed},
}

@Article{bergmann2016parallel,
  author     = {Bergmann, Ronny and Persch, Johannes and Steidl, Gabriele},
  journal    = {SIAM Journal on Imaging Sciences},
  title      = {A parallel Douglas--Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds},
  year       = {2016},
  number     = {3},
  pages      = {901--937},
  volume     = {9},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/445 - Bergmann - A parallel Douglas--Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds.pdf:PDF},
  groups     = {In Article, Lifting for SPA, RML Motivation symmetric spaces},
  id         = {445},
  publisher  = {SIAM},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{bergmann2021fenchel,
  author     = {Bergmann, Ronny and Herzog, Roland and Silva Louzeiro, Maur{\'\i}cio and Tenbrinck, Daniel and Vidal-N{\'u}{\~n}ez, Jos{\'e}},
  journal    = {Foundations of Computational Mathematics},
  title      = {Fenchel duality theory and a primal-dual algorithm on Riemannian manifolds},
  year       = {2021},
  number     = {6},
  pages      = {1465--1504},
  volume     = {21},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/211 - Bergmann - Fenchel Duality Theory and a Primal-Dual Algorithm on Riemannian Manifolds.pdf:PDF},
  groups     = {In Article, Manifold-valued tensor decompositions and approximations, MvTD motivation, Lifting for SPA, RML Motivation symmetric spaces},
  id         = {446},
  publisher  = {Springer},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{karcher1977riemannian,
  author     = {Karcher, Hermann},
  journal    = {Communications on pure and applied mathematics},
  title      = {Riemannian center of mass and mollifier smoothing},
  year       = {1977},
  number     = {5},
  pages      = {509--541},
  volume     = {30},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/557 - Karcher - Riemannian center of mass and mollifier smoothing.pdf:PDF},
  groups     = {In Article, Cryo-EM, Motivation, Riemannian geometry for efficient protein conformation analysis, Lifting for SPA, RML Motivation Riemannian geometry},
  id         = {557},
  publisher  = {Wiley Online Library},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{bergmann2018bezier,
  author     = {Bergmann, Ronny and Gousenbourger, Pierre-Yves},
  journal    = {Frontiers in Applied Mathematics and Statistics},
  title      = {A Variational Model for Data Fitting on Manifolds by Minimizing the Acceleration of a Bézier Curve},
  year       = {2018},
  issn       = {2297-4687},
  volume     = {4},
  abstract   = {We derive a variational model to fit a composite Bézier curve to a set of data points on a Riemannian manifold. The resulting curve is obtained in such a way that its mean squared acceleration is minimal in addition to remaining close the data points. We approximate the acceleration by discretizing the squared second order derivative along the curve. We derive a closed-form, numerically stable and efficient algorithm to compute the gradient of a Bézier curve on manifolds with respect to its control points, expressed as a concatenation of so-called adjoint Jacobi fields. Several examples illustrate the capabilities and validity of this approach both for interpolation and approximation. The examples also illustrate that the approach outperforms previous works tackling this problem.AMS subject classification (2010). 65K10, 65D10, 65D25, 53C22, 49Q99.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/564 - Bergmann - A Variational Model for Data Fitting on Manifolds by Minimizing the Acceleration of a Bézier Curve.pdf:PDF},
  groups     = {Manifold-guided protein deformations, Cryo-EM, Misc, Related work, Riemannian geometry for efficient protein conformation analysis, RML Motivation symmetric spaces, RML Motivation Riemannian geometry},
  id         = {564},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{mcinnes2018umap,
  author     = {McInnes, Leland and Healy, John and Melville, James},
  journal    = {arXiv preprint arXiv:1802.03426},
  title      = {Umap: Uniform manifold approximation and projection for dimension reduction},
  year       = {2018},
  comment    = {Main ideas:
- Propose UMAP for dimension reduction and visualisation
- construct a graph and use attractive and repulsive forces to tear the data apart
	- Losing global structure to some extend

Comments:
- Focus on local features
- Need uniform data on manifold
- Distances don't have any meaning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/581 - McInnes - Umap Uniform manifold approximation and projection for dimension reduction.pdf:PDF},
  groups     = {RML Motivation non-linear embedding},
  id         = {581},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{ghojogh2022spectral,
  author     = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
  journal    = {arXiv preprint arXiv:2201.09267},
  title      = {Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey},
  year       = {2022},
  comment    = {Comment:
- It does not really comment on the ups and downs of the methods

Related to my work:
- RML
	- Background in general types of metric learning + somewhat on deep metric learning. Complements 846 nicely. 
- Idea for project with Julian:
	- Can use the distances in (at least) section 3 to construct a non-linear metric},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/583 - Ghojogh - Spectral, Probabilistic, and Deep Metric Learning Tutorial and Survey.pdf:PDF},
  groups     = {General interest, IsSurvey, Riemannian manifold learning with extras, RML Motivation data analysis tools, RML Related work Metric learning},
  id         = {583},
  readstatus = {skimmed},
}

@Article{fletcher2004principal,
  author         = {Fletcher, P Thomas and Lu, Conglin and Pizer, Stephen M and Joshi, Sarang},
  journal        = {IEEE transactions on medical imaging},
  title          = {Principal geodesic analysis for the study of nonlinear statistics of shape},
  year           = {2004},
  number         = {8},
  pages          = {995--1005},
  volume         = {23},
  comment        = {Main ideas:
- Brought up principle geodesic analysis
	- still compute it in the tangent space

Comments:
- Lie group framework might be mathematically nice, but lacks physical motivation
- Similar to [594], but different parametrization

Related to my work:
- MbPD:
	- We use this in experiments
		- also see [588]},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/595 - Fletcher - Principal geodesic analysis for the study of nonlinear statistics of shape.pdf:PDF},
  groups         = {Cryo-EM, Manifold-guided protein deformations, Manifold-valued tensor decompositions and approximations, intrinsic with linearisation, Highly cited, Motivation, Riemannian geometry for efficient protein conformation analysis, RML Motivation Riemannian geometry},
  id             = {595},
  publisher      = {IEEE},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {skimmed},
}

@InProceedings{cai2008non,
  author         = {Cai, Deng and He, Xiaofei and Wu, Xiaoyun and Han, Jiawei},
  booktitle      = {2008 eighth IEEE international conference on data mining},
  title          = {Non-negative matrix factorization on manifold},
  year           = {2008},
  organization   = {IEEE},
  pages          = {63--72},
  comment        = {Ben: "Top results for ‘manifold NMF’ – [1, 3, 4]"

Main ideas:
- regularize NMF so that it picks up on the global geometry of the data through the graph laplacian

Comments:
- [714] about this paper: "Rigorously speaking, however,the problem (3) as it is has no optimal solutions differentfrom the NMF solutions. The following argument showsthat (3) has no critical points if the regularization works."

Related to my work:
- Manifold-valued tensor decomposition
	- although the method is actually geometrically motivated, the method still avoids actually working in the manifold due to practical considerations -- the maths wasn't there yet I guess.},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/603 - Cai - Non-negative matrix factorization on manifold.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, extrinsic with manifold assumption, RML Motivation data analysis tools},
  id             = {603},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {read},
}

@Article{guan2011manifold,
  author         = {Guan, Naiyang and Tao, Dacheng and Luo, Zhigang and Yuan, Bo},
  journal        = {IEEE Transactions on Image Processing},
  title          = {Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent},
  year           = {2011},
  number         = {7},
  pages          = {2030--2048},
  volume         = {20},
  comment        = {Ben: "Top results for ‘manifold NMF’ – [1, 3, 4]"

Main ideas:
- add a new regularizer based on the graph laplacian
- propose an optimisation scheme that at least converges to a local minimum

Related to my work:
- MvTD:
	- use the manifold in an extrinsic way},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/605 - Guan - Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, extrinsic with manifold assumption, RML Motivation data analysis tools},
  id             = {605},
  publisher      = {IEEE},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {read},
}

@Article{little2022balancing,
  author    = {Little, Anna and McKenzie, Daniel and Murphy, James M},
  journal   = {SIAM Journal on Mathematics of Data Science},
  title     = {Balancing Geometry and Density: Path Distances on High-Dimensional Data},
  year      = {2022},
  number    = {1},
  pages     = {72--99},
  volume    = {4},
  file      = {:/Users/wdiepeveen/Documents/PhD/References/611 - Little - Balancing Geometry and Density Path Distances on High-Dimensional Data.pdf:PDF},
  groups    = {General interest, RML Research, Riemannian manifold learning with extras},
  id        = {611},
  publisher = {SIAM},
}

@Article{bacak2016second,
  author         = {Bac{\'a}k, Miroslav and Bergmann, Ronny and Steidl, Gabriele and Weinmann, Andreas},
  journal        = {SIAM Journal on Scientific Computing},
  title          = {A second order nonsmooth variational model for restoring manifold-valued images},
  year           = {2016},
  number         = {1},
  pages          = {A567--A597},
  volume         = {38},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/615 - Bacak - A second order nonsmooth variational model for restoring manifold-valued images.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, RML Motivation symmetric spaces},
  id             = {615},
  publisher      = {SIAM},
  qualityassured = {qualityAssured},
  ranking        = {rank5},
  readstatus     = {read},
}

@Article{fefferman2020reconstruction,
  author     = {Fefferman, Charles and Ivanov, Sergei and Lassas, Matti and Narayanan, Hariharan},
  journal    = {SIAM Journal on Mathematics of Data Science},
  title      = {Reconstruction of a Riemannian manifold from noisy intrinsic distances},
  year       = {2020},
  number     = {3},
  pages      = {770--808},
  volume     = {2},
  comment    = {Main ideas:
- pairwise manifold distances can be reconstructed even in the case of noisy measurements if there is sufficient data

Comment:
- After the distances are denoised the authors provide 3 ways to compute a manifold embedding

Related to my work:
- Future research:
	- this could be the motivation for the existence of the diffeomorphism we try to learn in metric learning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/632 - Fefferman - Reconstruction of a Riemannian Manifold from Noisy Intrinsic Distances.pdf:PDF},
  groups     = {General interest, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW point cloud based},
  id         = {632},
  publisher  = {SIAM},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Article{budninskiy2019parallel,
  author     = {Budninskiy, Max and Yin, Gloria and Feng, Leman and Tong, Yiying and Desbrun, Mathieu},
  journal    = {SIAM Journal on Applied Algebra and Geometry},
  title      = {Parallel transport unfolding: a connection-based manifold learning approach},
  year       = {2019},
  number     = {2},
  pages      = {266--291},
  volume     = {3},
  comment    = {Main ideas:
- Isomap has issues with dimension reduction for non-geodesically convex
- using parallel transport this can be overcome while still having similar performance as Isomap

Comments:
- they talk about 
- the proof in the appendix could be interesting
- interesting to see how consistently the local methods fail under noise

Related to my work:
- Future research
	- consider first dimension reduction with this approach and then (now topology is known) either
		- use the old distances to train a NN to recover the distances better (e.g. through hyperbolic geometry or so)
		- use work from density based metric learning to actually pick up the geometry/geodesics
	- from there we can use the NN idea to learn a diffeomorphism that gives us the geodesics in the original space
		- staying with just this approach might make more sense than going for a second approach on top of it},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/638 - Budninskiy - Parallel transport unfolding a connection-based manifold learning approach.pdf:PDF},
  groups     = {General interest, Riemannian manifold learning with extras, RML Research},
  id         = {638},
  priority   = {prio1},
  publisher  = {SIAM},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{perraul2013non,
  author     = {Perraul-Joncas, Dominique and Meil{\^a}, Marina},
  journal    = {arXiv preprint arXiv:1305.7255},
  title      = {Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery},
  year       = {2013},
  comment    = {Main ideas:
- After a graph embedding algorithm, this paper suggests how to use the graph laplacian to construct a Riemannian manifold on the obtained chart.

Comments:
- we will need to store a whole bunch of extra features},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/640 - Perraul-Joncas - Non-linear dimensionality reduction Riemannian metric estimation and the problem of geometric discovery.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {640},
  readstatus = {skimmed},
}

@Article{coifman2006diffusion,
  author     = {Coifman, Ronald R and Lafon, St{\'e}phane},
  journal    = {Applied and computational harmonic analysis},
  title      = {Diffusion maps},
  year       = {2006},
  number     = {1},
  pages      = {5--30},
  volume     = {21},
  comment    = {Main ideas:
- Proposed diffusion maps
- Random walk over neirby pointclouds and obtain an embedding from the t-time product of the markov transition kernel},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/643 - Coifman - Diffusion maps.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation non-linear embedding},
  id         = {643},
  publisher  = {Elsevier},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{fefferman2020reconstructionB,
  author     = {Fefferman, Charles and Ivanov, Sergei and Kurylev, Yaroslav and Lassas, Matti and Narayanan, Hariharan},
  journal    = {Foundations of Computational Mathematics},
  title      = {Reconstruction and interpolation of manifolds. i: The geometric whitney problem},
  year       = {2020},
  number     = {5},
  pages      = {1035--1133},
  volume     = {20},
  comment    = {Main ideas:
- Answer when two geometric Whitney problems can be solved (construct an M from a subset and construct a Riemannian manifold from a metric space)
- Construction of the Riemannian manifold if the metric space is discrete.

Comments:
- "optimal reconstruction in terms of invariant bounds for the curvature and injectivity radius"
- Diverse review of types of manifold fitting},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/644 - Fefferman - Reconstruction and interpolation of manifolds I The geometric Whitney problem.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW point cloud based},
  id         = {644},
  publisher  = {Springer},
  readstatus = {skimmed},
}

@InProceedings{carreira2011manifold,
  author       = {Carreira-Perpin, Miguel A and Lu, Zhengdong and others},
  booktitle    = {2011 IEEE 11th International Conference on Data Mining},
  title        = {Manifold learning and missing data recovery through unsupervised regression},
  year         = {2011},
  organization = {IEEE},
  pages        = {1014--1019},
  comment      = {Related to my work:
- RML: 
	- downstream task: inpainting using a geometric prior},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/645 - Carreira-Perpin - Manifold learning and missing data recovery through unsupervised regression.pdf:PDF},
  groups       = {Riemannian manifold learning with extras, RML Motivation data analysis tools},
  id           = {645},
  readstatus   = {skimmed},
}

@InCollection{cox2008multidimensional,
  author     = {Cox, Michael AA and Cox, Trevor F},
  booktitle  = {Handbook of data visualization},
  publisher  = {Springer},
  title      = {Multidimensional scaling},
  year       = {2008},
  pages      = {315--347},
  comment    = {Main ideas:
- There are many MDS variations, symmarized in a work like this

Comments:
- Here it is already assumed that there are known distances},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/646 - Cox - Multidimensional scaling.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation non-linear embedding},
  id         = {646},
  readstatus = {skimmed},
}

@InProceedings{fefferman2018fitting,
  author       = {Fefferman, Charles and Ivanov, Sergei and Kurylev, Yaroslav and Lassas, Matti and Narayanan, Hariharan},
  booktitle    = {Conference On Learning Theory},
  title        = {Fitting a putative manifold to noisy data},
  year         = {2018},
  organization = {PMLR},
  pages        = {688--720},
  comment      = {Main ideas:
- Answer the question whether we can retrieve a manifold (as in the set of points) from noisy measurements
	- in terms of hausdorf distance between the sets

Related to my work:
- RML:
	- Not really Riemannian manifold learning. Just the manifold.
		- So maybe not that related to this project},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/647 - Fefferman - Fitting a putative manifold to noisy data.pdf:PDF},
  groups       = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW point cloud based},
  id           = {647},
  ranking      = {rank3},
  readstatus   = {skimmed},
}

@Article{fefferman2019fitting,
  author     = {Fefferman, Charles and Ivanov, Sergei and Lassas, Matti and Narayanan, Hariharan},
  journal    = {arXiv preprint arXiv:1910.05084},
  title      = {Fitting a manifold of large reach to noisy data},
  year       = {2019},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/648 - Fefferman - Fitting a manifold of large reach to noisy data.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW point cloud based},
  id         = {648},
  readstatus = {skimmed},
}

@Article{lin2008riemannian,
  author     = {Lin, Tong and Zha, Hongbin},
  journal    = {IEEE transactions on pattern analysis and machine intelligence},
  title      = {Riemannian manifold learning},
  year       = {2008},
  number     = {5},
  pages      = {796--809},
  volume     = {30},
  comment    = {Related to my work:
- RML
	- They only seem to construct an embedding. No geometric ideas, but is possible?},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/652 - Lin - Riemannian manifold learning.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {652},
  publisher  = {IEEE},
  readstatus = {skimmed},
}

@Article{tenenbaum2000global,
  author     = {Tenenbaum, Joshua B and Silva, Vin de and Langford, John C},
  journal    = {science},
  title      = {A global geometric framework for nonlinear dimensionality reduction},
  year       = {2000},
  number     = {5500},
  pages      = {2319--2323},
  volume     = {290},
  comment    = {Main ideas:
- Proposed isomap
	- compute graph distances within some radius + do MDS afterwards

Comments:
- Will break down if the to be mapped set is not geodesically convex

Related to my work:
- RML:
	- background + we use this to construct our distances as well
	- Could maybe use same data for experiments as all is just 2D here.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/654 - Tenenbaum - A global geometric framework for nonlinear dimensionality reduction.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation non-linear embedding, RML Research},
  id         = {654},
  publisher  = {American Association for the Advancement of Science},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Article{donoho2005image,
  author     = {Donoho, David L and Grimes, Carrie},
  journal    = {Journal of mathematical imaging and vision},
  title      = {Image manifolds which are isometric to Euclidean space},
  year       = {2005},
  number     = {1},
  pages      = {5--24},
  volume     = {23},
  comment    = {Main ideas:
- Go through examples where isomap can be expected to recover the manifold structure of images under an L2 metric.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/655 - Donoho - Image manifolds which are isometric to Euclidean space.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id         = {655},
  publisher  = {Springer},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{belkin2001laplacian,
  author     = {Belkin, Mikhail and Niyogi, Partha},
  journal    = {Advances in neural information processing systems},
  title      = {Laplacian eigenmaps and spectral techniques for embedding and clustering},
  year       = {2001},
  volume     = {14},
  comment    = {Main ideas:
- Proposes laplace eigenmaps for dimension reduction

Comment:
- Not sure what structure we try to encode here/what the practical motivation is beyond it being a new algorithm},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/656 - Belkin - Laplacian eigenmaps and spectral techniques for embedding and clustering.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation non-linear embedding},
  id         = {656},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@Article{donoho2003hessian,
  author     = {Donoho, David L and Grimes, Carrie},
  journal    = {Proceedings of the National Academy of Sciences},
  title      = {Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data},
  year       = {2003},
  number     = {10},
  pages      = {5591--5596},
  volume     = {100},
  comment    = {Main ideas:
- extension of LLE [851]

Related to my work:
- RML
	- They only seem to construct an embedding. No geometric ideas, but is possible?},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/657 - Donoho - Hessian eigenmaps Locally linear embedding techniques for high-dimensional data.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation non-linear embedding},
  id         = {657},
  publisher  = {National Acad Sciences},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{brand2002charting,
  author     = {Brand, Matthew},
  journal    = {Advances in neural information processing systems},
  title      = {Charting a manifold},
  year       = {2002},
  volume     = {15},
  comment    = {Main ideas:
- Construct smooth charts from local linear embeddings

Related to my work:
- Riemannian manifold learning},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/659 - Brand - Charting a manifold.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {659},
  readstatus = {skimmed},
}

@Article{roweis2001global,
  author     = {Roweis, Sam and Saul, Lawrence and Hinton, Geoffrey E},
  journal    = {Advances in neural information processing systems},
  title      = {Global coordination of local linear models},
  year       = {2001},
  volume     = {14},
  comment    = {Main ideas:
- Construct smooth charts from local linear embeddings

Related to my work:
- Riemannian manifold learning
	- Only does the chart. Not the geometry},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/660 - Roweis - Global coordination of local linear models.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {660},
  readstatus = {skimmed},
}

@Article{zhang2004principal,
  author         = {Zhang, Zhenyue and Zha, Hongyuan},
  journal        = {SIAM journal on scientific computing},
  title          = {Principal manifolds and nonlinear dimensionality reduction via tangent space alignment},
  year           = {2004},
  number         = {1},
  pages          = {313--338},
  volume         = {26},
  comment        = {Main ideas:
- "We address two interrelated objectives of nonlinear structure finding:(1) to construct the so-called principal manifold [11] that goes through “the middle”of the data points and (2) to find the global coordinate system that characterizes theset of data points in a low-dimensional space."
- Algorithm called LTSA

Comments:
- They claim to be a variant of local linear embedding
- For constructing the mapping into the original space, they need all the data
- Clear about limitations of method
- It is not entirely clear whether we can apply this to manifolds with underlying curvature

Related to my work:
- Riemannian metric learning
	- He more or less does what we try to do, but does not get extra features and the method is empirically non-robust [according to stress-based literature]},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/661 - Zhang - Principal manifolds and nonlinear dimensionality reduction via tangent space alignment.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, Highly cited, extrinsic with manifold assumption, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML Motivation non-linear embedding, RML RW chart based},
  id             = {661},
  publisher      = {SIAM},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {read},
}

@Article{hauser2017principles,
  author     = {Hauser, Michael and Ray, Asok},
  journal    = {Advances in neural information processing systems},
  title      = {Principles of Riemannian geometry in neural networks},
  year       = {2017},
  volume     = {30},
  comment    = {Main ideas:
- View ResNets as coordinate transformations
- Propose new discretisation schemes for ResNets

Comment:
- Make a big fuss about it without saying anything new

Related to my work:
- RML:
	- They also talk about learning coordinate charts + non-linear pullback geometry
	- They don't use the pulled back structure
		- Also it won't give much having seen the embeddings learned. It has only learned to separate data.
	- They also only pull back l2.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/693 - Hauser - Principles of Riemannian geometry in neural networks.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a Riemannian geometry on all of space},
  id         = {693},
  readstatus = {skimmed},
}

@InProceedings{behrmann2019invertible,
  author       = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky TQ and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
  booktitle    = {International Conference on Machine Learning},
  title        = {Invertible residual networks},
  year         = {2019},
  organization = {PMLR},
  pages        = {573--582},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/695 - Behrmann - Invertible residual networks.pdf:PDF},
  groups       = {Riemannian manifold learning with extras, RML Research},
  id           = {695},
  ranking      = {rank4},
  readstatus   = {skimmed},
}

@Article{braunsmann2022learning,
  author     = {Braunsmann, Juliane and Rajkovi{\'c}, Marko and Rumpf, Martin and Wirth, Benedikt},
  journal    = {arXiv preprint arXiv:2208.10193},
  title      = {Learning Low Bending and Low Distortion Manifold Embeddings: Theory and Applications},
  year       = {2022},
  comment    = {Main ideas:
- Train a VAE with regularisation that penalizes bending and stretching
- Analysis of the energy

Comments:
- some of the used data is fairly interesting
- can do compact manifolds
- still a bit awkward to get geodesics etc back

Related to my work:
- Riemannian metric learning
	- could use some of the data used
	- nice as background how others try to do it and that there is some maths done here},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/699 - Braunsmann - Learning Low Bending and Low Distortion Manifold Embeddings Theory and Applications.pdf:PDF},
  groups     = {General interest, Riemannian manifold learning with extras, RML Motivation non-linear embedding},
  id         = {699},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@InProceedings{shao2018riemannian,
  author     = {Shao, Hang and Kumar, Abhishek and Thomas Fletcher, P},
  booktitle  = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  title      = {The riemannian geometry of deep generative models},
  year       = {2018},
  pages      = {315--323},
  comment    = {[699]: "For sufficientlyregular data manifolds it is shown in [SKTF18] how to transfer this metric to the latent manifold, therebyturning it into a Riemannian manifold."

Comments:
- [418], [700] and [701] worked on this simultaneously},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/700 - Shao - The riemannian geometry of deep generative models.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {700},
  readstatus = {skimmed},
}

@InProceedings{chen2018metrics,
  author       = {Chen, Nutan and Klushyn, Alexej and Kurle, Richard and Jiang, Xueyan and Bayer, Justin and Smagt, Patrick},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  title        = {Metrics for deep generative models},
  year         = {2018},
  organization = {PMLR},
  pages        = {1540--1550},
  comment      = {Comments:
- many of these manifold embedding papers solely rely on local properties, even though we know that we need non-local information for global reconstruction of the manifold
- They have a nice data set
- [418], [700] and [701] worked on this simultaneously},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/701 - Chen - Metrics for deep generative models.pdf:PDF},
  groups       = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id           = {701},
  readstatus   = {skimmed},
}

@InProceedings{lopez2021symmetric,
  author         = {L{\'o}pez, Federico and Pozzetti, Beatrice and Trettel, Steve and Strube, Michael and Wienhard, Anna},
  booktitle      = {International Conference on Machine Learning},
  title          = {Symmetric spaces for graph embeddings: A finsler-riemannian approach},
  year           = {2021},
  organization   = {PMLR},
  pages          = {7090--7101},
  comment        = {Main ideas:
- Realize that symmetric spaces are very versatile for graph embeddings, but this hasn't been realized yet by the cummunity
- Propose to use Siegel spaces to embed data

Comment:
- They optimise to get the graph embedding

Related to my work
- Riemannian metric learning
	- Checkout the embedding manifold they use!!!
	- See C3 for loss function
		- They use different loss than we do
		- They seem to care more about what happens locally rather than globally (e.g., we don't expect geodesics to be preserved here)},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/703 - Lopez - Symmetric spaces for graph embeddings A finsler-riemannian approach.pdf:PDF},
  groups         = {General interest, MvTD motivation, Manifold-valued tensor decompositions and approximations, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {703},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {read},
}

@Article{sonthalia2021can,
  author     = {Sonthalia, Rishi and Van Buskirk, Greg and Raichel, Benjamin and Gilbert, Anna},
  journal    = {Advances in Neural Information Processing Systems},
  title      = {How can classical multidimensional scaling go wrong?},
  year       = {2021},
  pages      = {12304--12315},
  volume     = {34},
  comment    = {Main ideas:
- The discrepancy between the Frobenius norm difference of the EDMs of the minimiser of MDS and the original distance matrix does not decrease if the embedding dimension increases, which is counter-intuitive. 
- Propose a way of getting a better matrix to apply MDS on

Related to my work:
- RML:
	- they also provide better ways of constructing distance matrices as MDS input if we want to embed into Euclidean space
	- Can mention in remark that besides finding a better way of constructing the distances such as work by [638], this would be another alternative

Future research ideas:
- Can we get some inspiration here on how to answer questions pertaining whether it is possible to embed data in any symmetric space?},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/705 - Sonthalia - How can classical multidimensional scaling go wrong.pdf:PDF},
  groups     = {General interest, Riemannian manifold learning with extras, RML Motivation symmetric space embedding, Ideas I-xyz, RML Research},
  id         = {705},
  ranking    = {rank4},
  readstatus = {read},
}

@InProceedings{sala2018representation,
  author       = {Sala, Frederic and De Sa, Chris and Gu, Albert and R{\'e}, Christopher},
  booktitle    = {International conference on machine learning},
  title        = {Representation tradeoffs for hyperbolic embeddings},
  year         = {2018},
  organization = {PMLR},
  pages        = {4460--4469},
  comment      = {Main ideas:
- Embedding into hyperbolic 2-space in two steps
	- embed in tree
	- embed tree in hyperbolic space
- for hyperbolic embedding we might need a high precision (number of bits) to get low distortion
	- can be improved by passing to hyperbolic ball
- For tree embedding, there are standard results on the distortion
- Propose a H-MDS

Comments:
- the error they are talking about, is it the classical MDS stress of the difference in distance matrices?
	- if the second, we also expect similar results as in [705] that we do not necessarily get better embeddings if we go higher-dimensional
	- Yes they don't care about the manifold error, but do everything in the classical MDS setting either
		- Don't multiply the distance matrix with centering matrices, but they assume that everything is centered. So this is the same in the end

Future research ideas:
- Can we get some inspiration here on how to answer questions pertaining whether it is possible to embed data in any symmetric space?},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/706 - Sala - Representation tradeoffs for hyperbolic embeddings.pdf:PDF},
  groups       = {General interest, Riemannian manifold learning with extras, RML Motivation symmetric space embedding, Ideas I-xyz},
  id           = {706},
  ranking      = {rank3},
  readstatus   = {skimmed},
}

@Article{gruffaz2021learning,
  author     = {Gruffaz, Samuel and Poulet, Pierre-Emmanuel and Maheux, Etienne and Jedynak, Bruno and Durrleman, Stanley},
  journal    = {Advances in Neural Information Processing Systems},
  title      = {Learning Riemannian metric for disease progression modeling},
  year       = {2021},
  pages      = {23780--23792},
  volume     = {34},
  comment    = {Related to my work:
- Riemannian metric learning
	- Instead of pulling back metrics, they push it forward (proposition 1)
		- They also state that we get the exp and parallel transport, but they require isometry property, which we don't need},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/708 - Gruffaz - Learning Riemannian metric for disease progression modeling.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a Riemannian geometry on all of space, RML Research},
  id         = {708},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{shu2017multiple,
  author         = {Shu, Zhenqiu and Fan, Hongfei and Huang, Pu and Wu, Dong and Ye, Feiyue and Wu, Xiaojun},
  journal        = {IET Image Processing},
  title          = {Multiple Laplacian graph regularised low-rank representation with application to image representation},
  year           = {2017},
  number         = {6},
  pages          = {370--378},
  volume         = {11},
  comment        = {Main ideas:
- Assuming that there is not just one manifold but many, they propose to set up a couple of graph laplacians and optimise also over on which of them the data is distributed when computing an LLR

Related to my work:
- MvTD:
	- Extrinsic with manifold assumptions
	- Also future work as these ideas can be modified to the manifold settings, once the corresponding simple versions have been generalized},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/709 - Shu - Multiple Laplacian graph regularised low-rank representation with application to image representation.pdf:PDF},
  groups         = {extrinsic with manifold assumption, Manifold-valued tensor decompositions and approximations, RML Motivation data analysis tools},
  id             = {709},
  publisher      = {Wiley Online Library},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {read},
}

@Article{keller2020hydra,
  author         = {Keller-Ressel, Martin and Nargang, Stephanie},
  journal        = {Journal of Complex Networks},
  title          = {Hydra: a method for strain-minimizing hyperbolic embedding of network-and distance-based data},
  year           = {2020},
  number         = {1},
  pages          = {cnaa002},
  volume         = {8},
  comment        = {Main ideas:
- Reconsider hyperbolic embeddings
- In particular change the standard stress loss function and show that they can find a global minimiser for this new strain loss

Comment:
- "As a next step, we plan to make strain-minimizing hyperbolic embedding methods feasible for largeand very large networks. For such networks the computational complexity of O(nα) (with α > 2) ofthe proposed methods, but also of the graph distance calculation itself, are prohibitive. For this reason,heuristics such as the landmark heuristic of [7] will have to be adapted to strain-minimizing embeddingmethods."

Related to my work:
- MvTD:
	- motivation hyperbolic embeddings
- Riemannian metric learning:
	- they mention that this does not necessarily scale super well. Our approach might be of help here},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/712 - Keller-Ressel - Hydra a method for strain-minimizing hyperbolic embedding of network-and distance-based data.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding, RML Research},
  id             = {712},
  publisher      = {Oxford University Press},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {read},
}

@Article{zhang2012low,
  author         = {Zhang, Zhenyue and Zhao, Keke},
  journal        = {IEEE transactions on pattern analysis and machine intelligence},
  title          = {Low-rank matrix approximation with manifold regularization},
  year           = {2012},
  number         = {7},
  pages          = {1717--1729},
  volume         = {35},
  comment        = {Main idea:
- SVD, but with graph laplacian regularisation
- Closed form solution (useful for small data)
- Iterative solution for large scale data

Comment:
- he comments that NMF with manifold regularisation has no solution, which is interesting
	- [605] has a way around this
- Does much better than NMF for clustering

Related to my work:
- MvTD:
	- Actually is doing something very similar to what we want to do, but here it is not clear from a theoretical point of view whether this regulariser gives us everything we need
	- Probably should mention this one as the last one for the part in the related work},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/714 - Zhang - Low-rank matrix approximation with manifold regularization.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, extrinsic with manifold assumption, RML Motivation data analysis tools},
  id             = {714},
  publisher      = {IEEE},
  qualityassured = {qualityAssured},
  ranking        = {rank5},
  readstatus     = {read},
}

@InProceedings{cruceru2021computationally,
  author         = {Cruceru, Calin and B{\'e}cigneul, Gary and Ganea, Octavian-Eugen},
  booktitle      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title          = {Computationally tractable riemannian manifolds for graph embeddings},
  year           = {2021},
  number         = {8},
  pages          = {7133--7141},
  volume         = {35},
  comment        = {[703]: " or to use Grassmannian manifolds or the space ofsymmetric positive definite matrices (SPD) as a trade-offbetween the representation capability and the computationaltractability of the space (Huang & Gool, 2017; Huang et al.,2018; Cruceru et al., 2020)."

[703]: "In this work, we propose the systematic use of symmetricspaces in representation learning: this is a class comprisingall the aforementioned spaces"

Comment:
- Mentions a couple of ways to minimise loss
- Propose another loss},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/716 - Cruceru - Computationally tractable riemannian manifolds for graph embeddings.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {716},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {skimmed},
}

@InProceedings{pai2019dimal,
  author       = {Pai, Gautam and Talmon, Ronen and Bronstein, Alex and Kimmel, Ron},
  booktitle    = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title        = {Dimal: Deep isometric manifold learning using sparse geodesic sampling},
  year         = {2019},
  organization = {IEEE},
  pages        = {819--828},
  comment      = {Main ideas:
- use NN to do MDS from graph distances
	- Use farthest point sampling to get a small enough data set for efficient computation
- main upshot is that we can also embed new points now

Comments:
- they claim that such a method is more scalable, but except for the optimisation they still need to compute the same amount of paiwise distances as say Isomap

Related to my work:
- RML
	- Use similar training strategy
	- Use distances to go to embedding
		- Now there is a way forward if there is new data coming in
		- but there is no way back},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/723 - Pai - Dimal Deep isometric manifold learning using sparse geodesic sampling.pdf:PDF},
  groups       = {Riemannian manifold learning with extras, RML Research, RML Motivation non-linear embedding},
  id           = {723},
  ranking      = {rank3},
  readstatus   = {read},
}

@Article{wilson2014spherical,
  author         = {Wilson, Richard C and Hancock, Edwin R and Pekalska, El{\.z}bieta and Duin, Robert PW},
  journal        = {IEEE transactions on pattern analysis and machine intelligence},
  title          = {Spherical and hyperbolic embeddings of data},
  year           = {2014},
  number         = {11},
  pages          = {2255--2269},
  volume         = {36},
  comment        = {[703]: "For this reason, embeddings into hyperbolic(Krioukov et al., 2009; Nickel & Kiela, 2017; Salaet al., 2018; L´opez & Strube, 2020) and spherical spaces(Wilson et al., 2014; Liu et al., 2017; Xu & Durrett, 2018)have been developed."

[703]: "In this work, we propose the systematic use of symmetricspaces in representation learning: this is a class comprisingall the aforementioned spaces"

Main ideas:
- "In this paper, we propose a number of novel extensionsto address the problem of embedding into spherical andhyperbolic spaces. 
	- First, we show how to find and appropriateradius of curvature for the manifold directly from thedata. 
	- We then develop a method of embedding into thesespaces which, in contrast to other approaches, is not basedon optimization. 
	- Finally, we develop an optimizationscheme to refine the results which is specifically tailored tothe problem of constant-curvature embeddings and easilyextends to any number of dimensions in spherical or hyperbolicspace

Comment:
- Important to find the radius. 

Related to my work:
- MvTD
	- motivation},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/730 - Wilson - Spherical and hyperbolic embeddings of data.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {730},
  publisher      = {IEEE},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {skimmed},
}

@InProceedings{liu2017sphereface,
  author         = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
  booktitle      = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  title          = {Sphereface: Deep hypersphere embedding for face recognition},
  year           = {2017},
  pages          = {212--220},
  comment        = {[703]: "For this reason, embeddings into hyperbolic(Krioukov et al., 2009; Nickel & Kiela, 2017; Salaet al., 2018; L´opez & Strube, 2020) and spherical spaces(Wilson et al., 2014; Liu et al., 2017; Xu & Durrett, 2018)have been developed."

[703]: "In this work, we propose the systematic use of symmetricspaces in representation learning: this is a class comprisingall the aforementioned spaces"

Main ideas:
- Embed faces into sphere for downstream processing},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/731 - Liu - Sphereface Deep hypersphere embedding for face recognition.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Highly cited, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {731},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {skimmed},
}

@InProceedings{gu2019learning,
  author         = {Gu, Albert and Sala, Frederic and Gunel, Beliz and R{\'e}, Christopher},
  booktitle      = {International Conference on Learning Representations},
  title          = {Learning mixed-curvature representations in product spaces},
  year           = {2019},
  comment        = {[703]: "Recent work proposes to combinedifferent curvatures through several layers (Chami et al.,2019; Bachmann et al., 2020; Grattarola et al., 2020), toenrich the geometry by considering Cartesian products ofspaces (Gu et al., 2019; Tifrea et al., 2019; Skopek et al.,2020),"

[703]: "In this work, we propose the systematic use of symmetricspaces in representation learning: this is a class comprisingall the aforementioned spaces"

Main ideas:
- embed graph data into mixed curvature spaces
- develop some ideas how to pick these spaces
- use the geometry for downstream data processing

Comment:
- They also have to choose somehow how to compose the space
	- They actually do this in a well-motivated fashion:
		- "We address this challenge byintroducing a theory-guided heuristic estimator for the signature. We do so by matching an empiricalnotion of discrete curvature in our data with the theoretical distribution of the sectional curvature, afine-grained measure of curvature on Riemannian manifolds that is amenable to analysis in products.We verify that this approach recovers the correct signature on reconstruction tasks."

Related to my work:
- Riemannian metric learning
	- They use average distortion. We do not divide by distance
		- Doing that suggests that long distances are less important. It would be interesting to test that this is not the case
	- They actually use applications where one needs the Riemannian structure!},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/732 - Gu - Learning mixed-curvature representations in product spaces.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {732},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {read},
}

@InProceedings{skopek2020mixed,
  author         = {Skopek, Ondrej and Ganea, Octavian-Eugen and B{\'e}cigneul, Gary},
  booktitle      = {8th International Conference on Learning Representations (ICLR 2020)(virtual)},
  title          = {Mixed-curvature variational autoencoders},
  year           = {2020},
  organization   = {International Conference on Learning Representations},
  comment        = {[703]: "Recent work proposes to combinedifferent curvatures through several layers (Chami et al.,2019; Bachmann et al., 2020; Grattarola et al., 2020), toenrich the geometry by considering Cartesian products ofspaces (Gu et al., 2019; Tifrea et al., 2019; Skopek et al.,2020),"

[703]: "In this work, we propose the systematic use of symmetricspaces in representation learning: this is a class comprisingall the aforementioned spaces"},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/733 - Skopek - Mixed-curvature variational autoencoders.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {733},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {skimmed},
}

@Article{walter2004h,
  author         = {Walter, J{\"o}rg A},
  journal        = {Information systems},
  title          = {H-MDS: a new approach for interactive visualization with multidimensional scaling in the hyperbolic space},
  year           = {2004},
  number         = {4},
  pages          = {273--292},
  volume         = {29},
  comment        = {[712]: "The H-MDS method proposed in [1] is a gradient descent scheme for minimizing (stress) based onexplicit calculation of the gradient"},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/736 - Walter - H-MDS a new approach for interactive visualization with multidimensional scaling in the hyperbolic space.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {736},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {skimmed},
}

@Article{sammon1969nonlinear,
  author         = {Sammon, John W},
  journal        = {IEEE Transactions on computers},
  title          = {A nonlinear mapping for data structure analysis},
  year           = {1969},
  number         = {5},
  pages          = {401--409},
  volume         = {100},
  comment        = {Main ideas:
- Given a data set compute all distances between all points and embed these in low dimensional euclidean space

Comment:
- Seminal work non-linear mapping},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/738 - Sammon - A nonlinear mapping for data structure analysis.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Highly cited, Riemannian manifold learning with extras, RML Motivation non-linear embedding, RML Motivation symmetric space embedding},
  id             = {738},
  publisher      = {Ieee},
  qualityassured = {qualityAssured},
  ranking        = {rank3},
  readstatus     = {skimmed},
}

@Article{sonthalia2020tree,
  author         = {Sonthalia, Rishi and Gilbert, Anna},
  journal        = {Advances in Neural Information Processing Systems},
  title          = {Tree! i am no tree! i am a low dimensional hyperbolic embedding},
  year           = {2020},
  pages          = {845--856},
  volume         = {33},
  comment        = {Main ideas:
- Before hyperbolic embedding, first find a metric and then find a suitable hyperbolic space

Related to my work:
- MvTD:
	- Motivation hyperbolic embedding},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/752 - Sonthalia - Tree i am no tree i am a low dimensional hyperbolic embedding.pdf:PDF},
  groups         = {Manifold-valued tensor decompositions and approximations, MvTD motivation, Riemannian manifold learning with extras, RML Motivation symmetric space embedding},
  id             = {752},
  qualityassured = {qualityAssured},
  ranking        = {rank4},
  readstatus     = {skimmed},
}

@Article{nickel2017poincare,
  author         = {Nickel, Maximillian and Kiela, Douwe},
  journal        = {Advances in neural information processing systems},
  title          = {Poincar{\'e} embeddings for learning hierarchical representations},
  year           = {2017},
  volume         = {30},
  comment        = {Related to my work
- MvTD:
	- Early work hyperbolic embeddings},
  file           = {:/Users/wdiepeveen/Documents/PhD/References/764 - Nickel - Poincare embeddings for learning hierarchical representations.pdf:PDF},
  groups         = {MvTD motivation, Manifold-valued tensor decompositions and approximations, Highly cited, RML Motivation symmetric space embedding},
  id             = {764},
  qualityassured = {qualityAssured},
  ranking        = {rank2},
  readstatus     = {skimmed},
}

@Article{qin2020blind,
  author     = {Qin, Jing and Lee, Harlin and Chi, Jocelyn T and Drumetz, Lucas and Chanussot, Jocelyn and Lou, Yifei and Bertozzi, Andrea L},
  journal    = {IEEE Transactions on Geoscience and Remote Sensing},
  title      = {Blind hyperspectral unmixing based on graph total variation regularization},
  year       = {2020},
  number     = {4},
  pages      = {3338--3351},
  volume     = {59},
  comment    = {Main ideas:
- Use graph Laplacian to construct graphTV regularizer for some variant of NMF for hyperspectral unmixing

Comment:
- The graph regularizer is the only place atm where they use the geometry and this also seems to be the place where they need to put in the most effort to get things computationally feasible

Related to my work:
- Manifold learning with extras
	- Potential application of framework + what we have developed with Deanna
		- Could set up semi NMF on learned manifold so that we don't need the graph regularizer anymore},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/765 - Qin - Blind hyperspectral unmixing based on graph total variation regularization.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Motivation data analysis tools},
  id         = {765},
  publisher  = {IEEE},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{lopez2022gd,
  author     = {Lopez, Ryan and Atzberger, Paul J},
  journal    = {arXiv preprint arXiv:2206.05183},
  title      = {GD-VAEs: Geometric Dynamic Variational Autoencoders for Learning Nonlinear Dynamics and Dimension Reductions},
  year       = {2022},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/800 - Lopez - GD-VAEs Geometric Dynamic Variational Autoencoders for Learning Nonlinear Dynamics and Dimension Reductions.pdf:PDF},
  groups     = {General interest, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {800},
  readstatus = {skimmed},
}

@Article{younes2018diffeomorphic,
  author  = {Younes, Laurent},
  journal = {arXiv preprint arXiv:1806.01240},
  title   = {Diffeomorphic learning},
  year    = {2018},
  file    = {:/Users/wdiepeveen/Documents/PhD/References/805 - Younes - Diffeomorphic learning.pdf:PDF},
  groups  = {Riemannian manifold learning with extras, RML Research},
  id      = {805},
}

@Article{diepeveen2023curvature,
  author     = {Diepeveen, Willem and Chew, Joyce and Needell, Deanna},
  journal    = {arXiv preprint arXiv:2306.00507},
  title      = {Curvature corrected tangent space-based approximation of manifold-valued data},
  year       = {2023},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/817 - Diepeveen - Curvature corrected tangent space-based approximation of manifold-valued data.pdf:PDF},
  groups     = {Motivation, Riemannian geometry for efficient protein conformation analysis, Own work, RML Motivation symmetric spaces, RML Motivation Riemannian geometry},
  id         = {817},
  readstatus = {read},
}

@Article{chari2023specious,
  author     = {Chari, Tara and Pachter, Lior},
  journal    = {PLOS Computational Biology},
  title      = {The specious art of single-cell genomics},
  year       = {2023},
  number     = {8},
  pages      = {e1011288},
  volume     = {19},
  comment    = {Main ideas:
- Many nonlinear dimension reduction methods do not keep important features preserved such as local/global geometry and distances
- Many downstream tasks suffer from this
- It might be a better idea not to attach any meaning to the visuals and do data analysis in the ambient space as much as possible.

Future work:
- I-?: if RAE works, this paper is motivation to apply ideas to genomics data},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/835 - Chari - The specious art of single-cell genomics.pdf:PDF},
  groups     = {General interest, Ideas I-xyz, RML Motivation non-linear embedding},
  id         = {835},
  publisher  = {Public Library of Science San Francisco, CA USA},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{diepeveen2023riemannian,
  author     = {Diepeveen, Willem and Esteve-Yag{\"u}e, Carlos and Lellmann, Jan and {\"O}ktem, Ozan and Sch{\"o}nlieb, Carola-Bibiane},
  journal    = {arXiv preprint arXiv:2308.07818},
  title      = {Riemannian geometry for efficient analysis of protein dynamics data},
  year       = {2023},
  groups     = {Riemannian manifold learning with extras, RML Motivation Riemannian geometry},
  id         = {839},
  readstatus = {read},
}

@Article{kaya2019deep,
  author     = {Kaya, Mahmut and Bilge, Hasan {\c{S}}akir},
  journal    = {Symmetry},
  title      = {Deep metric learning: A survey},
  year       = {2019},
  number     = {9},
  pages      = {1066},
  volume     = {11},
  comment    = {Main ideas:
- Survey different deep learning-based metric learning algorithms

Related to my work:
- RML
	- The siamese net architecture is basically what we do, but now we have diffeomorphisms
	- Siamese networks are also working famously well, which is not surprising},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/846 - Kaya - Deep metric learning A survey.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, IsSurvey, RML Motivation data analysis tools, RML Research},
  id         = {846},
  publisher  = {MDPI},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{scholkopf1998nonlinear,
  author     = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  journal    = {Neural computation},
  title      = {Nonlinear component analysis as a kernel eigenvalue problem},
  year       = {1998},
  number     = {5},
  pages      = {1299--1319},
  volume     = {10},
  comment    = {Main ideas:
- Map data into an embedding space with a kernel first and do PCA later on the covariance matrix to get an embedding
- Given any algorithm that can be expressedsolely in terms of dot products (i.e., without explicit usage of the variablesthemselves), this kernel method enables us to construct different nonlinear versions of it

Related to my work:
- RML:
	- Background},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/849 - Schoelkopf - Nonlinear component analysis as a kernel eigenvalue problem.pdf:PDF},
  groups     = {RML Motivation non-linear embedding},
  id         = {849},
  publisher  = {MIT Press},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@Article{hastie1989principal,
  author     = {Hastie, Trevor and Stuetzle, Werner},
  journal    = {Journal of the American Statistical Association},
  title      = {Principal curves},
  year       = {1989},
  number     = {406},
  pages      = {502--516},
  volume     = {84},
  comment    = {Main ideas:
- Principal curves are smooth one-dimensional curves that pass through the middle of a p-dimensional data set, providing a nonlinear summary of the data. They are nonparametric, and their shape is suggested by

Related to my work:
- RML: 
	- Background manifold learning
		- Does not really learn an embedding, but is an example where we don't get something parametrized

Future research:
- If we want to learn geodesics from noisy data in an RML setting, we could get inspiration from this paper how to construct loss functions such that geodesics are best fits.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/850 - Hastie - Principal curves.pdf:PDF},
  groups     = {RML Motivation non-linear embedding, Ideas I-xyz},
  id         = {850},
  publisher  = {Taylor \& Francis},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{roweis2000nonlinear,
  author     = {Roweis, Sam T and Saul, Lawrence K},
  journal    = {science},
  title      = {Nonlinear dimensionality reduction by locally linear embedding},
  year       = {2000},
  number     = {5500},
  pages      = {2323--2326},
  volume     = {290},
  comment    = {Main ideas:
- Propose LLE},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/851 - Roweis - Nonlinear dimensionality reduction by locally linear embedding.pdf:PDF},
  groups     = {RML Motivation non-linear embedding},
  id         = {851},
  publisher  = {American Association for the Advancement of Science},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@Article{demers1992non,
  author     = {DeMers, David and Cottrell, Garrison},
  journal    = {Advances in neural information processing systems},
  title      = {Non-linear dimensionality reduction},
  year       = {1992},
  volume     = {5},
  comment    = {Main ideas:
- Auto-encoder (very early work on it anyways)

Related to my work:
- RML:
	- Background},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/852 - DeMers - Non-linear dimensionality reduction.pdf:PDF},
  groups     = {RML Motivation non-linear embedding},
  id         = {852},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{kingma2013auto,
  author     = {Kingma, Diederik P and Welling, Max},
  journal    = {arXiv preprint arXiv:1312.6114},
  title      = {Auto-encoding variational bayes},
  year       = {2013},
  comment    = {Main ideas:
- Propose a way to sample from a latent variable model
- Propose VAE as an application

Comments:
- Initially designed for sampling, but also used for non-linear embedding due to the regularisating effect of the sampling},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/853 - Kingma - Auto-encoding variational bayes.pdf:PDF},
  groups     = {RML Motivation non-linear embedding, RML Motivation Riemannian geometry, Riemannian manifold learning with extras},
  id         = {853},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{lawrence2005probabilistic,
  author     = {Lawrence, Neil and Hyv{\"a}rinen, Aapo},
  journal    = {Journal of machine learning research},
  title      = {Probabilistic non-linear principal component analysis with Gaussian process latent variable models.},
  year       = {2005},
  number     = {11},
  volume     = {6},
  comment    = {Main idea:
- Proposes Gaussian latent variable model

Comment:
- This method does allow a non-linear mapping from latent space to data space 
	- Unlike MDS-based mappings
- And can do that in a non-linear way
	- Unlike normal PCA},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/854 - Lawrence - Probabilistic non-linear principal component analysis with Gaussian process latent variable models.pdf:PDF},
  groups     = {RML Motivation non-linear embedding, RML Motivation Riemannian geometry, Riemannian manifold learning with extras},
  id         = {854},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{van2008visualizing,
  author     = {Van der Maaten, Laurens and Hinton, Geoffrey},
  journal    = {Journal of machine learning research},
  title      = {Visualizing data using t-SNE.},
  year       = {2008},
  number     = {11},
  volume     = {9},
  comment    = {Main ideas:
- t-SNE: construct transition probabilities for all data points and find embedding that has the same transition kernel from a t-distributed conditional probability

Comments:
- authors say that t-distribution is chosen for computational reasons},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/855 - Van der Maaten - Visualizing data using t-SNE.pdf:PDF},
  groups     = {RML Motivation non-linear embedding},
  id         = {855},
  readstatus = {skimmed},
}

@Article{schonsheck2019chart,
  author     = {Schonsheck, Stefan and Chen, Jie and Lai, Rongjie},
  journal    = {arXiv preprint arXiv:1912.10094},
  title      = {Chart auto-encoders for manifold structured data},
  year       = {2019},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/856 - Schonsheck - Chart auto-encoders for manifold structured data.pdf:PDF},
  groups     = {RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {856},
  readstatus = {skimmed},
}

@InProceedings{louis2019riemannian,
  author       = {Louis, Maxime and Couronn{\'e}, Rapha{\"e}l and Koval, Igor and Charlier, Benjamin and Durrleman, Stanley},
  booktitle    = {Information Processing in Medical Imaging: 26th International Conference, IPMI 2019, Hong Kong, China, June 2--7, 2019, Proceedings 26},
  title        = {Riemannian geometry learning for disease progression modelling},
  year         = {2019},
  organization = {Springer},
  pages        = {542--553},
  file         = {:/Users/wdiepeveen/Documents/PhD/References/857 - Louis - Riemannian geometry learning for disease progression modelling.pdf:PDF},
  groups       = {Riemannian manifold learning with extras, RML RW push forward immersion, RML Related work Finding a manifold and Riemannian geometry},
  id           = {857},
  readstatus   = {read},
}

@Article{hauberg2012geometric,
  author     = {Hauberg, S{\o}ren and Freifeld, Oren and Black, Michael},
  journal    = {Advances in Neural Information Processing Systems},
  title      = {A geometric take on metric learning},
  year       = {2012},
  volume     = {25},
  comment    = {Main ideas:
- Also compute Riemannian manifolds, but from only a couple of inner product matrices that vary only through combinations of positive weight functions

Comments:
- Metric learned from Large Margin Nearest Neighbour:
	- "We then cluster the body shapes into equal-sized clusters accordingto the measurement and learn a LMNN metric for each cluster [5], which we associate with themean of each class. These push the clusters apart, which introduces variance along the directionswhere the measurement changes"
- Figure 4 clearly shows that the learned distances are doing better than the Euclidean case
	- Pick up on low-dimensionality
- This procedure is intractableas the dimension of the data increases.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/858 - Hauberg - A geometric take on metric learning.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a Riemannian geometry on all of space},
  id         = {858},
  readstatus = {skimmed},
}

@InProceedings{cuzzolin2008learning,
  author     = {Cuzzolin, Fabio},
  booktitle  = {The 1st International Workshop on Machine Learning for Vision-based Motion Analysis-MLVMA'08},
  title      = {Learning pullback metrics for linear models},
  year       = {2008},
  comment    = {Main ideas:
- Also pull back the metric through some diffeomorphism
- application to dynamics classification

Comment:
- have a very strict small (only 3) parameter space of diffeomorphisms
- Manifold mapped into realistically makes mainly sense for the specific application (and e.g. as done in [872])
- It is here also used that we have closed-form geodesics},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/859 - Cuzzolin - Learning pullback metrics for linear models.pdf:PDF},
  groups     = {RML Related work Finding a Riemannian geometry on all of space, Riemannian manifold learning with extras},
  id         = {859},
  readstatus = {skimmed},
}

@Article{hauberg2018only,
  author     = {Hauberg, S{\o}ren},
  journal    = {arXiv preprint arXiv:1806.04994},
  title      = {Only bayes should learn a manifold (on the estimation of differential geometric structure from data)},
  year       = {2018},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/860 - Hauberg - Only bayes should learn a manifold (on the estimation of differential geometric structure from data).pdf:PDF},
  groups     = {RML RW chart based, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry},
  id         = {860},
  readstatus = {skimmed},
}

@Article{lou2021learning,
  author     = {Lou, Aaron and Nickel, Maximilian and Mukadam, Mustafa and Amos, Brandon},
  title      = {Learning Complex Geometric Structures from Data with Deep Riemannian Manifolds},
  year       = {2021},
  comment    = {Comments:
- They do not use the closed-form geodesics you can get from pull-back geometry
- I don't think the theoretical results are correct. No way that they can reproduce positive curvature with this network.},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/861 - Lou - Learning Complex Geometric Structures from Data with Deep Riemannian Manifolds.pdf:PDF},
  groups     = {RML Related work Finding a Riemannian geometry on all of space, Riemannian manifold learning with extras},
  id         = {861},
  priority   = {prio1},
  readstatus = {skimmed},
}

@Article{bromley1993signature,
  author     = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S{\"a}ckinger, Eduard and Shah, Roopak},
  journal    = {Advances in neural information processing systems},
  title      = {Signature verification using a "siamese" time delay neural network},
  year       = {1993},
  volume     = {6},
  comment    = {Comment:
- Proposed the siamese network for metric learning

Related to my work:
- RML:	
	- We stay in the siamese network tradition
	- Do we want to mention it in the intro though or just later on in the paper?},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/864 - Bromley - Signature verification using a siamese time delay neural network.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Research, RML Motivation non-linear embedding},
  id         = {863},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{tosi2014metrics,
  author     = {Tosi, Alessandra and Hauberg, S{\o}ren and Vellido, Alfredo and Lawrence, Neil D},
  journal    = {arXiv preprint arXiv:1411.7432},
  title      = {Metrics for probabilistic geometries},
  year       = {2014},
  comment    = {Main ideas:
- Early work where geometry is pulled back from latent space to enable Riemannian geometry in latent space

Comments:
- here we do have that we can also use uncertainty to avoid getting standard euclidean pull-back geometry
- Training might be expensive},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/866 - Tosi - Metrics for probabilistic geometries.pdf:PDF},
  groups     = {RML Motivation Riemannian geometry, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW chart based},
  id         = {866},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{bishop1998gtm,
  author     = {Bishop, Christopher M and Svens{\'e}n, Markus and Williams, Christopher KI},
  journal    = {Neural computation},
  title      = {GTM: The generative topographic mapping},
  year       = {1998},
  number     = {1},
  pages      = {215--234},
  volume     = {10},
  comment    = {Main ideas:
- proposes GTM: a probablistic latent variable model

Comment:
- Has embedding to data space mapping
- Need basis functions on latent space and some linear mapping + nonlinearity to data to data space
- Have to sample in EM algorithm. So might be pricy},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/867 - Bishop - GTM The generative topographic mapping.pdf:PDF},
  groups     = {RML Motivation non-linear embedding, RML Motivation Riemannian geometry, Riemannian manifold learning with extras},
  id         = {867},
  publisher  = {MIT Press},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@Article{peltonen2004improved,
  author     = {Peltonen, Jaakko and Klami, Arto and Kaski, Samuel},
  journal    = {Neural Networks},
  title      = {Improved learning of Riemannian metrics for exploratory analysis},
  year       = {2004},
  number     = {8-9},
  pages      = {1087--1100},
  volume     = {17},
  comment    = {Main ideas:
- Remetrize all of space under the Fisher information as tensor field.
- Approximate geodesics

Comments:
- Not clear whether we actually get the low-dimensionality we hope for...

Comment:
- Approximations are fairly simplistic
	- They assume straight lines or we need to solve a high-dimensional optimisation problem

Related to my work:
- RML:
	- also remetrize all of space, but copes with computability issues
		- Early work
	- also unclear whether the geodsics under this geometry actually give useful behaviour
		- as in, that geodesics go through the data set etc and that data looks low-dim},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/868 - Peltonen - Improved learning of Riemannian metrics for exploratory analysis.pdf:PDF},
  groups     = {RML Related work Finding a Riemannian geometry on all of space, Riemannian manifold learning with extras},
  id         = {868},
  publisher  = {Elsevier},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Misc{Yao2023,
  author        = {Zhigang Yao and Jiaji Su and Bingjie Li and Shing-Tung Yau},
  title         = {Manifold Fitting},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {Comments:
- Addresses only the first geometric Whitney problem from [644]},
  eprint        = {2304.07680},
  file          = {:PhD/References/870 - Yao - Manifold Fitting.pdf:PDF},
  groups        = {RML RW point cloud based, Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry},
  id            = {870},
  primaryclass  = {math.ST},
  readstatus    = {skimmed},
}

@Article{Fefferman2016,
  author     = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  journal    = {Journal of the American Mathematical Society},
  title      = {Testing the manifold hypothesis},
  year       = {2016},
  number     = {4},
  pages      = {983--1049},
  volume     = {29},
  comment    = {Main ideas:
- Develop tests for checking whether there is a manifold},
  file       = {:PhD/References/871 - Fefferman - Testing the manifold hypothesis.pdf:PDF},
  groups     = {Riemannian manifold learning with extras, RML Related work Finding a manifold and Riemannian geometry, RML RW point cloud based},
  id         = {871},
  readstatus = {skimmed},
}

@Article{Lebanon2006,
  author     = {Lebanon, Guy},
  journal    = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title      = {Metric learning for text documents},
  year       = {2006},
  number     = {4},
  pages      = {497--508},
  volume     = {28},
  comment    = {Main ideas:
- Also pull back the metric through some diffeomorphism
- application to text},
  file       = {:PhD/References/872 - Lebanon - Metric learning for text documents.pdf:PDF},
  groups     = {RML Related work Finding a Riemannian geometry on all of space, RML Research, Riemannian manifold learning with extras},
  id         = {872},
  publisher  = {IEEE},
  readstatus = {skimmed},
}

@Article{rumpf2015variational,
  author     = {Rumpf, Martin and Wirth, Benedikt},
  journal    = {IMA Journal of Numerical Analysis},
  title      = {Variational time discretization of geodesic calculus},
  year       = {2015},
  number     = {3},
  pages      = {1011--1046},
  volume     = {35},
  comment    = {Main ideas:
- given a Riemannian manifold, construct discrete geodesics in a particular way 
	- needs a local version of the distance that is equivalent to the manifold distance there
- show that the geodesics gamma-converge to the real geodesics

Comments:
- also works for infinite dimensional manifolds, which is really dope

Related work:
- MbPD:
	- even if we don;t have geodesics such as [560], we can still do it with such works
	- however, having everything in closed form is much appreciated},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/568 - Rumpf - Variational time discretization of geodesic calculus.pdf:PDF},
  groups     = {Manifold-guided protein deformations, Cryo-EM, Riemannian geometry of proteins and point clouds, Related work, Riemannian geometry for efficient protein conformation analysis},
  id         = {568},
  publisher  = {OUP},
  ranking    = {rank5},
  readstatus = {skimmed},
}

@Book{boothby2003introduction,
  author    = {Boothby, William M },
  publisher = {Gulf Professional Publishing},
  title     = {An introduction to differentiable manifolds and Riemannian geometry, Revised},
  year      = {2003},
  volume    = {120},
  file      = {:/Users/wdiepeveen/Documents/PhD/References/382 - Boothby - An introduction to differentiable manifolds and Riemannian geometry.pdf:PDF},
  groups    = {IsBook},
  id        = {382},
}

@InCollection{lee2013smooth,
  author     = {Lee, John M},
  booktitle  = {Introduction to Smooth Manifolds},
  publisher  = {Springer},
  title      = {Smooth manifolds},
  year       = {2013},
  pages      = {1--31},
  file       = {:/Users/wdiepeveen/Documents/PhD/References/314 - Lee - smooth manifolds.pdf:PDF},
  groups     = {Cryo-EM, Manifold-guided protein deformations, Misc, Related work, Riemannian geometry for efficient protein conformation analysis},
  id         = {314},
  readstatus = {skimmed},
}

@Article{bergmann2019recent,
  author    = {Bergmann, Ronny and Laus, Friederike and Persch, Johannes and Steidl, Gabriele},
  journal   = {Handbook of Numerical Analysis},
  title     = {Recent advances in denoising of manifold-valued images},
  year      = {2019},
  pages     = {553--578},
  volume    = {20},
  file      = {:/Users/wdiepeveen/Documents/PhD/References/384 - Bergmann - Recent advances in denoising of manifold-valued images.pdf:PDF},
  groups    = {Manifold-valued Bayesian IPs, In Article, Manifold-valued tensor decompositions and approximations, MvTD motivation, Lifting for SPA, RML Research},
  id        = {384},
  publisher = {Elsevier},
}

@article{clevert2015fast,
  title={Fast and accurate deep network learning by exponential linear units (elus)},
  author={Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:1511.07289},
  year={2015}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014},
  year =	 "1986",
  address =	 "Reading, MA"
}

@article{chen2023riemannian,
  title={Riemannian flow matching on general geometries},
  author={Chen, Ricky TQ and Lipman, Yaron},
  journal={arXiv preprint arXiv:2302.03660},
  year={2023}
}

@InProceedings{Scarvelis2023,
  author    = {Christopher Scarvelis and Justin Solomon},
  booktitle = {The Eleventh International Conference on Learning Representations},
  title     = {Riemannian Metric Learning via Optimal Transport},
  year      = {2023},
  file      = {:Scarvelis2023 - Riemannian Metric Learning Via Optimal Transport.pdf:PDF},
  groups    = {Data-driven Riemannian Geometry},
  priority  = {prio1},
  url       = {https://openreview.net/forum?id=v3y68gz-WEz},
}

@article{bousquet2003measure,
  title={Measure based regularization},
  author={Bousquet, Olivier and Chapelle, Olivier and Hein, Matthias},
  journal={Advances in Neural Information Processing Systems},
  volume={16},
  year={2003}
}

@inproceedings{sajama2005estimating,
  title={Estimating and computing density based distance metrics},
  author={Sajama and Orlitsky, Alon},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={760--767},
  year={2005}
}

@article{bijral2012semi,
  title={Semi-supervised learning with density based distances},
  author={Bijral, Avleen S and Ratliff, Nathan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1202.3702},
  year={2012}
}

@article{hwang2012shortest,
  title={Shortest path through random points},
  author={Hwang, Sung Jin and Damelin, Steven B and Hero III, Alfred O},
  journal={arXiv preprint arXiv:1202.0045},
  year={2012}
}

@inproceedings{chu2020exact,
  title={Exact computation of a manifold metric, via lipschitz embeddings and shortest paths on a graph},
  author={Chu, Timothy and Miller, Gary L and Sheehy, Donald R},
  booktitle={Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={411--425},
  year={2020},
  organization={SIAM}
}

@article{groisman2022nonhomogeneous,
  title={Nonhomogeneous Euclidean first-passage percolation and distance learning},
  author={Groisman, Pablo and Jonckheere, Matthieu and Sapienza, Facundo},
  journal={Bernoulli},
  volume={28},
  number={1},
  pages={255--276},
  year={2022},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@inproceedings{moscovich2017minimax,
  title={Minimax-optimal semi-supervised regression on unknown manifolds},
  author={Moscovich, Amit and Jaffe, Ariel and Boaz, Nadler},
  booktitle={Artificial Intelligence and Statistics},
  pages={933--942},
  year={2017},
  organization={PMLR}
}

@article{alamgir2012shortest,
  title={Shortest path distance in random k-nearest neighbor graphs},
  author={Alamgir, Morteza and Von Luxburg, Ulrike},
  journal={arXiv preprint arXiv:1206.6381},
  year={2012}
}

@article{mckenzie2019power,
  title={Power weighted shortest paths for clustering Euclidean data},
  author={Mckenzie, Daniel and Damelin, Steven},
  journal={arXiv preprint arXiv:1905.13345},
  year={2019}
}

@article{little2020path,
  title={Path-based spectral clustering: Guarantees, robustness to outliers, and fast algorithms},
  author={Little, Anna and Maggioni, Mauro and Murphy, James M},
  journal={Journal of machine learning research},
  volume={21},
  number={6},
  pages={1--66},
  year={2020}
}

@article{fefferman2016testing,
  title={Testing the manifold hypothesis},
  author={Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  journal={Journal of the American Mathematical Society},
  volume={29},
  number={4},
  pages={983--1049},
  year={2016}
}

@inproceedings{
sakamoto2024the,
title={The Geometry of Diffusion Models: Tubular Neighbourhoods and Singularities},
author={Kotaro Sakamoto and Masato Tanabe and Masatomo Akagawa and Yusuke Hayashi and Ryosuke Sakamoto and Manato Yaguchi and Masahiro Suzuki and Yutaka Matsuo},
booktitle={ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling},
year={2024},
url={https://openreview.net/forum?id=YTBE6mJBY7}
}

@inproceedings{
sun2024geometryaware,
title={Geometry-Aware Autoencoders for Metric Learning and Generative Modeling on Data Manifolds},
author={Xingzhi Sun and Danqi Liao and Kincaid MacDonald and Yanlei Zhang and Guillaume Huguet and Guy Wolf and Ian Adelstein and Tim G. J. Rudner and Smita Krishnaswamy},
booktitle={ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling},
year={2024},
url={https://openreview.net/forum?id=EYQZjMcn4l}
}


@article{zhong2021cryodrgn,
  title={CryoDRGN: reconstruction of heterogeneous cryo-EM structures using neural networks},
  author={Zhong, Ellen D and Bepler, Tristan and Berger, Bonnie and Davis, Joseph H},
  journal={Nature methods},
  volume={18},
  number={2},
  pages={176--185},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{gomari2022variational,
  title={Variational autoencoders learn transferrable representations of metabolomics data},
  author={Gomari, Daniel P and Schweickart, Annalise and Cerchietti, Leandro and Paietta, Elisabeth and Fernandez, Hugo and Al-Amin, Hassen and Suhre, Karsten and Krumsiek, Jan},
  journal={Communications Biology},
  volume={5},
  number={1},
  pages={645},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{chow2022predicting,
  title={Predicting drug polypharmacology from cell morphology readouts using variational autoencoder latent space arithmetic},
  author={Chow, Yuen Ler and Singh, Shantanu and Carpenter, Anne E and Way, Gregory P},
  journal={PLoS computational biology},
  volume={18},
  number={2},
  pages={e1009888},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{ternes2022multi,
  title={A multi-encoder variational autoencoder controls multiple transformational features in single-cell image analysis},
  author={Ternes, Luke and Dane, Mark and Gross, Sean and Labrie, Marilyne and Mills, Gordon and Gray, Joe and Heiser, Laura and Chang, Young Hwan},
  journal={Communications biology},
  volume={5},
  number={1},
  pages={255},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{vahdat2020nvae,
  title={NVAE: A deep hierarchical variational autoencoder},
  author={Vahdat, Arash and Kautz, Jan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19667--19679},
  year={2020}
}

@article{lecun2021self,
  title={Self-supervised learning: The dark matter of intelligence},
  author={LeCun, Yann},
  journal={Facebook Research},
  year={2021}
}
@inproceedings{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={GloVe: Global Vectors for Word Representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Amodei, Dario and others},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{oord2018representation,
  title={Representation Learning with Contrastive Predictive Coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={10256--10265},
  year={2018}
}

@inproceedings{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1597--1607},
  year={2020}
}

@inproceedings{he2020momentum,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={9729--9738},
  year={2020}
}

@article{goyal2021self,
  title={Self-supervised Pretraining of Visual Features in the Wild},
  author={Goyal, Priya and Mahajan, Dhruv and Gupta, Abhinav and Misra, Ishan},
  journal={arXiv preprint arXiv:2103.01988},
  year={2021}
}

@inproceedings{schneider2019wav2vec,
  title={wav2vec: Unsupervised Pre-training for Speech Recognition},
  author={Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael and Mohamed, Abdelrahman},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6415--6419},
  year={2019}
}

@inproceedings{baevski2020wav2vec,
  title={wav2vec 2.0: A Framework for Self-supervised Learning of Speech Representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={12449--12460},
  year={2020}
}

@article{radford2021learning,
  title={Learning Transferable Visual Models from Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@article{schwarzer2021pretraining,
  title={Pretraining Representations for Data-Efficient Reinforcement Learning},
  author={Schwarzer, Max and Anand, Ankesh and Goel, Rishabh and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={15812--15824},
  year={2021}
}

@article{hinton2006reducing,
  title={Reducing the Dimensionality of Data with Neural Networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={Science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006}
}

@article{sohl2015deep,
  title={Deep Unsupervised Learning Using Nonequilibrium Thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric A and Maheswaranathan, Niru and Ganguli, Surya},
  journal={arXiv preprint arXiv:1503.03585},
  year={2015}
}

@inproceedings{grill2020bootstrap,
  title={Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={21271--21284},
  year={2020}
}

@article{chen2021exploring,
  title={Exploring Simple Siamese Representation Learning},
  author={Chen, Xinlei and He, Kaiming},
  journal={arXiv preprint arXiv:2011.10566},
  year={2021}
}

@inproceedings{oord2016pixel,
  title={Pixel Recurrent Neural Networks},
  author={Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1747--1756},
  year={2016}
}

@inproceedings{you2018graphrnn,
  title={GraphRNN: Generating Realistic Graphs with Deep Auto-Regressive Models},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L and Leskovec, Jure},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={5708--5717},
  year={2018}
}

@inproceedings{kingma2018glow,
  title={Glow: Generative Flow with Invertible 1x1 Convolutions},
  author={Kingma, Durk P and Dhariwal, Prafulla},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={10215--10224},
  year={2018}
}

@inproceedings{shi2020graphaf,
  title={GraphAF: A Flow-based Autoregressive Model for Molecular Graph Generation},
  author={Shi, Chence and Luo, Maosong and Xu, Minkai and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{vincent2008extracting,
  title={Extracting and Composing Robust Features with Denoising Autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  journal={Proceedings of the 25th International Conference on Machine Learning (ICML)},
  pages={1096--1103},
  year={2008}
}


@article{oord2017neural,
  title={Neural Discrete Representation Learning},
  author={Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={6306--6315},
  year={2017}
}

@inproceedings{van2017neural,
  title={Neural audio synthesis of musical notes with WaveNet autoencoders},
  author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  volume={70},
  pages={1068--1077},
  year={2017}
}

@inproceedings{izmailov2021flowgmm,
  title={Semi-Supervised Learning with Normalizing Flows},
  author={Izmailov, Pavel and Kirichenko, Polina and Finzi, Marc and Wilson, Andrew Gordon},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://arxiv.org/abs/2012.04602}
}

@inproceedings{razavi2019generating,
  title={Generating diverse high-fidelity images with VQ-VAE-2},
  author={Razavi, Ali and van den Oord, Aaron and Vinyals, Oriol},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14866--14876},
  year={2019}
}

@article{ding2021vqvae,
  title={VQ-VAE for multimodal image synthesis},
  author={Ding, Yang and Hu, Hexin and Ge, Zhengliang and Wu, Jiaming and Zhang, Hanzhe and Ding, Chris},
  journal={arXiv preprint arXiv:2103.02398},
  year={2021}
}

@inproceedings{kipf2016variational,
  title={Variational Graph Auto-Encoders},
  author={Kipf, Thomas and Welling, Max},
  booktitle={NIPS Workshop on Bayesian Deep Learning},
  year={2016}
}

@inproceedings{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5753--5763},
  year={2019}
}

@inproceedings{caron2018deep,
  title={Deep Clustering for Unsupervised Learning of Visual Features},
  author={Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={132--149},
  year={2018}
}

@inproceedings{zbontar2021barlow,
  title={Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},
  organization={PMLR}
}

@inproceedings{bardes2021vicreg,
  title={VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  author={Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@inproceedings{caron2020unsupervised,
  title={Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={9912--9924},
  year={2020}
}

@article{tian2022,
  title={Understanding Self-supervised Contrastive Learning with Generalized Contrastive Losses},
  author={Tian, Yuge and others},
  journal={arXiv preprint arXiv:2202.09671},
  year={2022}
}

@inproceedings{pathak2016context,
  title={Context encoders: Feature learning by inpainting},
  author={Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2536--2544},
  year={2016}
}

@inproceedings{bao2021beit,
  title={BEiT: BERT pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural Computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}


@article{lecun2006tutorial,
  title={A Tutorial on Energy-Based Learning},
  author={LeCun, Yann and Chopra, Sumit and Hadsell, Raia},
  journal={Predicting Structured Data},
  year={2006},
  publisher={MIT Press}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xiangyu and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{xie2022simmim,
  title={SimMIM: A simple framework for masked image modeling},
  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zheng and Dai, Qi and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9653--9663},
  year={2022}
}

@article{achilli2024losing,
  title={Losing dimensions: Geometric memorization in generative diffusion},
  author={Achilli, Beatrice and Ventura, Enrico and Silvestri, Gianluigi and Pham, Bao and Raya, Gabriel and Krotov, Dmitry and Lucibello, Carlo and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2410.08727},
  year={2024}
}

@inproceedings{kamkari2024geometric,
  title={A Geometric Explanation of the Likelihood OOD Detection Paradox},
  author={Kamkari, Hamidreza and Ross, Brendan Leigh and Cresswell, Jesse C. and Caterini, Anthony L. and Krishnan, Rahul G. and Loaiza-Ganem, Gabriel},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  series={Proceedings of Machine Learning Research},
  volume={235},
  year={2024},
  publisher={PMLR},
  address={Vienna, Austria},
  url={https://github.com/layer6ai-labs/dgm_ood_detection},
  note={Copyright 2024 by the author(s).}
}

@inproceedings{kiani2024hardness,
  title     = {Hardness of Learning Neural Networks under the Manifold Hypothesis},
  author    = {Kiani, Bobak T. and Wang, Jason and Weber, Melanie},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2024},
  note      = {Spotlight Presentation},
  url       = {https://arxiv.org/abs/2406.01461},
}





