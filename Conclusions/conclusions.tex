%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Conclusions *********************************
%*******************************************************************************

\chapter{Conclusions and Outlook}

In this thesis, we have explored several novel directions in generative modeling and generative self-supervised learning. We began with conditional multiscale autoregressive modeling, implemented using conditional normalizing flows. Building on this foundation and inspired by the efficiency and effectiveness of multiscale normalizing flows, we developed non-uniform diffusion models to replicate similar dynamics within the more powerful and rapidly emerging diffusion framework.

At that time, diffusion models began capturing our research interest, leading us to explore various intriguing ideas, such as faster sampling techniques and improving the training efficiency of Schrödinger bridges, a more general class of diffusion models. While these efforts met with partial success, they opened the door to an unexpected insight that became a turning point in our research: diffusion models inherently capture the geometry of the data manifold through the approximation of the score function. 

This realization reshaped our research focus and deepened our understanding of the capabilities of generative models. We recognized that generative models like diffusion models—which do not rely on explicit assumptions about the latent structure, as VAEs or GANs do—can potentially uncover the intrinsic geometry of the data manifold. This pivotal insight fostered a profound connection between generative modeling and self-supervised learning, demonstrating that generative models can serve not only as tools for data generation but also as frameworks for understanding the underlying structure of data.

Building on this turning point, we proved that diffusion models approximate the normal bundle of the data manifold and introduced a novel method for estimating the intrinsic dimension of data manifolds. We further enhanced the modeling flexibility of vanilla $\beta$-VAEs by using a pretrained diffusion model in combination with the encoder to model the reconstruction distribution more effectively.

Finally, we introduced a new paradigm in generative modeling that goes beyond generating samples from the data distribution. This paradigm focuses on learning the underlying geometry of the data manifold, establishing a broader and more comprehensive perspective on the role of generative models.

In Chapter 3, we present \emph{CAFLOW}, a model that improves the flexibility and expressive power of conditional flows for image-to-image translation by introducing autoregressive modeling in a scale space derived from unconditional normalizing flows. Instead of factorizing the distribution at the pixel level, CAFLOW does so across multiple scales, conditioning higher frequency details of the target image on the corresponding lower frequency content from both the target and the conditioning images. This hierarchical, frequency-based conditioning strategy leads to more effective generative modeling than standard conditional flows, as demonstrated in our experiments. Interestingly, a similar concept of autoregressive factorization in scale space has recently been explored by Tian et al. (2024) in their work on Visual Autoregressive Modeling \cite{tian2024visual}. Using powerful vision transformer architectures, their approach employs a next-scale prediction strategy and has achieved state-of-the-art performance on ImageNet at a resolution of $256\times256$.

In Chapter 4, we introduce \emph{non-uniform diffusion models}, which apply a non-uniform noising schedule to different parts or scales of an image. By accelerating the diffusion of higher-frequency details while diffusing lower-frequency components more gradually, we obtain a multi-scale diffusion structure that mirrors the hierarchical design of multi-scale normalizing flows. This approach not only delivers superior image quality compared to standard uniform diffusion models, but also achieves significantly faster sampling—up to 4.4 times faster at a resolution of $128\times128$. We expect even greater speed-ups at higher resolutions, and believe that re-implementing this idea in modern, optimized diffusion codebases could challenge current state-of-the-art performance in image generation and sampling speed if combined with distillation methods.

Furthermore, we use non-uniform diffusion to introduce a new estimator for the conditional score—referred to here as the Conditional Multi-Speed Diffusive Estimator (CMDE), which matches the performance of the widely adopted Conditional Denoising Estimator (CDE). To our knowledge, we provide the first theoretical proof of consistency for the CDE estimator, establishing a solid foundation that both justifies and reinforces the empirical confidence previously placed in it. Lastly, we present a detailed comparison of different methodologies for learning conditional probability distributions using score-based diffusion models, offering insights that can guide future research and practical implementations.

In Chapter 6, we introduce \emph{ScoreVAE}, a novel approach that advances the Variational Autoencoder (VAE) framework by addressing fundamental limitations of conventional VAEs. Traditional VAEs model the reconstruction distribution $p(x|z)$ as a Gaussian, which often leads to overly smoothed and blurry reconstructions. This limitation arises because the Gaussian assumption fails to capture the complexity and multimodality of real-world data distributions, making it difficult for the model to accurately represent intricate details and sharp features. Our method proposes a more flexible parametrization for the reconstruction distribution $p(x|z)$ that is derived from the combination of a frozen pretrained diffusion model and the learnable diffusion-time dependent encoder. Our approach also simplifies the overall training process, as it decouples the encoder from the diffusion model, allowing for flexible integration of more powerful diffusion priors without requiring full retraining which is required in diffusion decoders \cite{preechakul2022diffusion_decoder}. Future promising research directions involve scaling to higher image resolutions and explore its applicability to other data modalities such as molecular structures where powerful pretrained diffusion models are publicly available. Acquiring high-quality latent representations of such complex data could enable meaningful manipulations and more refined, controllable generative modeling, further broadening the scope and impact of ScoreVAE in the domain of representation learning.

In Chapter 5, we prove that diffusion models encode data manifolds by approximating their normal bundles. Based on this result, we propose a novel method to estimate the local intrinsic dimension (LID) of data manifolds using a trained diffusion model. This work has opened many research avenues that the community is currently exploring.

For instance, \citet{achilli2024losing} leverage the LID estimates obtained using our method to investigate the degree of memorization in trained diffusion models. \citet{kamkari2024geometric} use the LID estimates derived from our method to achieve state-of-the-art performance in out-of-distribution (OOD) detection. They demonstrate that likelihood-based generative models can assign high likelihood to OOD samples confined to low-dimensional manifolds and address this by pairing the likelihood with LID estimates obtained from a pretrained diffusion model. This pairing robustifies OOD detection and sets a new benchmark in performance. Additionally, \citet{kiani2024hardness} employ our LID estimation method to characterize real image manifolds as part of their investigation into the hardness of learning neural networks under the manifold hypothesis. Their findings conclude that these manifolds reside within the potentially learnable regime of their proposed framework, further showcasing the utility of our LID estimation method.

Inspired by the theoretical insights of Chapter 5, in Chapter 7 we propose a data-driven pullback metric based on the pseudo score function. The pseudo score function contains the same geometrical information as the score function while also being a diffeomorphism, which enables us to define a well-defined pullback Riemannian metric on the data manifold. Under certain assumptions on the form of the data distribution, this metric allows us to derive closed-form expressions for fundamental geometric constructs, such as geodesics, distances, exponential and logarithmic maps. In addition to offering closed-form expressions for manifold maps, the constructed Riemannian structure yields a Riemannian Auto-encoder (RAE) that simultaneously learns the intrinsic dimension \( k \) of the data manifold, a global chart mapping from the data manifold \(\mathcal{M}\) to \(\mathbb{R}^k\) (the encoder), and the inverse chart mapping from \(\mathbb{R}^k\) back to \(\mathcal{M}\) (the decoder).

We implement the score-based pullback Riemannian geometry framework with Normalizing Flows. We introduce two modifications that are theoretically motivated by our analysis. We replace the fixed isotropic Gaussian base distribution with Gaussian that has a learnable covariance matrix and introduce a regularization term to keep the Normalizing Flow as close as possible to an isometry. These modifications encourage the flow to align its latent space with the true geometry of the data manifold, effectively learning an isometric embedding of the manifold into a Euclidean latent space. As a result, the learned manifold maps are more stable, more accurate, and provide a natural route to constructing a Riemannian Auto-encoder. Latent dimensions associated with high learned variance capture meaningful on-manifold degrees of freedom, while those with negligible variance represent non-informative directions and are effectively collapsed. The Normalizing Flow essentially transforms into a Riemannian Auto-encoder that captures the intrinsic dimension of the data manifold, provides the chart (as the forward map), the inverse chart (as the reverse map) and additionally provides closed form expressions for the manifold maps. 


Our theory and practical implementation based on Normalizing Flows are currently guaranteed to work reliably for unimodal data distributions that are supported on or near a manifold. Interestingly, our method also performs well for multimodal data distributions. When applied to distributions that fall outside the unimodal assumption, we observe that the Riemannian Auto-encoder (RAE) tends to slightly overestimate the intrinsic dimension but still delivers good reconstructions. For instance, on the MNIST dataset, our RAE identifies 176 latent dimensions, while diffusion-based methods estimate the maximum local intrinsic dimension at approximately 152.

Looking ahead, two key avenues of exploration stand out as particularly promising for driving this work forward. The first involves refining our approach under the unimodal assumption by employing more expressive diffeomorphisms—such as rational quadratic splines \cite{durkan2019neural} and free-form flows \cite{draxler2024free}—or by adapting the framework to continuous dynamical system-based methods like Flow Matching or Schrödinger bridges. These refinements have the potential to significantly improve the performance and the scalability of the framework.

The second direction focuses on developing a comprehensive theory and methodology for handling multimodal data distributions. While our current implementation, designed under the unimodal assumption, already performs well in many multimodal scenarios, achieving consistently excellent performance in such settings requires an adaptation of the framework. Advancing this direction entails establishing a rigorous theoretical framework for geometric extraction in multimodal settings and designing an efficient, practical implementation capable of accurately capturing and leveraging the intricate geometry of complex, multimodal data distributions. These advancements will ensure the framework is robust and scalable across a broader range of data distributions.

In conclusion, this thesis forges new links between generative modeling, Riemannian geometry, and self-supervised learning. By showing how diffusion models capture the intrinsic geometric structure of data manifolds, we introduce a novel geometry-aware perspective for modeling high-dimensional data. Our data-driven pullback metric and Riemannian Auto-encoder allow for principled learning of the geometry of the data manifold. Going forward, refining these methods for both unimodal and multimodal data will lead to more robust, scalable, and geometry-driven approaches to generative modeling.