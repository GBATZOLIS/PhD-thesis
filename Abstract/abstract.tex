% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
    This thesis introduces a novel, geometry-driven perspective on deep generative modeling, encompassing normalizing flows, diffusion models, and self-supervised learning. We begin by developing CAFLOW, a multi-scale, autoregressive conditional normalizing flow that factorizes image distributions across different frequency scales to achieve superior conditional generation. Building upon these insights, we then propose non-uniform diffusion models, whose multi-scale structure both accelerates sampling and enhances fidelity by selectively diffusing high-frequency components more aggressively. A pivotal realization soon emerged: score-based diffusion models inherently encode data manifold geometry by approximating the normal bundle via the score functionâ€”prompting us to devise a method for estimating intrinsic dimension directly from a trained diffusion model. We next tackle the Gaussian-decoding assumption in Variational Autoencoders by introducing ScoreVAE, which uses pretrained diffusion-based decoders to produce sharper reconstructions and a more expressive latent space. Finally, we formalize a data-driven Riemannian geometry framework, adapting normalizing flow training to learn an isometric embedding of the data manifold. This framework provides closed-form geodesics, intrinsic dimensionality estimates, and practical manifold mappings through a Riemannian Auto-Encoder. While certain parametric and unimodality assumptions remain, these contributions collectively illuminate how generative models can transcend sample generation to reveal and exploit manifold geometry, unlocking promising avenues in self-supervised learning, optimization on data manifolds, and beyond.
\end{abstract}
