\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\citation{vae}
\citation{zaho2017understanding_vaes}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Variational Diffusion Auto-encoder}{81}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{81}{section.5.1}\protected@file@percent }
\citation{diffusion_models,ddpm}
\citation{dhariwal2021diffusion_beats_gans}
\citation{kong2020diffWave}
\citation{batzolis2022non_uniform,saharia2021sr3}
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\citation{cifar}
\citation{liu2015celeba}
\citation{vae,rezende2014vae2}
\citation{higgins2016beta_vae}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Background}{83}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Variational Autoencoders}{83}{subsection.5.2.1}\protected@file@percent }
\newlabel{ch4:eq:elbo}{{5.1}{83}{Variational Autoencoders}{equation.5.2.1}{}}
\citation{rybkin2021sigma_vae}
\citation{song2020score}
\citation{score_matching}
\citation{diffusion_models,ddpm}
\citation{anderson1982reverse_time_sde}
\newlabel{ch4:eq:elbo}{{5.2}{84}{Variational Autoencoders}{equation.5.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Score-based diffusion models}{84}{subsection.5.2.2}\protected@file@percent }
\newlabel{ch4:sec:background_score}{{5.2.2}{84}{Score-based diffusion models}{subsection.5.2.2}{}}
\newlabel{ch4:eq:forward_sde}{{5.3}{84}{Score-based diffusion models}{equation.5.2.3}{}}
\newlabel{ch4:eq:reverse_sde}{{5.4}{84}{Score-based diffusion models}{equation.5.2.4}{}}
\citation{song2021maximum}
\citation{score_matching,vincent2011connection,song2020score}
\citation{song2020score}
\citation{vincent2011connection}
\citation{song2021maximum}
\citation{song2021maximum}
\newlabel{ch4:eq:approximated_reverse_sde}{{5.5}{85}{Score-based diffusion models}{equation.5.2.5}{}}
\newlabel{ch4:DSM_for_uniform_diffusion_models}{{5.7}{85}{Score-based diffusion models}{equation.5.2.7}{}}
\citation{kingmaVDM}
\citation{song2020score}
\citation{vincent2011connection,song2020score}
\citation{batzolis2022non_uniform}
\newlabel{ch4:Likelihood_Weighting_for_Uniform_Diffusion_Models}{{5.8}{86}{Score-based diffusion models}{equation.5.2.8}{}}
\newlabel{ch4:eq:conditional_reverse_sde}{{5.9}{86}{Score-based diffusion models}{equation.5.2.9}{}}
\newlabel{ch4:CDE}{{5.10}{86}{Score-based diffusion models}{equation.5.2.10}{}}
\citation{zaho2017understanding_vaes}
\citation{zhang2018lpips}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Graphical overview of our method. The time-dependent encoder network $e_\phi $ induces the encoder distribution $q_\phi ( \textbf  {z} | \textbf  {x}_t, t) \approx p_t( \textbf  {z} | \textbf  {x}_t)$. The data $\textbf  {x}_0$ is encoded with the encoder into a latent vector $\textbf  {z}$ by sampling $q_\phi ( \textbf  {z} | \textbf  {x}_0, 0)$. Then the reconstruction $\hat  {\textbf  {x}}_0$ is obtained by running the conditional reverse diffusion process using the approximate conditional data score $s_{\theta , \phi }(\textbf  {x}_t, \textbf  {z},t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {x}_t | \textbf  {z})}$. The model $s_{\theta , \phi }(\textbf  {x}_t, \textbf  {z},t)$ is obtained by adding the score of unconditional diffusion model $s_\theta (\textbf  {x}_t,t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {x}_t)} $ and the score of the encoder distribution $ \nabla _{\textbf  {x}_t} \ln q_\phi ( \textbf  {z} | \textbf  {x}_t, t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {z} | \textbf  {x}_t )} $. The latter can be computed via automatic differentiation with respect to the input $\textbf  {x}_t$.}}{87}{figure.caption.84}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Method}{87}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Problems with conventional VAEs}{87}{subsection.5.3.1}\protected@file@percent }
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Conditional Diffusion Models as decoders}{88}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Score VAE: Encoder with unconditional diffusion model as prior}{88}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Modeling the latent posterior score $\nabla _{\textbf  {x}_t} \ln {p(\textbf  {z} | \textbf  {x}_t )}$}{89}{subsection.5.3.4}\protected@file@percent }
\newlabel{ch4:eq:true_posterior}{{5.11}{89}{Modeling the latent posterior score $\nabla _{\textbf {x}_t} \ln {p(\textbf {z} | \textbf {x}_t )}$}{equation.5.3.11}{}}
\newlabel{ch4:definition_of_variational_approximation}{{5.12}{89}{Modeling the latent posterior score $\nabla _{\textbf {x}_t} \ln {p(\textbf {z} | \textbf {x}_t )}$}{equation.5.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.5}Encoder Training Objective}{90}{subsection.5.3.5}\protected@file@percent }
\newlabel{ch4:Encoder_Training_Objective}{{5.3.5}{90}{Encoder Training Objective}{subsection.5.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.6}Correction of the variational error}{90}{subsection.5.3.6}\protected@file@percent }
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Experiments}{91}{section.5.4}\protected@file@percent }
\newlabel{ch4:Experiments}{{5.4}{91}{Experiments}{section.5.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Cifar10}}{91}{table.caption.85}\protected@file@percent }
\newlabel{ch4:tbl:Cifar10}{{5.1}{91}{Cifar10}{table.caption.85}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces CelebA $64\times 64$}}{91}{table.caption.85}\protected@file@percent }
\newlabel{ch4:tbl:CelebA}{{5.2}{91}{CelebA $64\times 64$}{table.caption.85}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusions}{91}{section.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Cifar10}}{92}{table.caption.86}\protected@file@percent }
\newlabel{ch4:fig:cifar10_qualitative_comparison}{{5.3}{92}{Cifar10}{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces CelebA $64\times 64$}}{92}{table.caption.87}\protected@file@percent }
\newlabel{ch4:fig:celebA_qualitative_comparison}{{5.4}{92}{CelebA $64\times 64$}{table.caption.87}{}}
\@setckpt{Chapter4/chapter4}{
\setcounter{page}{93}
\setcounter{equation}{12}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{1}
\setcounter{table}{4}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{29}
\setcounter{Hfootnote}{7}
\setcounter{bookmark@seq@number}{75}
\setcounter{theorem}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{1}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALC@unique}{11}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
}
