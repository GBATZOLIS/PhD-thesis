\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\citation{vae}
\citation{zaho2017understanding_vaes}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Variational Diffusion Auto-encoder}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}\protected@file@percent }
\citation{diffusion_models,ddpm}
\citation{dhariwal2021diffusion_beats_gans}
\citation{kong2020diffWave}
\citation{batzolis2022non_uniform,saharia2021sr3}
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\citation{cifar}
\citation{liu2015celeba}
\citation{vae,rezende2014vae2}
\citation{higgins2016beta_vae}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Background}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Variational Autoencoders}{3}{subsection.1.2.1}\protected@file@percent }
\newlabel{ch4:eq:elbo}{{1.1}{3}{Variational Autoencoders}{equation.1.2.1}{}}
\citation{rybkin2021sigma_vae}
\citation{song2020score}
\citation{score_matching}
\citation{diffusion_models,ddpm}
\citation{anderson1982reverse_time_sde}
\newlabel{ch4:eq:elbo}{{1.2}{4}{Variational Autoencoders}{equation.1.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Score-based diffusion models}{4}{subsection.1.2.2}\protected@file@percent }
\newlabel{ch4:sec:background_score}{{1.2.2}{4}{Score-based diffusion models}{subsection.1.2.2}{}}
\newlabel{ch4:eq:forward_sde}{{1.3}{4}{Score-based diffusion models}{equation.1.2.3}{}}
\newlabel{ch4:eq:reverse_sde}{{1.4}{4}{Score-based diffusion models}{equation.1.2.4}{}}
\citation{song2021maximum}
\citation{score_matching,vincent2011connection,song2020score}
\citation{song2020score}
\citation{vincent2011connection}
\citation{song2021maximum}
\citation{song2021maximum}
\newlabel{ch4:eq:approximated_reverse_sde}{{1.5}{5}{Score-based diffusion models}{equation.1.2.5}{}}
\newlabel{ch4:DSM_for_uniform_diffusion_models}{{1.7}{5}{Score-based diffusion models}{equation.1.2.7}{}}
\citation{kingmaVDM}
\citation{song2020score}
\citation{vincent2011connection,song2020score}
\citation{batzolis2022non_uniform}
\newlabel{ch4:Likelihood_Weighting_for_Uniform_Diffusion_Models}{{1.8}{6}{Score-based diffusion models}{equation.1.2.8}{}}
\newlabel{ch4:eq:conditional_reverse_sde}{{1.9}{6}{Score-based diffusion models}{equation.1.2.9}{}}
\newlabel{ch4:CDE}{{1.10}{6}{Score-based diffusion models}{equation.1.2.10}{}}
\citation{zaho2017understanding_vaes}
\citation{zhang2018lpips}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Graphical overview of our method. The time-dependent encoder network $e_\phi $ induces the encoder distribution $q_\phi ( \textbf  {z} | \textbf  {x}_t, t) \approx p_t( \textbf  {z} | \textbf  {x}_t)$. The data $\textbf  {x}_0$ is encoded with the encoder into a latent vector $\textbf  {z}$ by sampling $q_\phi ( \textbf  {z} | \textbf  {x}_0, 0)$. Then the reconstruction $\hat  {\textbf  {x}}_0$ is obtained by running the conditional reverse diffusion process using the approximate conditional data score $s_{\theta , \phi }(\textbf  {x}_t, \textbf  {z},t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {x}_t | \textbf  {z})}$. The model $s_{\theta , \phi }(\textbf  {x}_t, \textbf  {z},t)$ is obtained by adding the score of unconditional diffusion model $s_\theta (\textbf  {x}_t,t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {x}_t)} $ and the score of the encoder distribution $ \nabla _{\textbf  {x}_t} \ln q_\phi ( \textbf  {z} | \textbf  {x}_t, t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {z} | \textbf  {x}_t )} $. The latter can be computed via automatic differentiation with respect to the input $\textbf  {x}_t$.}}{7}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Method}{7}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Problems with conventional VAEs}{7}{subsection.1.3.1}\protected@file@percent }
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Conditional Diffusion Models as decoders}{8}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Score VAE: Encoder with unconditional diffusion model as prior}{8}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Modeling the latent posterior score $\nabla _{\textbf  {x}_t} \ln {p(\textbf  {z} | \textbf  {x}_t )}$}{9}{subsection.1.3.4}\protected@file@percent }
\newlabel{ch4:eq:true_posterior}{{1.11}{9}{Modeling the latent posterior score $\nabla _{\textbf {x}_t} \ln {p(\textbf {z} | \textbf {x}_t )}$}{equation.1.3.11}{}}
\newlabel{ch4:definition_of_variational_approximation}{{1.12}{9}{Modeling the latent posterior score $\nabla _{\textbf {x}_t} \ln {p(\textbf {z} | \textbf {x}_t )}$}{equation.1.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Encoder Training Objective}{10}{subsection.1.3.5}\protected@file@percent }
\newlabel{ch4:Encoder_Training_Objective}{{1.3.5}{10}{Encoder Training Objective}{subsection.1.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Correction of the variational error}{10}{subsection.1.3.6}\protected@file@percent }
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Experiments}{11}{section.1.4}\protected@file@percent }
\newlabel{ch4:Experiments}{{1.4}{11}{Experiments}{section.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Cifar10}}{11}{table.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ch4:tbl:Cifar10}{{1.1}{11}{Cifar10}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces CelebA $64\times 64$}}{11}{table.caption.8}\protected@file@percent }
\newlabel{ch4:tbl:CelebA}{{1.2}{11}{CelebA $64\times 64$}{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Conclusions}{11}{section.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.3}{\ignorespaces Cifar10}}{12}{table.caption.9}\protected@file@percent }
\newlabel{ch4:fig:cifar10_qualitative_comparison}{{1.3}{12}{Cifar10}{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.4}{\ignorespaces CelebA $64\times 64$}}{12}{table.caption.10}\protected@file@percent }
\newlabel{ch4:fig:celebA_qualitative_comparison}{{1.4}{12}{CelebA $64\times 64$}{table.caption.10}{}}
\@setckpt{Chapter4/chapter4}{
\setcounter{page}{13}
\setcounter{equation}{12}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{1}
\setcounter{table}{4}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{5}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{17}
\setcounter{theorem}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
}
