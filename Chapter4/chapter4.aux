\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\citation{vae}
\citation{zaho2017understanding_vaes}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Variational Diffusion Auto-encoder}{105}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{105}{section.6.1}\protected@file@percent }
\citation{diffusion_models,ddpm}
\citation{dhariwal2021diffusion_beats_gans}
\citation{kong2020diffWave}
\citation{batzolis2022non_uniform,saharia2021sr3}
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\citation{cifar}
\citation{liu2015celeba}
\citation{vae,rezende2014vae2}
\citation{higgins2016beta_vae}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Background}{107}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Variational Autoencoders}{107}{subsection.6.2.1}\protected@file@percent }
\newlabel{ch4:eq:elbo}{{6.1}{107}{Variational Autoencoders}{equation.6.2.1}{}}
\citation{rybkin2021sigma_vae}
\citation{song2020score}
\citation{score_matching}
\citation{diffusion_models,ddpm}
\citation{anderson1982reverse_time_sde}
\newlabel{ch4:eq:elbo}{{6.2}{108}{Variational Autoencoders}{equation.6.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Score-based diffusion models}{108}{subsection.6.2.2}\protected@file@percent }
\newlabel{ch4:sec:background_score}{{6.2.2}{108}{Score-based diffusion models}{subsection.6.2.2}{}}
\newlabel{ch4:eq:forward_sde}{{6.3}{108}{Score-based diffusion models}{equation.6.2.3}{}}
\newlabel{ch4:eq:reverse_sde}{{6.4}{108}{Score-based diffusion models}{equation.6.2.4}{}}
\citation{song2021maximum}
\citation{score_matching,vincent2011connection,song2020score}
\citation{song2020score}
\citation{vincent2011connection}
\citation{song2021maximum}
\citation{song2021maximum}
\newlabel{ch4:eq:approximated_reverse_sde}{{6.5}{109}{Score-based diffusion models}{equation.6.2.5}{}}
\newlabel{ch4:DSM_for_uniform_diffusion_models}{{6.7}{109}{Score-based diffusion models}{equation.6.2.7}{}}
\citation{kingmaVDM}
\citation{song2020score}
\citation{vincent2011connection,song2020score}
\citation{batzolis2022non_uniform}
\newlabel{ch4:Likelihood_Weighting_for_Uniform_Diffusion_Models}{{6.8}{110}{Score-based diffusion models}{equation.6.2.8}{}}
\newlabel{ch4:eq:conditional_reverse_sde}{{6.9}{110}{Score-based diffusion models}{equation.6.2.9}{}}
\newlabel{ch4:CDE}{{6.10}{110}{Score-based diffusion models}{equation.6.2.10}{}}
\citation{zaho2017understanding_vaes}
\citation{zhang2018lpips}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Graphical overview of our method. The time-dependent encoder network $e_\phi $ induces the encoder distribution $q_\phi ( \textbf  {z} | \textbf  {x}_t, t) \approx p_t( \textbf  {z} | \textbf  {x}_t)$. The data $\textbf  {x}_0$ is encoded with the encoder into a latent vector $\textbf  {z}$ by sampling $q_\phi ( \textbf  {z} | \textbf  {x}_0, 0)$. Then the reconstruction $\hat  {\textbf  {x}}_0$ is obtained by running the conditional reverse diffusion process using the approximate conditional data score $s_{\theta , \phi }(\textbf  {x}_t, \textbf  {z},t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {x}_t | \textbf  {z})}$. The model $s_{\theta , \phi }(\textbf  {x}_t, \textbf  {z},t)$ is obtained by adding the score of unconditional diffusion model $s_\theta (\textbf  {x}_t,t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {x}_t)} $ and the score of the encoder distribution $ \nabla _{\textbf  {x}_t} \ln q_\phi ( \textbf  {z} | \textbf  {x}_t, t) \approx \nabla _{\textbf  {x}_t} \ln {p(\textbf  {z} | \textbf  {x}_t )} $. The latter can be computed via automatic differentiation with respect to the input $\textbf  {x}_t$.}}{111}{figure.caption.103}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Method}{111}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Problems with conventional VAEs}{111}{subsection.6.3.1}\protected@file@percent }
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Conditional Diffusion Models as decoders}{112}{subsection.6.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Score VAE: Encoder with unconditional diffusion model as prior}{112}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Modeling the latent posterior score $\nabla _{\textbf  {x}_t} \ln {p(\textbf  {z} | \textbf  {x}_t )}$}{113}{subsection.6.3.4}\protected@file@percent }
\newlabel{ch4:eq:true_posterior}{{6.11}{113}{Modeling the latent posterior score $\nabla _{\textbf {x}_t} \ln {p(\textbf {z} | \textbf {x}_t )}$}{equation.6.3.11}{}}
\newlabel{ch4:definition_of_variational_approximation}{{6.12}{113}{Modeling the latent posterior score $\nabla _{\textbf {x}_t} \ln {p(\textbf {z} | \textbf {x}_t )}$}{equation.6.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Encoder Training Objective}{114}{subsection.6.3.5}\protected@file@percent }
\newlabel{ch4:Encoder_Training_Objective}{{6.3.5}{114}{Encoder Training Objective}{subsection.6.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}Correction of the variational error}{114}{subsection.6.3.6}\protected@file@percent }
\citation{preechakul2022diffusion_decoder,yang2023ldiffusion_decoder_compression}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Experiments}{115}{section.6.4}\protected@file@percent }
\newlabel{ch4:Experiments}{{6.4}{115}{Experiments}{section.6.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Cifar10}}{115}{table.caption.104}\protected@file@percent }
\newlabel{ch4:tbl:Cifar10}{{6.1}{115}{Cifar10}{table.caption.104}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces CelebA $64\times 64$}}{115}{table.caption.104}\protected@file@percent }
\newlabel{ch4:tbl:CelebA}{{6.2}{115}{CelebA $64\times 64$}{table.caption.104}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Conclusions}{115}{section.6.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Cifar10}}{116}{table.caption.105}\protected@file@percent }
\newlabel{ch4:fig:cifar10_qualitative_comparison}{{6.3}{116}{Cifar10}{table.caption.105}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces CelebA $64\times 64$}}{116}{table.caption.106}\protected@file@percent }
\newlabel{ch4:fig:celebA_qualitative_comparison}{{6.4}{116}{CelebA $64\times 64$}{table.caption.106}{}}
\@setckpt{Chapter4/chapter4}{
\setcounter{page}{117}
\setcounter{equation}{12}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{1}
\setcounter{table}{4}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{27}
\setcounter{Hfootnote}{7}
\setcounter{bookmark@seq@number}{75}
\setcounter{theorem}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{1}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALC@unique}{11}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
}
