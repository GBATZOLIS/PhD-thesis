\begin{thebibliography}{}

\bibitem[Anderson, 1982]{anderson1982reverse_time_sde}
Anderson, B.~D. (1982).
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326.

\bibitem[Arjovsky et~al., 2017]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L. (2017).
\newblock Wasserstein {GAN}.
\newblock In {\em Proceedings of the 34th International Conference on Machine Learning (ICML)}, volume~70, pages 214--223. PMLR.

\bibitem[Baevski et~al., 2020]{baevski2020wav2vec}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. (2020).
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 12449--12460.

\bibitem[Bao et~al., 2021]{bao2021beit}
Bao, H., Dong, L., and Wei, F. (2021).
\newblock Beit: Bert pre-training of image transformers.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Bardes et~al., 2022]{bardes2021vicreg}
Bardes, A., Ponce, J., and LeCun, Y. (2022).
\newblock Vicreg: Variance-invariance-covariance regularization for self-supervised learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Behrmann et~al., 2021]{behrmann2021understanding}
Behrmann, J., Vicol, P., Wang, K.-C., Grosse, R., and Jacobsen, J.-H. (2021).
\newblock Understanding and mitigating exploding inverses in invertible neural networks.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1792--1800. PMLR.

\bibitem[Caron et~al., 2018]{caron2018deep}
Caron, M., Bojanowski, P., Joulin, A., and Douze, M. (2018).
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 132--149.

\bibitem[Caron et~al., 2020]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., et~al. (2020).
\newblock Unsupervised learning of visual features by contrasting cluster assignments.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 9912--9924.

\bibitem[Chen et~al., 2020]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020).
\newblock A simple framework for contrastive learning of visual representations.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages 1597--1607.

\bibitem[Chen and He, 2021]{chen2021exploring}
Chen, X. and He, K. (2021).
\newblock Exploring simple siamese representation learning.
\newblock {\em arXiv preprint arXiv:2011.10566}.

\bibitem[Cornish et~al., 2020]{cornish2020relaxing}
Cornish, R., Caterini, A., Deligiannidis, G., and Doucet, A. (2020).
\newblock Relaxing bijectivity constraints with continuously indexed normalising flows.
\newblock In {\em International conference on machine learning}, pages 2133--2143. PMLR.

\bibitem[Devlin et~al., 2019]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}.

\bibitem[Ding et~al., 2021]{ding2021vqvae}
Ding, Y., Hu, H., Ge, Z., Wu, J., Zhang, H., and Ding, C. (2021).
\newblock Vq-vae for multimodal image synthesis.
\newblock {\em arXiv preprint arXiv:2103.02398}.

\bibitem[Du and Mordatch, 2019]{du2019implicit}
Du, Y. and Mordatch, I. (2019).
\newblock Implicit generation and modeling with energy-based models.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~32.

\bibitem[Esser et~al., 2021]{esser2021taming}
Esser, P., Rombach, R., and Ommer, B. (2021).
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 12873--12883.

\bibitem[Fefferman et~al., 2016]{fefferman2016testing}
Fefferman, C., Mitter, S., and Narayanan, H. (2016).
\newblock Testing the manifold hypothesis.
\newblock {\em Journal of the American Mathematical Society}, 29(4):983--1049.

\bibitem[Gidaris et~al., 2018]{gidaris2018unsupervised}
Gidaris, S., Singh, P., and Komodakis, N. (2018).
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Goodfellow et~al., 2016]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A. (2016).
\newblock {\em Deep learning}.
\newblock MIT Press.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014generative}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial networks.

\bibitem[Goyal et~al., 2021]{goyal2021self}
Goyal, P., Mahajan, D., Gupta, A., and Misra, I. (2021).
\newblock Self-supervised pretraining of visual features in the wild.
\newblock {\em arXiv preprint arXiv:2103.01988}.

\bibitem[Grathwohl et~al., 2020]{grathwohl2020your}
Grathwohl, W., Wang, K.~C., Jacobsen, J.-H., Duvenaud, D., Swersky, K., and Norouzi, M. (2020).
\newblock Your classifier is secretly an energy-based model and you should treat it like one.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Grill et~al., 2020]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., et~al. (2020).
\newblock Bootstrap your own latent: A new approach to self-supervised learning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 21271--21284.

\bibitem[He et~al., 2022]{he2022masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R. (2022).
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 16000--16009.

\bibitem[He et~al., 2020]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020).
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 9729--9738.

\bibitem[Hinton, 2002]{hinton2002training}
Hinton, G.~E. (2002).
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural Computation}, 14(8):1771--1800.

\bibitem[Ho et~al., 2020]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P. (2020).
\newblock Denoising diffusion probabilistic models.

\bibitem[Hochreiter and Schmidhuber, 1997]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780.

\bibitem[Izmailov et~al., 2021]{izmailov2021flowgmm}
Izmailov, P., Kirichenko, P., Finzi, M., and Wilson, A.~G. (2021).
\newblock Semi-supervised learning with normalizing flows.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Karras et~al., 2019]{karras2019style}
Karras, T., Laine, S., and Aila, T. (2019).
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4401--4410.

\bibitem[Karras et~al., 2020]{karras2020analyzing}
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. (2020).
\newblock Analyzing and improving the image quality of stylegan.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 8110--8119.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[LeCun, 2021]{lecun2021self}
LeCun, Y. (2021).
\newblock Self-supervised learning: The dark matter of intelligence.
\newblock {\em Facebook Research}.

\bibitem[LeCun et~al., 2015]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G. (2015).
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444.

\bibitem[LeCun et~al., 1998]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[LeCun et~al., 2006]{lecun2006tutorial}
LeCun, Y., Chopra, S., and Hadsell, R. (2006).
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting Structured Data}.

\bibitem[Liu et~al., 2019]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019).
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}.

\bibitem[Mikolov et~al., 2013]{mikolov2013efficient}
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).
\newblock Efficient estimation of word representations in vector space.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Misra et~al., 2016]{misra2016shuffle}
Misra, I., Zitnick, C.~L., and Hebert, M. (2016).
\newblock Shuffle and learn: unsupervised learning using temporal order verification.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 527--544. Springer.

\bibitem[Noroozi and Favaro, 2016]{noroozi2016unsupervised}
Noroozi, M. and Favaro, P. (2016).
\newblock Unsupervised learning of visual representations by solving jigsaw puzzles.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 69--84. Springer.

\bibitem[Oord et~al., 2016a]{vanwavenet2016}
Oord, A. v.~d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016a).
\newblock Wavenet: A generative model for raw audio.
\newblock {\em arXiv preprint arXiv:1609.03499}.

\bibitem[Oord et~al., 2016b]{oord2016pixel}
Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K. (2016b).
\newblock Pixel recurrent neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages 1747--1756.

\bibitem[Oord et~al., 2018]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O. (2018).
\newblock Representation learning with contrastive predictive coding.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 10256--10265.

\bibitem[Oord et~al., 2017]{oord2017neural}
Oord, A. v.~d., Vinyals, O., and Kavukcuoglu, K. (2017).
\newblock Neural discrete representation learning.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 6306--6315.

\bibitem[Papamakarios et~al., 2021]{papamakarios2019normalizing}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and Lakshminarayanan, B. (2021).
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em Journal of Machine Learning Research}, 22(57):1--64.

\bibitem[Pennington et~al., 2014]{pennington2014glove}
Pennington, J., Socher, R., and Manning, C.~D. (2014).
\newblock Glove: Global vectors for word representation.
\newblock In {\em Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 1532--1543.

\bibitem[Poggio et~al., 2017]{poggio2017theory}
Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. (2017).
\newblock Why and when can deep--but not shallow--networks avoid the curse of dimensionality: A review.
\newblock {\em International Journal of Automation and Computing}, 14(5):503--519.

\bibitem[Radford et~al., 2021]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., et~al. (2021).
\newblock Learning transferable visual models from natural language supervision.
\newblock {\em arXiv preprint arXiv:2103.00020}.

\bibitem[Radford et~al., 2019]{radford2019language}
Radford, A., Wu, J., Amodei, D., et~al. (2019).
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9.

\bibitem[Razavi et~al., 2019]{razavi2019generating}
Razavi, A., van~den Oord, A., and Vinyals, O. (2019).
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 14866--14876.

\bibitem[Rezende and Mohamed, 2015]{rezende2015variational}
Rezende, D. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages 1530--1538. PMLR.

\bibitem[Rombach et~al., 2022]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022).
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695.

\bibitem[Roweis and Saul, 2000]{roweis2000nonlinear}
Roweis, S.~T. and Saul, L.~K. (2000).
\newblock Nonlinear dimensionality reduction by locally linear embedding.
\newblock {\em science}, 290(5500):2323--2326.

\bibitem[Salimans and Ho, 2022]{salimans2022progressive}
Salimans, T. and Ho, J. (2022).
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}.

\bibitem[Schneider et~al., 2019]{schneider2019wav2vec}
Schneider, S., Baevski, A., Collobert, R., Auli, M., and Mohamed, A. (2019).
\newblock wav2vec: Unsupervised pre-training for speech recognition.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 6415--6419.

\bibitem[Scott, 2015]{scott2015multivariate}
Scott, D.~W. (2015).
\newblock {\em Multivariate Density Estimation: Theory, Practice, and Visualization}.
\newblock John Wiley \& Sons.

\bibitem[Silverman, 1986]{silverman1986density}
Silverman, B.~W. (1986).
\newblock {\em Density Estimation for Statistics and Data Analysis}.
\newblock Chapman and Hall.

\bibitem[Song et~al., 2020a]{song2020denoising}
Song, J., Meng, C., and Ermon, S. (2020a).
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}.

\bibitem[Song et~al., 2023]{song2023consistency}
Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. (2023).
\newblock Consistency models.
\newblock {\em arXiv preprint arXiv:2303.01469}.

\bibitem[Song et~al., 2021a]{song2021maximum}
Song, Y., Durkan, C., Murray, I., and Ermon, S. (2021a).
\newblock Maximum likelihood training of score-based diffusion models.

\bibitem[Song et~al., 2020b]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B. (2020b).
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}.

\bibitem[Song et~al., 2021b]{song2021sde}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B. (2021b).
\newblock Score-based generative modeling through stochastic differential equations.

\bibitem[Tenenbaum et~al., 2000]{tenenbaum2000global}
Tenenbaum, J.~B., Silva, V.~d., and Langford, J.~C. (2000).
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock {\em science}, 290(5500):2319--2323.

\bibitem[Tian et~al., 2022]{tian2022}
Tian, Y. et~al. (2022).
\newblock Understanding self-supervised contrastive learning with generalized contrastive losses.
\newblock {\em arXiv preprint arXiv:2202.09671}.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 5998--6008.

\bibitem[Wasserman, 2006]{wasserman2006all}
Wasserman, L. (2006).
\newblock {\em All of Nonparametric Statistics}.
\newblock Springer Science \& Business Media.

\bibitem[Zbontar et~al., 2021]{zbontar2021barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. (2021).
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In {\em International Conference on Machine Learning (ICML)}. PMLR.

\bibitem[Zhou et~al., 2022]{zhou2022ibot}
Zhou, D., Wang, Z., Wang, L., Han, J., Dai, X., Shen, Y., and Feng, J. (2022).
\newblock ibot: Image bert pre-training with online tokenizer.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}
