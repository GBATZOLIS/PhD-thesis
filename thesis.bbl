\begin{thebibliography}{}

\bibitem[Anderson, 1982]{anderson1982reverse_time_sde}
Anderson, B.~D. (1982).
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326.

\bibitem[Arandjelovic and Zisserman, 2017]{arandjelovic2017look}
Arandjelovic, R. and Zisserman, A. (2017).
\newblock Look, listen and learn.
\newblock In {\em Proceedings of the IEEE International Conference on Computer Vision}, pages 609--617.

\bibitem[Arjovsky et~al., 2017]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L. (2017).
\newblock Wasserstein {GAN}.
\newblock In {\em Proceedings of the 34th International Conference on Machine Learning (ICML)}, volume~70, pages 214--223. PMLR.

\bibitem[Baek et~al., 2021]{baek2021accurate}
Baek, M., DiMaio, F., Anishchenko, I., Dauparas, J., Ovchinnikov, S., Lee, G.~R., Wang, J., Cong, Q., Kinch, L.~N., Schaeffer, R.~D., et~al. (2021).
\newblock Accurate prediction of protein structures and interactions using a three-track neural network.
\newblock {\em Science}, 373(6557):871--876.

\bibitem[Baevski et~al., 2020]{baevski2020wav2vec}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. (2020).
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 12449--12460.

\bibitem[Bao et~al., 2021]{bao2021beit}
Bao, H., Dong, L., and Wei, F. (2021).
\newblock Beit: Bert pre-training of image transformers.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Bardes et~al., 2022]{bardes2021vicreg}
Bardes, A., Ponce, J., and LeCun, Y. (2022).
\newblock Vicreg: Variance-invariance-covariance regularization for self-supervised learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Batzolis et~al., 2024]{batzolis2024caflow}
Batzolis, G., Carioni, M., Etmann, C., Afyouni, S., Kourtzi, Z., and Sch{\"o}nlieb, C.-B. (2024).
\newblock Caflow: Conditional autoregressive flows.
\newblock {\em Foundations of Data Science}, 6(4):553--583.
\newblock The first author is supported by GSK. Early access: June 2024.

\bibitem[Batzolis et~al., 2023]{batzolis2023variational}
Batzolis, G., Stanczuk, J., and Sch{\"o}nlieb, C.-B. (2023).
\newblock Variational diffusion auto-encoder: Latent space extraction from pre-trained diffusion models.
\newblock {\em arXiv preprint arXiv:2304.12141}.

\bibitem[Batzolis et~al., 2022]{batzolis2022non_uniform}
Batzolis, G., Stanczuk, J., Schönlieb, C.-B., and Etmann, C. (2022).
\newblock Non-uniform diffusion models.

\bibitem[Behrmann et~al., 2021]{behrmann2021understanding}
Behrmann, J., Vicol, P., Wang, K.-C., Grosse, R., and Jacobsen, J.-H. (2021).
\newblock Understanding and mitigating exploding inverses in invertible neural networks.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1792--1800. PMLR.

\bibitem[Boothby, 2003]{boothby2003introduction}
Boothby, W.~M. (2003).
\newblock {\em An introduction to differentiable manifolds and Riemannian geometry, Revised}, volume 120.
\newblock Gulf Professional Publishing.

\bibitem[Brown et~al., 2020]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020).
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1877--1901.

\bibitem[Butler et~al., 2018]{butler2018machine}
Butler, K.~T., Davies, D.~W., Cartwright, H., Isayev, O., and Walsh, A. (2018).
\newblock Machine learning for molecular and materials science.
\newblock {\em Nature}, 559(7715):547--555.

\bibitem[Carmo, 1992]{carmo1992riemannian}
Carmo, M. P.~d. (1992).
\newblock {\em Riemannian geometry}.
\newblock Birkh{\"a}user.

\bibitem[Caron et~al., 2018]{caron2018deep}
Caron, M., Bojanowski, P., Joulin, A., and Douze, M. (2018).
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 132--149.

\bibitem[Caron et~al., 2020]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., et~al. (2020).
\newblock Unsupervised learning of visual features by contrasting cluster assignments.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 9912--9924.

\bibitem[Chen et~al., 2020]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020).
\newblock A simple framework for contrastive learning of visual representations.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages 1597--1607.

\bibitem[Chen and He, 2021]{chen2021exploring}
Chen, X. and He, K. (2021).
\newblock Exploring simple siamese representation learning.
\newblock {\em arXiv preprint arXiv:2011.10566}.

\bibitem[Cornish et~al., 2020]{cornish2020relaxing}
Cornish, R., Caterini, A., Deligiannidis, G., and Doucet, A. (2020).
\newblock Relaxing bijectivity constraints with continuously indexed normalising flows.
\newblock In {\em International conference on machine learning}, pages 2133--2143. PMLR.

\bibitem[Devlin et~al., 2019a]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019a).
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.

\bibitem[Devlin et~al., 2019b]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019b).
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}.

\bibitem[Diepeveen et~al., 2024]{diepeveen2024score}
Diepeveen, W., Batzolis, G., Shumaylov, Z., and Sch{\"o}nlieb, C.-B. (2024).
\newblock Score-based pullback riemannian geometry.
\newblock {\em arXiv preprint arXiv:2410.01950}.

\bibitem[Ding et~al., 2021]{ding2021vqvae}
Ding, Y., Hu, H., Ge, Z., Wu, J., Zhang, H., and Ding, C. (2021).
\newblock Vq-vae for multimodal image synthesis.
\newblock {\em arXiv preprint arXiv:2103.02398}.

\bibitem[Du and Mordatch, 2019]{du2019implicit}
Du, Y. and Mordatch, I. (2019).
\newblock Implicit generation and modeling with energy-based models.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~32.

\bibitem[Esser et~al., 2021]{esser2021taming}
Esser, P., Rombach, R., and Ommer, B. (2021).
\newblock Taming transformers for high-resolution image synthesis.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 12873--12883.

\bibitem[Fefferman et~al., 2016]{fefferman2016testing}
Fefferman, C., Mitter, S., and Narayanan, H. (2016).
\newblock Testing the manifold hypothesis.
\newblock {\em Journal of the American Mathematical Society}, 29(4):983--1049.

\bibitem[Gentile et~al., 2020]{gentile2020deep}
Gentile, F., Agrawal, V., Hsing, M.-F., Ton, A.-T., Ban, F., Norinder, U., Gleave, M.~E., and Cherkasov, A. (2020).
\newblock Deep docking: A deep learning platform for ultra-large virtual screening.
\newblock {\em Journal of Chemical Information and Modeling}, 60(9):4291--4305.

\bibitem[Gidaris et~al., 2018]{gidaris2018unsupervised}
Gidaris, S., Singh, P., and Komodakis, N. (2018).
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Goodfellow et~al., 2016]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A. (2016).
\newblock {\em Deep learning}.
\newblock MIT Press.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014generative}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial networks.

\bibitem[Goyal et~al., 2021]{goyal2021self}
Goyal, P., Mahajan, D., Gupta, A., and Misra, I. (2021).
\newblock Self-supervised pretraining of visual features in the wild.
\newblock {\em arXiv preprint arXiv:2103.01988}.

\bibitem[Grathwohl et~al., 2020]{grathwohl2020your}
Grathwohl, W., Wang, K.~C., Jacobsen, J.-H., Duvenaud, D., Swersky, K., and Norouzi, M. (2020).
\newblock Your classifier is secretly an energy-based model and you should treat it like one.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Grill et~al., 2020]{grill2020bootstrap}
Grill, J.-B., Strub, F., Altch{\'e}, F., et~al. (2020).
\newblock Bootstrap your own latent: A new approach to self-supervised learning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 21271--21284.

\bibitem[He et~al., 2022]{he2022masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R. (2022).
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 16000--16009.

\bibitem[He et~al., 2020]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020).
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 9729--9738.

\bibitem[Higgins et~al., 2017]{higgins2017beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2017).
\newblock Beta-vae: Learning basic visual concepts with a constrained variational framework.
\newblock In {\em 5th International Conference on Learning Representations (ICLR)}.

\bibitem[Hinton, 2002]{hinton2002training}
Hinton, G.~E. (2002).
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural Computation}, 14(8):1771--1800.

\bibitem[Ho et~al., 2020]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P. (2020).
\newblock Denoising diffusion probabilistic models.

\bibitem[Hochreiter and Schmidhuber, 1997]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780.

\bibitem[Hossain et~al., 2020]{hossain2019comprehensive}
Hossain, M., Sohel, F., Shiratuddin, M.~F., and Laga, H. (2020).
\newblock A comprehensive survey of image caption generation techniques.
\newblock {\em Journal of Visual Communication and Image Representation}, 71:102844.

\bibitem[Izmailov et~al., 2021]{izmailov2021flowgmm}
Izmailov, P., Kirichenko, P., Finzi, M., and Wilson, A.~G. (2021).
\newblock Semi-supervised learning with normalizing flows.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Jumper et~al., 2021]{jumper2021highly}
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., {\v{Z}}{\'\i}dek, A., Potapenko, A., et~al. (2021).
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596(7873):583--589.

\bibitem[Karras et~al., 2019a]{karras2019style}
Karras, T., Laine, S., and Aila, T. (2019a).
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4401--4410.

\bibitem[Karras et~al., 2019b]{karras2019stylegan}
Karras, T., Laine, S., and Aila, T. (2019b).
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4401--4410.

\bibitem[Karras et~al., 2020]{karras2020analyzing}
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. (2020).
\newblock Analyzing and improving the image quality of stylegan.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 8110--8119.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[Kong et~al., 2020]{kong2020diffwave}
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2020).
\newblock Diffwave: A versatile diffusion model for audio synthesis.

\bibitem[Langevin, 1908]{langevin1908theorie}
Langevin, P. (1908).
\newblock Sur la théorie du mouvement brownien.
\newblock {\em Comptes Rendus de l'Académie des Sciences (Paris)}, 146:530--533.

\bibitem[LeCun, 2021]{lecun2021self}
LeCun, Y. (2021).
\newblock Self-supervised learning: The dark matter of intelligence.
\newblock {\em Facebook Research}.

\bibitem[LeCun et~al., 2015]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G. (2015).
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444.

\bibitem[LeCun et~al., 1998]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[LeCun et~al., 2006]{lecun2006tutorial}
LeCun, Y., Chopra, S., and Hadsell, R. (2006).
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting Structured Data}.

\bibitem[Lee, 2013]{lee2013smooth}
Lee, J.~M. (2013).
\newblock Smooth manifolds.
\newblock In {\em Introduction to Smooth Manifolds}, pages 1--31. Springer.

\bibitem[Liu et~al., 2019]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019).
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}.

\bibitem[Lugmayr et~al., 2022]{lugmayr2022repaint}
Lugmayr, A., Danelljan, M., Romero, A., Sabater, N., Timofte, R., and Van~Gool, L. (2022).
\newblock Repaint: Inpainting using denoising diffusion probabilistic models.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11461--11471.

\bibitem[Mescheder et~al., 2018]{mescheder2018which}
Mescheder, L., Geiger, A., and Nowozin, S. (2018).
\newblock Which training methods for gans do actually converge?
\newblock In {\em Proceedings of the 35th International Conference on Machine Learning (ICML)}, pages 3481--3490.

\bibitem[Mikolov et~al., 2013]{mikolov2013efficient}
Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013).
\newblock Efficient estimation of word representations in vector space.
\newblock In {\em International Conference on Learning Representations (ICLR)}.

\bibitem[Misra et~al., 2016]{misra2016shuffle}
Misra, I., Zitnick, C.~L., and Hebert, M. (2016).
\newblock Shuffle and learn: unsupervised learning using temporal order verification.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 527--544. Springer.

\bibitem[Neal, 2011]{neal2011mcmc}
Neal, R.~M. (2011).
\newblock {MCMC Using Hamiltonian Dynamics}.
\newblock In Brooks, S., Gelman, A., Jones, G.~L., and Meng, X.-L., editors, {\em {Handbook of Markov Chain Monte Carlo}}, pages 113--162. Chapman and Hall/CRC, Boca Raton, FL.

\bibitem[Noroozi and Favaro, 2016]{noroozi2016unsupervised}
Noroozi, M. and Favaro, P. (2016).
\newblock Unsupervised learning of visual representations by solving jigsaw puzzles.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages 69--84. Springer.

\bibitem[Oord et~al., 2016a]{vanwavenet2016}
Oord, A. v.~d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016a).
\newblock Wavenet: A generative model for raw audio.
\newblock {\em arXiv preprint arXiv:1609.03499}.

\bibitem[Oord et~al., 2016b]{oord2016pixel}
Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K. (2016b).
\newblock Pixel recurrent neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages 1747--1756.

\bibitem[Oord et~al., 2018]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O. (2018).
\newblock Representation learning with contrastive predictive coding.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 10256--10265.

\bibitem[Oord et~al., 2017]{oord2017neural}
Oord, A. v.~d., Vinyals, O., and Kavukcuoglu, K. (2017).
\newblock Neural discrete representation learning.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, pages 6306--6315.

\bibitem[Papamakarios et~al., 2021]{papamakarios2019normalizing}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and Lakshminarayanan, B. (2021).
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em Journal of Machine Learning Research}, 22(57):1--64.

\bibitem[Pennington et~al., 2014]{pennington2014glove}
Pennington, J., Socher, R., and Manning, C.~D. (2014).
\newblock Glove: Global vectors for word representation.
\newblock In {\em Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 1532--1543.

\bibitem[Poggio et~al., 2017]{poggio2017theory}
Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., and Liao, Q. (2017).
\newblock Why and when can deep--but not shallow--networks avoid the curse of dimensionality: A review.
\newblock {\em International Journal of Automation and Computing}, 14(5):503--519.

\bibitem[Radford et~al., 2021]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al. (2021).
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International Conference on Machine Learning}, pages 8748--8763. PMLR.

\bibitem[Radford et~al., 2019]{radford2019language}
Radford, A., Wu, J., Amodei, D., et~al. (2019).
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI Blog}, 1(8):9.

\bibitem[Ramesh et~al., 2021]{ramesh2021zero}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021).
\newblock Zero-shot text-to-image generation.
\newblock In {\em International conference on machine learning}, pages 8821--8831. Pmlr.

\bibitem[Razavi et~al., 2019]{razavi2019generating}
Razavi, A., van~den Oord, A., and Vinyals, O. (2019).
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 14866--14876.

\bibitem[Rezende and Mohamed, 2015]{rezende2015variational}
Rezende, D. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages 1530--1538. PMLR.

\bibitem[Rezende et~al., 2014]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic backpropagation and approximate inference in deep generative models.
\newblock In {\em Proceedings of the 31st International Conference on Machine Learning (ICML)}, volume~32, pages 1278--1286. PMLR.

\bibitem[Rives et~al., 2021]{rives2021biological}
Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C.~L., Ma, J., and Fergus, R. (2021).
\newblock Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(15).

\bibitem[Rombach et~al., 2022]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022).
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695.

\bibitem[Roweis and Saul, 2000]{roweis2000nonlinear}
Roweis, S.~T. and Saul, L.~K. (2000).
\newblock Nonlinear dimensionality reduction by locally linear embedding.
\newblock {\em science}, 290(5500):2323--2326.

\bibitem[Saharia et~al., 2022]{saharia2022image}
Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.~J., and Norouzi, M. (2022).
\newblock Image super-resolution via iterative refinement.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}.

\bibitem[Sakai, 1996]{sakai1996riemannian}
Sakai, T. (1996).
\newblock {\em Riemannian geometry}, volume 149.
\newblock American Mathematical Soc.

\bibitem[Salimans and Ho, 2022]{salimans2022progressive}
Salimans, T. and Ho, J. (2022).
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}.

\bibitem[Schneider et~al., 2019]{schneider2019wav2vec}
Schneider, S., Baevski, A., Collobert, R., Auli, M., and Mohamed, A. (2019).
\newblock wav2vec: Unsupervised pre-training for speech recognition.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 6415--6419.

\bibitem[Scott, 2015]{scott2015multivariate}
Scott, D.~W. (2015).
\newblock {\em Multivariate Density Estimation: Theory, Practice, and Visualization}.
\newblock John Wiley \& Sons.

\bibitem[Silverman, 1986]{silverman1986density}
Silverman, B.~W. (1986).
\newblock {\em Density Estimation for Statistics and Data Analysis}.
\newblock Chapman and Hall.

\bibitem[Song et~al., 2020a]{song2020denoising}
Song, J., Meng, C., and Ermon, S. (2020a).
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}.

\bibitem[Song et~al., 2023]{song2023consistency}
Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. (2023).
\newblock Consistency models.
\newblock {\em arXiv preprint arXiv:2303.01469}.

\bibitem[Song et~al., 2021a]{song2021maximum}
Song, Y., Durkan, C., Murray, I., and Ermon, S. (2021a).
\newblock Maximum likelihood training of score-based diffusion models.

\bibitem[Song et~al., 2020b]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B. (2020b).
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}.

\bibitem[Song et~al., 2021b]{song2021score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B. (2021b).
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Song et~al., 2021c]{song2021sde}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B. (2021c).
\newblock Score-based generative modeling through stochastic differential equations.

\bibitem[Stanczuk et~al., 2024]{pmlr-v235-stanczuk24a}
Stanczuk, J.~P., Batzolis, G., Deveney, T., and Sch\"{o}nlieb, C.-B. (2024).
\newblock Diffusion models encode the intrinsic dimension of data manifolds.
\newblock In Salakhutdinov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, {\em Proceedings of the 41st International Conference on Machine Learning}, volume 235 of {\em Proceedings of Machine Learning Research}, pages 46412--46440. PMLR.

\bibitem[Stokes et~al., 2020]{stokes2020deep}
Stokes, J.~M., Yang, K., Swanson, K., Jin, W., Cubillos-Ruiz, J.~R., Donghia, N.~M., MacNair, C.~R., French, S., Carfrae, L.~A., Bloom-Ackermann, Z., et~al. (2020).
\newblock A deep learning approach to antibiotic discovery.
\newblock {\em Cell}, 180(4):688--702.

\bibitem[Sun et~al., 2019a]{sun2019videobert}
Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C. (2019a).
\newblock Videobert: A joint model for video and language representation learning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 7464--7473.

\bibitem[Sun et~al., 2019b]{Dual-Glow}
Sun, H., Mehta, R., Zhou, H.~H., Huang, Z., Johnson, S.~C., Prabhakaran, V., and Singh, V. (2019b).
\newblock Dual-glow: Conditional flow-based generative model for modality transfer.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}.

\bibitem[Suvorov et~al., 2022]{suvorov2022resolution}
Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Korzhenkov, D., Galyautdinov, I., Kong, N., Goka, H., and Lempitsky, V. (2022).
\newblock Resolution-robust large mask inpainting with fourier convolutions.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8564--8573.

\bibitem[Tenenbaum et~al., 2000]{tenenbaum2000global}
Tenenbaum, J.~B., Silva, V.~d., and Langford, J.~C. (2000).
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock {\em science}, 290(5500):2319--2323.

\bibitem[Tian et~al., 2022]{tian2022}
Tian, Y. et~al. (2022).
\newblock Understanding self-supervised contrastive learning with generalized contrastive losses.
\newblock {\em arXiv preprint arXiv:2202.09671}.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 5998--6008.

\bibitem[Wang et~al., 2023]{wang2022image}
Wang, R., Wei, F., Sun, C., Murphy, K., and Schmid, C. (2023).
\newblock Imagebind: Learning joint representations of vision, audio, and language without supervision.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2256--2266.

\bibitem[Wang et~al., 2024]{wang2024sinsr}
Wang, Y., Yang, W., Chen, X., Wang, Y., Guo, L., Chau, L.-P., Liu, Z., Qiao, Y., Kot, A.~C., and Wen, B. (2024).
\newblock Sinsr: Diffusion-based image super-resolution in a single step.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 25796--25805.

\bibitem[Wasserman, 2006]{wasserman2006all}
Wasserman, L. (2006).
\newblock {\em All of Nonparametric Statistics}.
\newblock Springer Science \& Business Media.

\bibitem[Xu et~al., 2021]{xu2021videoclip}
Xu, H., Ghosh, G., Song, Y., Zhang, X., Li, C.-Y., Sigal, L., and Wang, Y. (2021).
\newblock Videoclip: Contrastive pre-training for zero-shot video-text understanding.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~34, pages 12471--12484.

\bibitem[Zbontar et~al., 2021]{zbontar2021barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. (2021).
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In {\em International Conference on Machine Learning (ICML)}. PMLR.

\bibitem[Zhang et~al., 2021]{zhang2021unified}
Zhang, X., Zhao, P., Zhang, J., Weng, Y., Yuan, H., Li, Y., and Wang, H. (2021).
\newblock A unified framework for molecule generation and reaction prediction.
\newblock {\em Nature Communications}, 12(1):3117.

\bibitem[Zhou et~al., 2022]{zhou2022ibot}
Zhou, D., Wang, Z., Wang, L., Han, J., Dai, X., Shen, Y., and Feng, J. (2022).
\newblock ibot: Image bert pre-training with online tokenizer.
\newblock In {\em International Conference on Learning Representations}.

\end{thebibliography}
