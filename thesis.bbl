\begin{thebibliography}{}

\bibitem[Anderson, 1982]{anderson1982reverse_time_sde}
Anderson, B.~D. (1982).
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326.

\bibitem[Behrmann et~al., 2021]{behrmann2021understandin}
Behrmann, J., Vicol, P., Wang, K.-C., Grosse, R., and Jacobsen, J.-H. (2021).
\newblock Understanding and mitigating exploding inverses in invertible neural networks.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1792--1800. PMLR.

\bibitem[Bishop and Tipping, 2001]{ppca}
Bishop, C.~M. and Tipping, M.~E. (2001).
\newblock Probabilistic principal component analysis.
\newblock PPCA.

\bibitem[Brehmer and Cranmer, 2020]{brehmer2020flows}
Brehmer, J. and Cranmer, K. (2020).
\newblock Flows for simultaneous manifold learning and density estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 33:442--453.

\bibitem[Camastra and Vinciarelli, 2002]{fractal_dim}
Camastra, F. and Vinciarelli, A. (2002).
\newblock Estimating the intrinsic dimension of data with a fractal-based method.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 24(10):1404--1407.

\bibitem[Campadelli et~al., 2015]{campadelli2015intrinsic}
Campadelli, P., Casiraghi, E., Ceruti, C., and Rozza, A. (2015).
\newblock Intrinsic dimension estimation: Relevant techniques and a benchmark framework.
\newblock {\em Mathematical Problems in Engineering}, 2015:1--21.

\bibitem[Chen et~al., 2023]{chen2023manifold_linear}
Chen, M., Huang, K., Zhao, T., and Wang, M. (2023).
\newblock Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data.

\bibitem[Cornish et~al., 2020]{cornish2020relaxing}
Cornish, R., Caterini, A., Deligiannidis, G., and Doucet, A. (2020).
\newblock Relaxing bijectivity constraints with continuously indexed normalising flows.
\newblock In {\em International conference on machine learning}, pages 2133--2143. PMLR.

\bibitem[Deng et~al., 2009]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255.

\bibitem[Fan et~al., 2010]{fan_local_pca}
Fan, M., Gu, N., Qiao, H., and Zhang, B. (2010).
\newblock Intrinsic dimension estimation of data by principal component analysis.

\bibitem[Fefferman et~al., 2013]{manifold_hypothesis}
Fefferman, C., Mitter, S., and Narayanan, H. (2013).
\newblock Testing the manifold hypothesis.

\bibitem[Fukunaga and Olsen, 1971]{Karhunen-Loeve}
Fukunaga, K. and Olsen, D. (1971).
\newblock An algorithm for finding intrinsic dimensionality of data.
\newblock {\em IEEE Transactions on Computers}, C-20(2):176--183.

\bibitem[Goodfellow et~al., 2014]{gan}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial networks.

\bibitem[Goyal and Bengio, 2022]{goyal2022inductive}
Goyal, A. and Bengio, Y. (2022).
\newblock Inductive biases for deep learning of higher-level cognition.
\newblock {\em Proceedings of the Royal Society A}, 478(2266):20210068.

\bibitem[Haro et~al., 2008]{haro_mle}
Haro, G., Randall, G., and Sapiro, G. (2008).
\newblock Translated poisson mixture model for stratification learning.
\newblock {\em Int. J. Comput. Vis.}, 80(3):358--374.

\bibitem[Ho et~al., 2020a]{ddpm}
Ho, J., Jain, A., and Abbeel, P. (2020a).
\newblock Denoising diffusion probabilistic models.

\bibitem[Ho et~al., 2020b]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P. (2020b).
\newblock Denoising diffusion probabilistic models.

\bibitem[Horvat and Pfister, 2022]{horvat2022nfid}
Horvat, C. and Pfister, J.-P. (2022).
\newblock Intrinsic dimensionality estimation using normalizing flows.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, {\em Advances in Neural Information Processing Systems}, volume~35, pages 12225--12236. Curran Associates, Inc.

\bibitem[Hyv{{\"a}}rinen, 2005]{score_matching}
Hyv{{\"a}}rinen, A. (2005).
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(24):695--709.

\bibitem[Jaini et~al., 2020]{jaini2020tails}
Jaini, P., Kobyzev, I., Yu, Y., and Brubaker, M. (2020).
\newblock Tails of lipschitz triangular flows.
\newblock In {\em International Conference on Machine Learning}, pages 4673--4681. PMLR.

\bibitem[Johnsson, 2016]{R_intrinsic_dim}
Johnsson, K. (2016).
\newblock intrinsicdimension: Intrinsic dimension estimation.

\bibitem[K\'{e}gl, 2002]{packing_number}
K\'{e}gl, B. (2002).
\newblock Intrinsic dimension estimation using packing numbers.
\newblock In Becker, S., Thrun, S., and Obermayer, K., editors, {\em Advances in Neural Information Processing Systems}, volume~15. MIT Press.

\bibitem[Kim et~al., 2019]{kim2019kde}
Kim, J., Shin, J., Rinaldo, A., and Wasserman, L. (2019).
\newblock Uniform convergence rate of the kernel density estimator adaptive to intrinsic volume dimension.
\newblock In {\em International Conference on Machine Learning}, pages 3398--3407. PMLR.

\bibitem[Kingma and Welling, 2014]{vae}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-encoding variational bayes.
\newblock In Bengio, Y. and LeCun, Y., editors, {\em 2nd International Conference on Learning Representations, {ICLR} 2014}.

\bibitem[Kpotufe, 2011]{kpotufe2011knn}
Kpotufe, S. (2011).
\newblock k-nn regression adapts to local intrinsic dimension.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Krizhevsky, 2012]{cifar}
Krizhevsky, A. (2012).
\newblock Learning multiple layers of features from tiny images.
\newblock {\em University of Toronto}.

\bibitem[Laszkiewicz et~al., 2021]{laszkiewicz2021copula}
Laszkiewicz, M., Lederer, J., and Fischer, A. (2021).
\newblock Copula-based normalizing flows.
\newblock {\em arXiv preprint arXiv:2107.07352}.

\bibitem[LeCun and Cortes, 2010]{mnist}
LeCun, Y. and Cortes, C. (2010).
\newblock {MNIST} handwritten digit database.

\bibitem[Lee, 2019]{lee2019_riemman}
Lee, J. (2019).
\newblock {\em Introduction to Riemannian Manifolds}.
\newblock Graduate Texts in Mathematics. Springer International Publishing.

\bibitem[Levina and Bickel, 2004]{dim_MLE}
Levina, E. and Bickel, P. (2004).
\newblock Maximum likelihood estimation of intrinsic dimension.
\newblock In Saul, L., Weiss, Y., and Bottou, L., editors, {\em Advances in Neural Information Processing Systems}, volume~17. MIT Press.

\bibitem[Minka, 2000]{auto_ppca}
Minka, T. (2000).
\newblock Automatic choice of dimensionality for pca.
\newblock In Leen, T., Dietterich, T., and Tresp, V., editors, {\em Advances in Neural Information Processing Systems}, volume~13. MIT Press.

\bibitem[Nicolaescu, 2011]{nicolaescu2011morse_theory}
Nicolaescu, L. (2011).
\newblock {\em An Invitation to Morse Theory}.
\newblock Universitext. Springer New York.

\bibitem[Oko et~al., 2023]{oko2023diffusion_mini_max}
Oko, K., Akiyama, S., and Suzuki, T. (2023).
\newblock Diffusion models are minimax optimal distribution estimators.

\bibitem[Palais and Terng, 1988]{palais1988critical}
Palais, R.~S. and Terng, C. (1988).
\newblock {\em Critical Point Theory and Submanifold Geometry}.
\newblock Critical Point Theory and Submanifold Geometry. Springer.

\bibitem[Pedregosa et~al., 2011]{sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et~al. (2011).
\newblock Scikit-learn: Machine learning in python.
\newblock {\em Journal of machine learning research}, 12(Oct):2825--2830.

\bibitem[Pettis et~al., 1979]{pettis_nn_dim_estimator}
Pettis, K.~W., Bailey, T.~A., Jain, A.~K., and Dubes, R.~C. (1979).
\newblock An intrinsic dimensionality estimator from near-neighbor information.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, PAMI-1(1):25--37.

\bibitem[Pidstrigach, 2022]{pidstrigach2022manifold_jakiw}
Pidstrigach, J. (2022).
\newblock Score-based generative models detect manifolds.

\bibitem[Pope et~al., 2021]{pope2021intrinsic}
Pope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2021).
\newblock The intrinsic dimension of images and its impact on learning.
\newblock {\em arXiv preprint arXiv:2104.08894}.

\bibitem[Sohl-Dickstein et~al., 2015]{diffusion_models}
Sohl-Dickstein, J., Weiss, E.~A., Maheswaranathan, N., and Ganguli, S. (2015).
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.

\bibitem[Song et~al., 2021]{song2021maximum}
Song, Y., Durkan, C., Murray, I., and Ermon, S. (2021).
\newblock Maximum likelihood training of score-based diffusion models.

\bibitem[Song et~al., 2020]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B. (2020).
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}.

\bibitem[Tempczyk et~al., 2022]{tempczyk2022lidl}
Tempczyk, P., Michaluk, R., Łukasz Garncarek, Spurek, P., Tabor, J., and Goliński, A. (2022).
\newblock Lidl: Local intrinsic dimension estimation using approximate likelihood.

\bibitem[Vincent, 2011]{vincent2011connection}
Vincent, P. (2011).
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural Computation}, 23(7):1661--1674.

\bibitem[Weed and Bach, 2019]{weed2019sharp}
Weed, J. and Bach, F. (2019).
\newblock Sharp asymptotic and finite-sample rates of convergence of empirical measures in wasserstein distance.

\end{thebibliography}
