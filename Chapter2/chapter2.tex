%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Conditional Image Generation with Score-Based Diffusion Models}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi


Score-based diffusion models have emerged as one of the most promising frameworks for deep generative modelling. In this work we conduct a systematic comparison and theoretical analysis of different approaches to learning conditional probability distributions with score-based diffusion models. In particular, we prove results which provide a theoretical justification for one of the most successful estimators of the conditional score. Moreover, we introduce a multi-speed diffusion framework, which leads to a new estimator for the conditional score, performing on par with previous state-of-the-art approaches. Our theoretical and experimental findings are accompanied by an open source library \texttt{MSDiff} which allows for application and further research of multi-speed diffusion models.

\section{Introduction}

\begin{figure}[h]
    \begin{center}
    \begin{tabular}{ccc}
        \scriptsize Original image $x$ & \scriptsize  Observation $y$ &  \scriptsize  Sample from $p_\theta(x|y)$  \\

        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/797_x.png} &   
        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/797_y.png} &
        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/797_1.png}  \\

        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/x.png} &   
        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/y.png} &
        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/DV.png}  \\

        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/189_x.png} &   
        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/189_y.png} &
        \includegraphics[width=.13\textwidth]{Chapter2/samples/teaser/189_3.png}  \\
    \end{tabular}
    \end{center}
    \caption{Results from our conditional multi-speed diffusive estimator.}
    \label{fig: teaser}
\end{figure}

The goal of generative modelling is to learn a  probability distribution from a finite set of samples. This classical problem in statistics has been studied for many decades, but until recently efficient learning of high-dimensional distributions remained impossible in practice. For images, the strong inductive biases of convolutional neural networks have recently enabled the modelling of such distributions, giving rise to the field of deep generative modelling.

Deep generative modelling became one of the central areas of deep learning with many successful applications \cite{saharia2021sr3}, \cite{SRFLOW}, \cite{liu2023i2sb}, \cite{donahue2019adversarial_audio}, \cite{zhu2020cycle_gan}, \cite{tang2021attentiongan}.
In recent years much progress has been made in unconditional and conditional image generation.
The most prominent approaches are auto-regressive models \cite{bengio2005autoregressive}, variational auto-encoders (VAEs) \cite{kingma2014autoencoding},  normalizing flows \cite{papamakarios2021normalizing} and generative adversarial networks (GANs) \cite{goodfellow2014generative}.

Despite their success, each of the above methods suffers from important limitations. Auto-regressive models allow for likelihood estimation and high-fidelity image generation, but require a lot of computational resources and suffer from poor time complexity in high resolutions. VAEs and normalizing flows are less computationally expensive and allow for likelihood estimation, but tend to produce samples of lower visual quality. Moreover, normalizing flows put restrictions on the possible model architectures (requiring invertibility of the network and a Jacobian log-determinant that is computationally tractable), thus limiting their expressivity. While GANs produce state-of-the art quality samples, they don't allow for likelihood estimation and are notoriously hard to train due to training instabilities and mode collapse. 

Recently, score-based \cite{hyvarinen2005score_original} and diffusion-based  \cite{sohldickstein2015diffusion_original} generative models have been revived and improved in \cite{song2020generative_score} and \cite{ho2020denoising}.  
The connection between the two frameworks in discrete-time formulation has been discovered in \cite{vincent2011connection}. 
Recently in \cite{song2021sde}, both frameworks have been unified into a single continuous-time approach based on stochastic differential equations \cite{song2021sde} and called score-based diffusion models. 
These approaches have recently received a lot of attention, achieving state-of-the-art performance in likelihood estimation \cite{song2021sde} and unconditional image generation \cite{dhariwal2021diffusion_beats_gans}, surpassing even the celebrated success of GANs.

In addition to achieving state-of-the art performance in both image generation and likelihood estimation, score-based diffusion models don't suffer from training instabilities or mode collapse \cite{dhariwal2021diffusion_beats_gans, song2021sde}. Moreover, their time complexity in high resolutions is much better than that of auto-regressive models \cite{dhariwal2021diffusion_beats_gans}. This makes score-based diffusion  models very attractive for deep generative modelling.

In this work, we examine how score-based diffusion  models can be applied to conditional image generation. We conduct a review and classification of existing approaches and perform a systematic comparison to find the best way of estimating the conditional score. We provide a proof of validity for the \textit{conditional denoising estimator} (which has been used in \cite{saharia2021sr3,tashiro2021csdi} without justification), and we thereby provide a firm theoretical foundation for using it in future research.

Moreover, we extend the original framework to support \textit{multi-speed diffusion}, where different parts of the input tensor diffuse according to different speeds. This allows us to introduce a novel estimator of the conditional score and opens an avenue for further research.


\noindent
\textbf{The contributions of this paper are as follows:}
\begin{enumerate}
    \item We review and empirically compare score-based diffusion  approaches to modelling conditional distributions of image data. The models are evaluated on the tasks of super-resolution, inpainting and edge to image translation.
    \item We provide a proof of consistency for the \textit{conditional denoising estimator} - one of the most successful approaches to estimating the conditional score. 
    \item We introduce a multi-speed diffusion framework which leads to \textit{conditional multi-speed diffusive estimator} (CMDE), a novel estimator of conditional score, which unifies previous methods of conditional score estimation.
    \item We provide an open-source library \texttt{MSDiff}, to facilitate further research on conditional and multi-speed diffusion models. \footnote{The code will be released in the near future.}
\end{enumerate}

\section{Notation}

In this work we will use the following notation:
\begin{itemize}
    \item \textbf{Functions of time}
    \begin{gather*}
        f_t := f(t)
    \end{gather*}
    \item \textbf{Indexing vectors} \\
    Let $v = (v_1, ..., v_n) \in \mathbb{R}^n$ and let $ 1 \leq i < j < n$. Then:
    \begin{align*}
        %v[i,j] &:= (v_i, v_{i+1}, ..., v_j) \in \mathbb{R}^{j - i + 1} \\ \nonumber
        v[:j] &:= (v_1, v_{2}, ..., v_j) \in \mathbb{R}^{j},
    \end{align*}
    cf. Section \ref{sec:CDiffE}.
    \item \textbf{Probability distributions} \\
    We denote the probability distribution of a random variable solely via the name of its density's argument, e.g.
    \begin{gather*}
        p(x_t) := p_{X_t}(x_t),
    \end{gather*}   
    where $x_t$ is a realisation of the random variable $X_t$.
    \item \textbf{Iterated Expectations}
    %\begin{gather*}
    %    \mathbb{E}_{\subalign{&x \sim p(x) \\ &y \sim p(y)}}[f(x,y)] := \mathbb{E}_{x \sim p(x)}\mathbb{E}_{y \sim p(y)}[f(x,y)]
    %\end{gather*}
    \begin{align*}
        &\mathbb{E}_{\subalign{z_1 &\sim p(z_1) \\ &{\myvdots} \\z_n &\sim p(z_n)}}[f(z_1,\Compactcdots,z_n)] \\
        :=&\mathbb{E}_{z_1 \sim p(z_1)} \Compactcdots \mathbb{E}_{z_n \sim p(z_n)}[f(z_1,\Compactcdots,z_n)]
    \end{align*}
\end{itemize}

\section{Methods}
In the following, we will provide details about the framework and estimators discussed in this paper.
\subsection{Background: Score matching through Stochastic Differential Equations}
\subsubsection{Unconditional generation}


In a recent work \cite{song2021sde} score-based  \cite{hyvarinen2005score_original, song2020generative_score} and diffusion-based \cite{sohldickstein2015diffusion_original, ho2020denoising} generative models have been unified into a single continuous-time score-based framework with diffusion driven by stochastic differential equations.  This continuous-time score-based diffusion technique relies on Anderson's Theorem \cite{anderson1982reverse_time_sde}, which states that (under certain assumptions on $\mu : \mathbb{R}^{n_x} \times \mathbb{R} \xrightarrow{} \mathbb{R}^{n_x}$ and $\sigma : \mathbb{R} \xrightarrow{} \mathbb{R}$) a forward diffusion process
\begin{gather}
\label{eq:forward_sde}
 dx = \mu(x,t)dt+\sigma(t)dw   
\end{gather} 
has a reverse diffusion process governed by the following SDE:
\begin{gather}
\label{eq:reverse_sde}
    dx=[\mu(x,t)-\sigma(t)^{2}\nabla_{x}{\ln{p_{X_t}(x)}}]dt + \sigma(t)d\Bar{w},
\end{gather}
where $\Bar{w}$ is a standard Wiener process in reverse time. 

The forward diffusion process transforms the \textit{target distribution} $p(x_0)$ to a \textit{diffused distribution} $p(x_T)$. By appropriately selecting the drift and the diffusion coefficients of the forward SDE, we can make sure that after sufficiently long time $T$, the diffused distribution $p(x_T)$ approximates a simple distribution, such as $\mathcal{N}(0,I)$. We refer to this simple distribution as the \textit{prior distribution}, denoted by $\pi$. 

If we have access to the score of the marginal distribution, $\nabla_{x_t}{\ln{p(x_t)}}$, for all $t$, we can derive the reverse diffusion process and simulate it to map $p_T$ to $p_0$. In practice, we approximate the score of the time-dependent distribution by a neural network $s_{\theta}(x_t,t) \approx \nabla_{x_t}{\ln{p(x_t)}}$ and map the prior distribution $\pi \approx p(x_T)$ to $p_\theta(x) \approx p(x_0)$ by solving the reverse-time SDE from time $T$ to time $0$. One can integrate the reverse SDE using standard numerical SDE solvers such Euler–Maruyama or other discretisation strategies. The authors propose to couple the standard integration step with a fixed number of Langevin MCMC steps to leverage the knowledge of the score of the distribution at each intermediate timestep. The MCMC correction step improves sampling; the combined algorithm is known as a predictor-corrector scheme.  We refer to \cite{song2021sde} for details.

In order to fit a neural network model $s_\theta(x_t,t)$ to approximate the score $\nabla_{x_t}{\ln{p(x_t)}}$, we minimize the weighted Fisher's divergence
\begin{gather}
    \mathcal{L}_{SM}(\theta) := \frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &x_t \sim p(x_t)}} [\lambda(t) \norm{\nabla_{x_t}{\ln{p(x_t)}} - s_\theta(x_t,t)}_2^2]
\end{gather}
where $\lambda: [0,T] \xrightarrow{} \mathbb{R}_+$ is a positive weighting function.

The above quantity cannot be optimized directly since we don't have access to the ground truth score $\nabla_{x_t}{\ln{p(x_t)}}$. Therefore in practice, a different objective has to be used \cite{hyvarinen2005score_original, song2020generative_score, song2021sde}. In \cite{song2021sde}, the continuous denoising score-matching objective is chosen, which is equal to $\mathcal{L}_{SM}(\theta)$ up to an additive term, which does not depend on $\theta$ and is defined as 
\begin{gather}
\begin{aligned}
    &\mathcal{L}_{DSM}(\theta) := \\ 
    &\frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &x_0 \sim p(x_0) \\ &x_t \sim p(x_t | x_0)}} [\lambda(t) \norm{\nabla_{x_t}{\ln{p(x_t | x_0)}} - s_\theta(x_t,t)}_2^2]
\end{aligned}
\end{gather}
The above expression involves only $\nabla_{x_t}{\ln{p(x_t | x_0)}}$ which can be computed analytically from the transition kernel of the forward diffusion process (provided that $\mu$ and $\sigma$ are sufficiently simple). The expectation can be approximated using Monte Carlo estimation in the following way: First, we sample time points $t_i$ from the uniform distribution on $[0,T]$, then we sample points $\tilde{x}_i$ from the target distribution $p_0$ (available via training set). Next, for each $\tilde{x}$, we sample points $x_i$ from the transition kernel $p(x_t | \tilde{x})$. Finally, we average the expression inside the expectation over all samples obtained in this way.

\subsection{Conditional generation}
The continuous score-matching framework can be extended to conditional generation, as shown in  \cite{song2021sde}. Suppose we are interested in $p(x|y)$, where $x$ is a \textit{target image} and $y$ is a \textit{condition image}. Again, we use the forward diffusion process (Equation \ref{eq:forward_sde}) to obtain a family of diffused distributions $p(x_t | y)$ and apply Anderson's Theorem to derive the \textit{conditional reverse-time SDE}
\begin{equation}
    \label{eq:conditional_reverse_sde}
    dx = [\mu(x,t) - \sigma(t)^2 \nabla_{x} \ln p_{X_t}(x | y)]dt + \sigma(t)d\tilde{w}.
\end{equation}
Now we need to learn the score $\nabla_{x_t} \ln p(x_t|y)$ in order to be able to sample from $p(x | y)$ using reverse-time diffusion.\\

In this work, we discuss the following approaches to estimating the conditional score $\nabla_{x_t} \ln p(x_t|y)$:
\begin{enumerate}
    \item Conditional denoising estimators
    \item Conditional diffusive estimators
    \item Multi-speed conditional diffusive estimators (our method)
\end{enumerate}
We discuss each of them in a separate section.\\

In \cite{song2021sde} an additional approach to conditional score estimation was suggested:
This method proposes learning $\nabla_{x_t} \ln p(x_t)$ with an unconditional score model, and  learning $p(y | x_t)$ with an auxiliary model. Then, one can use 
    \begin{equation*}
        \nabla_{x_t}\ln p(x_t |y) = \nabla_{x_t} \ln p(x_t) + \nabla_{x_t} \ln p(y | x_t)
    \end{equation*}
to obtain $\nabla_{x_t} \ln p(x_t | y)$. Unlike other approaches, this requires training a separate model for $p(y|x_t)$. Appropriate choices of such models for tasks discussed in this paper have not been explored yet. Therefore we exclude this approach from our study. 

\subsubsection{Conditional denoising estimator (CDE)}
The conditional denoising estimator (CDE) is a way of estimating $p(x_t|y)$ using the denoising score matching approach \cite{vincent2011connection, song2020generative_score}. In order to approximate $p(x_t|y)$, the conditional denoising estimator minimizes
\begin{gather}
\begin{aligned}
        \label{CDN}
        &\frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &x_0, y \sim p(x_0, y) \\ &x_t \sim p(x_t | x_0)}} 
        [\lambda(t) \norm{\nabla_{x_t} \ln{p(x_t | x_0)} - s_\theta(x_t, y, t)}_2^2]
\end{aligned}
\end{gather}
This estimator has been shown to be successful in previous works \cite{saharia2021sr3,tashiro2021csdi}, also confirmed in our experimental findings (cf. Section \ref{sec:experiments}). 

Despite the practical success, this estimator has previously been used without a theoretical justification of why training the above objective yields the desired conditional distribution. Since $p(x_t|y)$ does not appear in the training objective, it is not obvious that the minimizer approximates the correct quantity. 

By extending the arguments of \cite{vincent2011connection}, we provide a formal proof that the minimizer of the above loss does indeed approximate the correct conditional score $p(x_t|y)$. This is expressed in the following theorem.

\begin{theorem}
    \label{thm:CDE_consistency}
    The minimizer (in $\theta$) of
    \begin{gather*}
    \begin{aligned}
            \frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &x_0, y \sim p(x_0, y) \\ &x_t \sim p(x_t | x_0)}} 
            [\lambda(t) \norm{\nabla_{x_t} \ln{p(x_t | x_0)} - s_\theta(x_t, y, t)}_2^2]
    \end{aligned}
    \end{gather*}    
    is the same as the minimizer of 
    \begin{gather*}
        \frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &x_t, y \sim p(x_t, y)}} 
        [\lambda(t) \norm{\nabla_{x_t} \ln{p(x_t | y)} - s_\theta(x_t, y,t)}_2^2]
    \end{gather*}
\end{theorem}
\noindent
The proof for this statement can be found in Appendix \ref{appendix:minimizers}. 
Using the above theorem, the consistency of the estimator can be established.
\begin{corollary}
    Let $\theta^\ast$ be a minimizer of a Monte Carlo approximation of (\ref{CDN}), then (under technical assumptions, cf. Appendix \ref{appendix:consistency}) the conditional denoising estimator $s_{\theta^\ast}(x,y,t)$ is a consistent estimator of the conditional score $\nabla_{x_t} \ln p(x_t | y)$, i.e.
    \begin{gather*}
        s_{\theta^\ast}(x,y,t) \overset{P}{\to} \nabla_{x_t} \ln p(x_t | y)   
    \end{gather*}
    as the number of Monte Carlo samples approaches infinity.
\end{corollary}
\noindent
This follows from the previous theorem and the uniform law of large numbers. Proof in the Appendix \ref{appendix:consistency}.

\subsubsection{Conditional diffusive estimator (CDiffE)}
\label{sec:CDiffE}
Conditional diffusive estimators (CDiffE) have first been suggested in \cite{song2021sde}. The core idea is that instead of learning $p(x_t | y)$ directly, we diffuse both $x$ and $y$ and approximate $p(x_t | y_t)$, using the denoising score matching. Just like learning diffused distribution $\nabla_{x_t} \ln p(x_t)$ improves upon direct estimation of $\nabla_{x} \ln p(x)$ \cite{song2020generative_score, song2021sde}, diffusing both the input $x$ and condition $y$, and then learning $\nabla_{x_t} \ln p(x_t | y_t)$ could make optimization easier and give better results than learning  $\nabla_{x_t} \ln p(x_t | y)$ directly.
    
In order to learn $p(x_t | y_t)$, observe that
\begin{gather*}
    \nabla_{x_t}\ln p(x_t | y_t) = \nabla_{x_t}\ln p(x_t, y_t) = \nabla_{z_t}\ln p(z_t)[:n_x],
\end{gather*}
where $z_t := (x_t, y_t)$ and $n_x$ is the dimensionality of $x$. Therefore we can learn the (unconditional) score of the joint distribution $p(x_t, y_t)$ using the denoising score matching objective just like as in the unconditional case, i.e
\begin{gather}
    \label{CDF}
\begin{aligned}
    &\frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &z_0 \sim p_0(z_0) \\ &z_t \sim p(z_t | z_0)}} [\lambda(t) \norm{\nabla_{z_t} \ln{p(z_t |z_0)} - s_\theta(z_t,t)}_2^2].
\end{aligned}
\end{gather}
We can then extract our approximation for the conditional score $\nabla_{x_t} \ln p(x_t|y_t)$ by simply taking the first $n_x$ components of $s_\theta(x_t, y_t,t)$.

The aim is to approximate $\nabla_{x_t} \ln p(x_t | y)$ with $\nabla_{x_t} \ln p(x_t|\hat{y_t})$, where $\hat{y}_t$ is a sample from $p(y_t | y)$. Of course this approximation is imperfect and introduces an error, which we call the \textit{approximation error}. CDiffE aims to achieve smaller optimization error by diffusing the condition $y$ and making the optimization landscape easier, at a cost of making this approximation error.

Now in order to obtain samples from the conditional distribution, we sample a point $x_T \sim \pi$ and integrate
\begin{gather*}
    dx = [\mu(x,t) - \sigma(t)^2 \nabla_{x} \ln p_{X_t|Y_t}(x | \hat{y}_t)]dt + \sigma(t)d\tilde{w}
\end{gather*}
from $T$ to $0$, sampling $\hat{y}_t \sim p(y_t | y)$ at each time step.

\subsubsection{Conditional multi-speed diffusive estimator (CMDE)}
\label{sec:CMDE}
\begin{figure}
\begin{mdframed}
    \begin{center}
       \textbf{Sources of error for different estimators}
    \end{center}
    \textbf{CDE}\\
    Optimization error:
    \begin{gather*}
    s_\theta(x,y,t) \approx \nabla_{x_t}\ln p(x_t|y)   
    \end{gather*}
    \textbf{CDiffE and CMDE}\\
    Optimization error:
    \begin{gather*}
        s_\theta(x,y,t) \approx \nabla_{x_t}\ln p(x_t|y_t)   
        \end{gather*}
    Approximation error:
    \begin{gather*}
        \nabla_{x_t}\ln p(x_t|\hat{y}_t) \approx \nabla_{x_t}\ln p(x_t|y)   
        \end{gather*}
    CDiffE aims to achieve smaller optimization error at a cost of higher approximation error. By controlling the diffusion speed of $y$, CMDE tries to find an optimal balance between optimization error and approximation error.
\end{mdframed}
\caption{Sources of error for different estimators}
\label{fig:box}
\end{figure}
In this section we present a novel estimator for the conditional score $\nabla_{x_t} \ln p(x_t | y)$ which we call the \textit{conditional multi-speed diffusive estimator} (CMDE). 

Our approach is based on two insights. Firstly, there is no reason why $x_t$ and $y_t$ in conditional diffusive estimation need to diffuse at the same rate. Secondly, by decreasing the diffusion rate of $y_t$ while keeping the diffusion speed of $x_t$ the same, we can bring $p(x_t |y_t)$ closer to $p(x_t |y)$, at the possible cost of making the optimization more difficult. This way we can \emph{interpolate} between the conditional denoising estimator and the conditional diffusive estimator and find an optimal balance between optimization error and approximation error (cf. Figure \ref{fig:box}). This can lead to a better performance, as indicated by our experimental findings (cf. Section \ref{sec:experiments}).

In our conditional multi-speed diffusive estimator, $x_t$ and $y_t$ diffuse according to SDEs with the same drift but different diffusion rates,
\begin{gather*}
    dx = \mu(x,t)dt+\sigma^x(t)dw  \\
    dy = \mu(y,t)dt+\sigma^y(t)dw.
\end{gather*}


Then, just like in the case of conditional diffusive estimator, we try to approximate the joint score $\nabla_{x_t, y_t} \ln p(x_t, y_t)$ with a neural network. Since $x_t$ and $y_t$ now diffuse according to different SDEs, we need to take this into account and replace the weighting function $\lambda(t):\mathbb{R} \xrightarrow{} \mathbb{R}_+ $ with a positive definite weighting matrix $\Lambda(t): \mathbb{R} \xrightarrow{} \mathbb{R}^{(n_x + n_y) \times (n_x + n_y)}$. Hence, the new training objective becomes
\begin{gather}
    \label{eq:CMDE}
    \begin{aligned}
        \frac{1}{2} \mathbb{E}_{\subalign{&t \sim U(0,T)\\ &z_0 \sim p_0(z_0) \\ &z_t \sim p(z_t | z_0)}} 
        [
            v^T \Lambda(t) v
        ],
    \end{aligned}
\end{gather}
where $v=\nabla_{z_t} \ln{p(z_t |z_0)} - s_\theta(z_t,t)$, $z_t=(x_t,y_t)$.

In \cite{song2021maximum} authors derive a likelihood weighting function $\lambda^{\text{MLE}}(t)$, which ensures that the objective of the score-based model upper-bounds the negative log-likelihood of the data, thus enabling approximate maximum likelihood training of score-based diffusion models. We generalize this result to the multi-speed diffusion case by providing a likelihood weighting matrix $\Lambda^{\text{MLE}}(t)$ with the same properties.

\begin{theorem}
    \label{thm:weightning}
    Let $\mathcal{L}(\theta)$ be the CMDE training objective (Equation \ref{eq:CMDE}) with the following weighting:
    \begin{gather*}
        \Lambda^{\text{MLE}}_{i,j}(t) =  
        \begin{cases} 
            \sigma^x(t)^2, \text{ if } i=j, \ i \leq n_x \\ 
            \sigma^y(t)^2, \text{ if } i=j, \ n_x < i \leq n_y  \\
            0, \text{ otherwise}
        \end{cases}            
    \end{gather*}
    Then the joint negative log-likelihood is upper bounded (up to a constant in $\theta$) by the training objective of CMDE
    \begin{gather*}
        -\mathbb{E}_{(x,y) \sim p(x,y)}[\ln p_\theta(x,y)] \leq \mathcal{L}(\theta) + C.
    \end{gather*}
\noindent
\end{theorem}

\noindent
The proof can be found in Appendix \ref{appendix:weighting}. 

Moreover we show that the mean squared approximation error of a multi-speed diffusion model is upper bounded and the upper bound goes to zero as the diffusion speed of the condition $\sigma^y(t)$ approaches zero.
\begin{theorem}
    Fix $t$, $x_t$ and $y$. Under mild technical assumptions (cf. Appendix \ref{appendix:mse}) there exists a function  $E: \mathbb{R} \xrightarrow{} \mathbb{R}$ monotonically decreasing to $0$, such that
    \begin{gather*}
        \mathbb{E}_{y_t \sim p(y_t|y)}[
            \norm{ \nabla_{x_t} \ln p(x_t|y_t) - \nabla_{x_t} \ln p(x_t|y)}_2^2
            ] \\
            \leq E(1/\sigma^y(t)).
    \end{gather*}
\end{theorem}
\noindent
The proof can be found in Appendix \ref{appendix:mse}.

Thus we see that the objective of CMDE approaches that of CDE as  $\sigma^y(t) \to 0$, and  CMDE coincides with CDiffE when $ \sigma^y(t) = \sigma^x(t)$ (cf. Figure \ref{fig:box}).

We experimented with different configurations of $\sigma^x(t)$ and $\sigma^y(t)$ and found configurations that lead to  improvements upon CDiffE and  CDE in certain tasks. The experimental results are discussed in detail in Section \ref{sec:experiments}.

\subsubsection{\texttt{MSDiff}: Beyond multi-speed diffusion}
\label{sec:multi-sde}

Based on this work, we provide our open source library \texttt{MSDiff}, which generalizes the original framework of \cite{song2021sde} and allows to diffuse $x_t$ and $y_t$ not only at different diffusion rates, but with two entirely different forward SDEs (i.e. with different diffusion coefficients \emph{and} different drifts):
\begin{gather*}
    dx = \mu^x(x,t)dt+\sigma^x(t)dw  \\
    dy = \mu^y(y,t)dt+\sigma^y(t)dw  
\end{gather*}
Moreover, the likelihood weighting of Theorem \ref{thm:weightning} holds in this more general case, allowing for principled training of multi-sde diffusion models (cf. Appendix \ref{appendix:weighting}).
This flexibility opens room for further research into multi-speed diffusion based training of score-based models, which we intend to examine in a future study. 

\section{Experiments}
\label{sec:experiments}

\begin{figure*}
    \captionsetup[subfigure]{labelformat=empty}
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/x.png}
        \caption{\scriptsize Original image $x$}
    \end{subfigure}
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/y.png}
        \caption{\scriptsize Observation $y := Ax$}
    \end{subfigure}
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/1.png}
        \caption{\scriptsize Reconstruction $\hat{x}_1$}
    \end{subfigure} 
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/2.png}
        \caption{\scriptsize Reconstruction $\hat{x}_2$}
    \end{subfigure}    
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/3.png}
        \caption{\scriptsize Reconstruction $\hat{x}_3$}
    \end{subfigure}    
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/4.png}
        \caption{\scriptsize Reconstruction $\hat{x}_4$}
    \end{subfigure}
    \begin{subfigure}{.135\textwidth}
        \includegraphics[width=\textwidth]{Chapter2/samples/diversity/5.png}
        \caption{\scriptsize Reconstruction $\hat{x}_5$}
    \end{subfigure} 
    \caption{Diversity of five different CMDE reconstructions for a given masked image.}
\end{figure*}

In this section we conduct a systematic comparison of different score-based diffusion approaches to modelling conditional distributions of image data. We evaluate these approaches on the tasks of super-resolution, inpainting and edge to image translation.  Moreover, we compare the most successful score-based diffusion approaches for super-resolution with HCFlow  \cite{liang2021hrflow} -- a state-of-the-art method in super-resolution.

\noindent
\textbf{Datasets} In our experiments, we use the CelebA \cite{2015celeba} and Edges2shoes \cite{yu2014sketch2shoe,isola2018pix2pix} datasets. We pre-processed the CelebA dataset as in \cite{liang2021hrflow}.

\noindent
\textbf{Models and hyperparameters} In order to ensure the fair comparison, we separate the evaluation of a particular estimator of conditional score from the evaluation of a particular neural network model. To this end, we train the same neural network architecture for all estimators. The architecture is based on the DDPM model used in \cite{ho2020denoising, song2021sde}. 
We used the variance-exploding SDE \cite{song2021sde} given by: 
$$dx = \sqrt{\frac{d}{dt}\sigma^2(t)}dw, \hspace{1cm}
\sigma(t) = \sigma_{min} \left(\frac{\sigma_{max}}{\sigma_{min}}\right)^t$$
Likelihood weighting was employed for all experiments. For CMDE, the diffusion speed of $y$ was controlled by picking an appropriate $\sigma^y_{\max}$, which we found by trial-and-error. The performance of CMDE could be potentially improved by performing a systematic hyperparameter search for optimal $\sigma^y_{\max}$.
Details on hyperparameters and architectures used in our experiments can be found in Appendix \ref{appendix:hyperparams}.

\noindent 
\textbf{Inverse problems} The tasks of inpainting, super-resolution and edge to image translation are special cases of inverse problems \cite{arridge2019ip, muller2012ip}. In each case, we are given a (possibly random) forward operator $A$ which maps our data $x$ (full image) to an observation $y$ (masked image, compressed image, sketch). The task is to come up with a high-quality reconstruction $\hat{x}$ of the image $x$ based on an observation $y$. The problem of reconstructing $x$ from $y$ is typically ill-posed, since $y$ does not contain all information about $x$. Therefore, an ideal algorithm would produce a reconstruction $\hat{x}$, which looks like a realistic image (i.e. is a likely sample from $p(x)$) and is consistent with the observation $y$ (i.e. $A\hat{x} \approx y$). Notice that if a conditional score model learns the conditional distribution correctly, then our reconstruction $\hat{x}$ is a sample from the posterior distribution $p(x | y)$, which satisfies bespoke requirements. This strategy for solving inverse problems is generally referred to as \emph{posterior sampling}.

\noindent
\textbf{Evaluation: Reconstruction quality} Ill-posedness often means that we should not strive to reconstruct $x$ perfectly. Nonetheless reconstruction error does correlate with the performance of the algorithm and has been one of the most widely-used metrics in the community. To evaluate the reconstruction quality for each task, we measure the Peak signal-to-noise ratio (PSNR) \cite{zhou2004psnr+ssim}, Structural similarity index measure (SSIM) \cite{zhou2004psnr+ssim} and Learned Perceptual Image Patch Similarity (LPIPS) \cite{zhang2018lpips} between the original image $x$ and the reconstruction $\hat{x}$.

\noindent
\textbf{Evaluation: Consistency}
In order to evaluate the consistency of the reconstruction, for each task we calculate the PSNR between $y:=Ax$ and $\hat{y}:=A\hat{x}$. 

\noindent
\textbf{Evaluation: Diversity}
We evaluate diversity of each approach by generating five reconstructions $(\hat{x})_{i=1}^5$ for a given observation ${y}$. Then for each $y$ we calculate the average standard deviation for each pixel among the reconstructions  $(\hat{x})_{i=1}^5$ . Finally, we average this quality over 5000 test observations.

\noindent
\textbf{Evaluation: Distributional distances}
If our algorithm generates realistic reconstructions while preserving diversity, then the distribution of reconstructions $p(\hat{x})$ should be similar to the distribution of original images $p(x)$. Therefore, we measure the Fr\'{e}chet Inception Distance (FID) \cite{heusel2018fid} between unconditional distributions $p(x)$ and $p(\hat{x})$ based on 5000 samples. Moreover, we calculate the FID score between the joint distributions $p(\hat{x}, y)$ and $p(x,y)$, which allows us to simultaneously check the realism of the reconstructions and the consistency with the observation. 
We use abbreviation UFID to refer to FID between between unconditional distributions and JFID to refer to FID between joints.  In our judgement, FID and especially the JFID is the most principled of the used metrics, since it measures how far $p_\theta(x | y)$ is from $p(x|y)$.

\begin{table*}
  \begin{center}
  \caption{Results of conditional generation tasks.}
  \label{tbl:results}
  \begin{tabular}{cccccccc}
  \toprule
  &Estimator & PSNR/SSIM $\uparrow$  & LPIPS $\downarrow$ & UFID/JFID $\downarrow$ & Consistency $\uparrow$ & Diversity $\uparrow$ \\
  \midrule
  \multirow{3}{*}{Inpainting} 
  &CDE & \textbf{25.12}/\textbf{0.870}  & \textbf{0.042} & 13.07/18.06 & \textbf{28.54} & 4.79  \\
  &CDiffE & 23.07/0.844   & 0.057 & 13.28/19.25 &  26.61 & \textbf{6.52}   \\
  &CMDE ($\sigma^y_{max} = 1$) & 24.92/0.864  & 0.044 & \textbf{12.07/17.07} & 28.32 & 4.98  \\
  \midrule
  \multirow{4}{*}{Super-resolution} 
  &CDE & 23.80/0.650  & 0.114 & 10.36/15.77 & 54.18 & \textbf{8.51}  \\
  &CDiffE & 23.83/0.656  & 0.139 & 14.29/20.20 & 51.90 & 7.41  \\
  &CMDE ($\sigma^y_{max} = 0.5$) & 23.91/0.654  & 0.109 & \textbf{10.28/15.68} & 53.03 & 8.33  \\
  &HCFLOW & \textbf{24.95/0.702} & \textbf{0.107} & 14.13/19.55 & \textbf{55.31} & 6.26 \\
  \midrule
  \multirow{3}{*}{Edge to image} 
  &CDE & \textbf{18.35/0.699}  & \textbf{0.156} & \textbf{11.87/21.31} & \textbf{10.45} & 14.40 \\
  &CDiffE & 10.00/0.365   & 0.350 & 33.41/55.22  & 7.78 & \textbf{43.45} \\
  &CMDE ($\sigma^y_{max} = 1$) & 18.16/0.692  & 0.158 & 12.62/22.09 & 10.38 & 15.20  \\
  \bottomrule
  \end{tabular}
  \end{center}
\end{table*}

\subsection{Inpainting}

We perform the inpainting experiment using CelebA dataset. In inpainting, the forward operator $A$ is an application of a given binary mask to an image $x$.  In our case, we made the task more difficult by using randomly placed (square) masks. Then the conditional score model is used to obtain a reconstruction $\hat{x}$ from the masked image $y$. We select the position of the mask uniformly at random and cover $25\%$ of the image. The quantitative results are summarised in Table \ref{tbl:results} and samples are presented in Figure \ref{fig:inpainting}. We observe that CDE and CMDE significantly outperform CDiffE in all metrics, with CDE having a small advantage over CMDE in terms of reconstruction error and consistency. On the other hand, CMDE achieves the best FID scores.

\begin{figure*}
  \begin{center}
      \begingroup
      \setlength{\tabcolsep}{2pt}
  \begin{tabular}{ccccc}
      Original image $x$ & Observation $y$ & CDE & CDiffE & CMDE (Ours) \\

      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/1/x.png} &   
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/1/y.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/1/sr3.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/1/Song.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/1/DV.png} \\

      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/2/x.png} &   
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/2/y.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/2/sr3.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/2/Song.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/2/DV.png} \\

      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/3/x.png} &   
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/3/y.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/3/sr3.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/3/Song.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/inpainting/table/3/DV.png} \\
  \end{tabular}
  \endgroup
  \end{center}
  \caption{Inpainting results.}
  \label{fig:inpainting}
\end{figure*}


\subsection{Super-resolution}
We perform 8x super-resolution using the CelebA dataset. A high resolution 160x160 pixel image $x$ is compressed to a low resolution 20x20 pixels image $y$. Here we use bicubic downscaling \cite{keyes1981bicubic} as the forward operator  $A$. Then using a score model we obtain a 160x160 pixel reconstruction image $\hat{x}$. The quantitative results are summarised in Table \ref{tbl:results} and samples are presented in Figure \ref{fig:super-resolution}. We find that CMDE and CDE perform similarly, while significantly outperforming CDiffE. CMDE achieves the smallest reconstruction error and captures the distribution most accurately according to FID scores. 



\begin{figure*}
  \begin{center}
  \begingroup
  \setlength{\tabcolsep}{2pt}
  %\renewcommand{\arraystretch}{1.5}

  \begin{tabular}{cccccc}
      Original image $x$ & Observation $ y$ & HCFlow & CDE & CDiffE & CMDE (Ours) \\

      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/1/x.png} &   
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/1/y.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/1/hcflow.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/1/sr3.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/1/Song.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/1/DV.png} \\

      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/2/x.png} &   
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/2/y.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/2/hcflow.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/2/sr3.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/2/Song.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/2/DV.png} \\

      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/3/x.png} &   
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/3/y.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/3/hcflow.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/3/sr3.png} & 
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/3/Song.png} &
      \includegraphics[width=.15\textwidth]{Chapter2/samples/super-resolution/table/3/DV.png} \\
  \end{tabular}
  \endgroup
  \end{center}
  \caption{Super-resolution results.}
  \label{fig:super-resolution}
\end{figure*}



\subsection{Edge to image translation}
We perform an edge to image translation task on the Edges2shoes dataset. The forward operator $A$ is given by a neural network edge detector \cite{xie2015edges}, which takes an original photo of a shoe $x$ and transforms it into a sketch $y$. Then a conditional score model is used to create an artificial photo of a shoe $\hat{x}$ matching the sketch. The quantitative results are summarised in Table \ref{tbl:results} and samples are presented in Figure \ref{fig:edges-to-shoes}. Unlike in inpainting and super-resolution where CDiffE achieved reasonable performance, in edge to image translation, it fails to create samples consistent with the condition (which leads to inflated diversity scores). CDE and CMDE are comparable, but CDE performed slightly better across all metrics. However, the performance of CMDE could be potentially improved by  tuning the diffusion speed $\sigma^y(t)$.

\begin{figure}
\renewcommand{\arraystretch}{1.25}
  \begin{tabular}{lccc}
      \begin{tabular}{@{}l@{}}
          Original image $x$
          \\[25pt]
      \end{tabular}
       & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/1/x.png} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/2/x.png} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/3/x.png} \\
      
      \begin{tabular}{@{}l@{}}
          Observation \\ $ y := Ax$
          \\[25pt]
      \end{tabular}
       & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/1/y.png} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/2/y.png} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/3/y.png} \\
      
       \begin{tabular}{@{}c@{}}
          CDE
          \\[25pt]
      \end{tabular} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/1/sr3.png}  & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/2/sr3.png}  & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/3/sr3.png} \\

      \begin{tabular}{@{}c@{}}
          CDiffE
          \\[25pt]
      \end{tabular} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/1/Song.png} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/2/Song.png} & \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/3/Song.png}\\

      \begin{tabular}{@{}c@{}}
          CMDE (Ours)
          \\[25pt]
      \end{tabular}  &  \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/1/DV.png} &  \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/2/DV.png} &  \includegraphics[width=.08\textwidth]{Chapter2/samples/edges-to-shoes/table/3/DV.png} 
  \end{tabular}
  \caption{Edge to image translation results.}
  \label{fig:edges-to-shoes}
\end{figure}



\section{Comparison with state-of-the-art}
We compare score-based diffusion approaches with HCFlow \cite{liang2021hrflow} -- a state-of-the-art method in super-resolution. To ensure a fair comparison, we used the data pre-processing and hyperparameters for HCFlow exactly as in the original paper \cite{liang2021hrflow}. We find that although HCFlow performs marginally better in terms of the reconstruction error, CDE and CMDE obtain a significantly better FID and diversity scores indicating better distribution coverage. 
%We reiterate that minimizing the reconstruction error is not the ultimate goal due to ill-posedness. The method should aim to sample from the posterior distribution $p(x|y)$ rather than trying to exactly recreate $x$. 
We recall that a perfect reconstruction on a per-image basis is generally not desirable due to ill-posedness of the inverse problem and therefore in our view FID is the most principled of the used metrics.
The FID scores suggest that CMDE was the most successful approach to approximating the posterior distribution.




\section{Conclusions and future work}

In this article, we conducted a systematic comparison of score-based diffusion models in conditional image generation tasks and provided an in-depth theoretical analysis of the estimators of conditional score. In particular, we proved the consistency of the conditional denoising estimator, thus providing a firm theoretical justification for using it in future research. 

Moreover, we introduced a multi-speed diffusion framework, which led to CMDE, a novel estimator for conditional score which interpolates between conditional denoising estimator (CDE) and conditional diffusive estimator (CDiffE) by controlling the diffusion speed of the condition.

Our study showed that CMDE and CDE perform on par, while significantly outperforming CDiffE. This is particularly apparent in edge to image translation, where CDiffE fails to produce samples consistent with the condition image. Furthermore, CMDE outperformed CDE in terms of  FID scores in inpainting and super-resolution tasks, which indicates that diffusing the condition at the appropriate speed can have beneficial effect on the optimization landscape, and yield better approximation of the posterior distribution.

We found that score-based diffusion models perform on par with prior state-of-the-art methods in super-resolution task and achieve better posterior approximation according to FID score.

\section{Acknowledgements} 
GB acknowledges the support from GSK and the Cantab Capital Institute for the Mathematics of Information. 
JS acknowledges the support from Aviva and the Cantab Capital Institute for the Mathematics of Information. 
CBS acknowledges support from the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRC advanced career fellowship EP/V029428/1, EPSRC grants EP/S026045/1 and EP/T003553/1, EP/N014588/1, EP/T017961/1, the Wellcome Innovator Award RG98755, the Leverhulme Trust project Unveiling the invisible, the European Union Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 777826 NoMADS, the Cantab Capital Institute for the Mathematics of Information and the Alan Turing Institute. CE acknowledges support from
the Wellcome Innovator Award RG98755 for part of the work that was done at Cambridge.
