\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\citation{fefferman2016testing}
\citation{belkin2001laplacian,coifman2006diffusion,roweis2000nonlinear,sammon1969nonlinear,tenenbaum2000global}
\citation{demers1992non,kingma2013auto}
\citation{chow2022predicting,gomari2022variational,ternes2022multi,vahdat2020nvae,zhong2021cryodrgn}
\citation{arvanitidis2016locally,diepeveen2024pulling,hauberg2012geometric,peltonen2004improved,Scarvelis2023,sorrenson2024learningdistancesdatanormalizing,sun2024geometryaware}
\citation{sorrenson2024learningdistancesdatanormalizing}
\citation{dinh2017density,song2020score}
\citation{sakamoto2024the,stanczuk2022your}
\citation{sorrenson2024learningdistancesdatanormalizing}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Score-based pullback Riemannian geometry}{75}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{75}{section.5.1}\protected@file@percent }
\citation{diepeveen2023curvature,fletcher2004principal}
\citation{chen2023riemannian,huang2022riemannian}
\citation{diepeveen2024pulling}
\citation{kapusniak2024metricflowmatchingsmooth}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  Approximate data manifolds learned by the Riemannian autoencoder generated by score-based pullback Riemannian geometry for three datasets. The orange surfaces represent the manifolds learned by the model, while the blue points correspond to the training data. Each manifold provides a convincing low-dimensional representation of the data, isometric to its respective latent space. }}{76}{figure.caption.51}\protected@file@percent }
\newlabel{fig:learned_charts}{{5.1}{76}{Approximate data manifolds learned by the Riemannian autoencoder generated by score-based pullback Riemannian geometry for three datasets. The orange surfaces represent the manifolds learned by the model, while the blue points correspond to the training data. Each manifold provides a convincing low-dimensional representation of the data, isometric to its respective latent space}{figure.caption.51}{}}
\citation{diepeveen2024pulling}
\citation{sorrenson2024learningdistancesdatanormalizing}
\citation{diepeveen2024pulling}
\newlabel{fig:toy-example-barycentre}{{5.2a}{77}{Barycentre}{figure.caption.52}{}}
\newlabel{sub@fig:toy-example-barycentre}{{a}{77}{Barycentre}{figure.caption.52}{}}
\newlabel{fig:toy-example-geodesic}{{5.2b}{77}{Geodesic}{figure.caption.52}{}}
\newlabel{sub@fig:toy-example-geodesic}{{b}{77}{Geodesic}{figure.caption.52}{}}
\newlabel{fig:toy-example-log}{{5.2c}{77}{Logarithm}{figure.caption.52}{}}
\newlabel{sub@fig:toy-example-log}{{c}{77}{Logarithm}{figure.caption.52}{}}
\newlabel{fig:toy-example-exp}{{5.2d}{77}{Exponential}{figure.caption.52}{}}
\newlabel{sub@fig:toy-example-exp}{{d}{77}{Exponential}{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Score-based pullback Riemannian geometry (proposed) from a toy probability density.}}{77}{figure.caption.52}\protected@file@percent }
\newlabel{fig:toy-example}{{5.2}{77}{Score-based pullback Riemannian geometry (proposed) from a toy probability density}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Contributions}{77}{subsection.5.1.1}\protected@file@percent }
\citation{boothby2003introduction,carmo1992riemannian,lee2013smooth,sakai1996riemannian}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Outline}{78}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Notation}{78}{section.5.2}\protected@file@percent }
\newlabel{sec:notation}{{5.2}{78}{Notation}{section.5.2}{}}
\newlabel{eq:pull-back-metric}{{5.2}{79}{Notation}{equation.5.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Riemannian geometry from unimodal probability densities}{79}{section.5.3}\protected@file@percent }
\newlabel{sec:unimodal-riemannian geometry}{{5.3}{79}{Riemannian geometry from unimodal probability densities}{section.5.3}{}}
\newlabel{eq:stroco-diffeo-density}{{5.3}{79}{Riemannian geometry from unimodal probability densities}{equation.5.3.3}{}}
\citation{diepeveen2024pulling}
\citation{diepeveen2024pulling}
\newlabel{eq:stroco-diffeo-pullback}{{5.4}{80}{Riemannian geometry from unimodal probability densities}{equation.5.3.4}{}}
\newlabel{eq:quadratic-stroco}{{5.6}{80}{Riemannian geometry from unimodal probability densities}{equation.5.3.6}{}}
\newlabel{thm:pull-back-mappings}{{5.3.1}{80}{}{theorem.5.3.1}{}}
\newlabel{eq:thm-geodesic-remetrized}{{5.8}{81}{}{equation.5.3.8}{}}
\newlabel{eq:thm-geodesic-remetrized-special-psi}{{5.9}{81}{}{equation.5.3.9}{}}
\newlabel{eq:thm-log-remetrized}{{5.10}{81}{}{equation.5.3.10}{}}
\newlabel{eq:thm-log-remetrized-special-psi}{{5.11}{81}{}{equation.5.3.11}{}}
\newlabel{eq:thm-exp-remetrized}{{5.12}{81}{}{equation.5.3.12}{}}
\newlabel{eq:thm-exp-remetrized-special-psi}{{5.13}{81}{}{equation.5.3.13}{}}
\newlabel{eq:thm-distance-remetrized}{{5.14}{81}{}{equation.5.3.14}{}}
\newlabel{eq:thm-distance-remetrized-special-psi}{{5.15}{81}{}{equation.5.3.15}{}}
\citation{diepeveen2024pulling}
\citation{diepeveen2024pulling}
\newlabel{eq:stroco-diffeo-barycentre-formal}{{5.16}{82}{}{equation.5.3.16}{}}
\newlabel{eq:diffeo-barycentre-formal}{{5.17}{82}{}{equation.5.3.17}{}}
\newlabel{rem:stability-manifold-mappings}{{5.3.2}{82}{}{theorem.5.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Riemannian autoencoder from unimodal probability densities}{82}{section.5.4}\protected@file@percent }
\newlabel{sec:rae}{{5.4}{82}{Riemannian autoencoder from unimodal probability densities}{section.5.4}{}}
\citation{kingma2013auto}
\newlabel{eq:dimind-epsilon}{{5.19}{83}{Riemannian autoencoder from unimodal probability densities}{equation.5.4.19}{}}
\newlabel{eq:rae-encoder}{{5.20}{83}{Riemannian autoencoder from unimodal probability densities}{equation.5.4.20}{}}
\newlabel{eq:rae-decoder}{{5.21}{83}{Riemannian autoencoder from unimodal probability densities}{equation.5.4.21}{}}
\newlabel{thm:rae-error}{{5.4.1}{83}{}{theorem.5.4.1}{}}
\newlabel{eq:thm-rae-bound}{{5.22}{83}{}{equation.5.4.22}{}}
\newlabel{eq:RAEinvdiffeoRegConstantB}{{5.23}{83}{}{equation.5.4.23}{}}
\newlabel{rem:interpretability-rae}{{5.4.2}{83}{}{theorem.5.4.2}{}}
\citation{dinh2017density}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Learning unimodal probability densities }{84}{section.5.5}\protected@file@percent }
\newlabel{sec:adapting-normalizing-flows}{{5.5}{84}{Learning unimodal probability densities}{section.5.5}{}}
\newlabel{eq:nf-density}{{5.26}{84}{Learning unimodal probability densities}{equation.5.5.26}{}}
\citation{durkan2019neural}
\citation{dinh2017density}
\citation{diepeveen2024pulling}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Experiments}{85}{section.5.6}\protected@file@percent }
\newlabel{sec:numerics}{{5.6}{85}{Experiments}{section.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Manifold mappings}{85}{subsection.5.6.1}\protected@file@percent }
\newlabel{sec:manifold-mappings-experiments}{{5.6.1}{85}{Manifold mappings}{subsection.5.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison of geodesics computed using different methods on the river dataset. The geodesics generated by the proposed method have least artifacts, which is in line with our expectations from Table~\ref {tab:geodesic-variation-errors}.}}{86}{figure.caption.54}\protected@file@percent }
\newlabel{fig:geodesics_comparison}{{5.3}{86}{Comparison of geodesics computed using different methods on the river dataset. The geodesics generated by the proposed method have least artifacts, which is in line with our expectations from Table~\ref {tab:geodesic-variation-errors}}{figure.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Comparison of evaluation metrics for different methods across three datasets. Best-performing results for each metric are highlighted in bold. Values are reported as mean (std). The proposed method performs best in all metrics on each data set.}}{87}{table.caption.53}\protected@file@percent }
\newlabel{tab:geodesic-variation-errors}{{5.1}{87}{Comparison of evaluation metrics for different methods across three datasets. Best-performing results for each metric are highlighted in bold. Values are reported as mean (std). The proposed method performs best in all metrics on each data set}{table.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Riemannian autoencoder}{87}{subsection.5.6.2}\protected@file@percent }
\newlabel{sec:RAE-experiments}{{5.6.2}{87}{Riemannian autoencoder}{subsection.5.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{1D and 2D manifolds}{87}{section*.55}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces  Approximate data manifold learned by the Riemannian autoencoder for the Sinusoid(1, 100) dataset. The orange curves depict the manifold learned by the model, while the blue points show the training data. We visualize three different combinations of the ambient dimensions. }}{88}{figure.caption.56}\protected@file@percent }
\newlabel{fig:learned_charts_for_Sinusoid_1_100}{{5.4}{88}{Approximate data manifold learned by the Riemannian autoencoder for the Sinusoid(1, 100) dataset. The orange curves depict the manifold learned by the model, while the blue points show the training data. We visualize three different combinations of the ambient dimensions}{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{Higher-dimensional manifolds}{88}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Conclusions}{89}{section.5.7}\protected@file@percent }
\newlabel{sec:conclusions}{{5.7}{89}{Conclusions}{section.5.7}{}}
\newlabel{fig:hemisphere_variances}{{5.5a}{90}{Learned variances in decreasing order}{figure.caption.58}{}}
\newlabel{sub@fig:hemisphere_variances}{{a}{90}{Learned variances in decreasing order}{figure.caption.58}{}}
\newlabel{fig:hemisphere_reconstruction_errors}{{5.5b}{90}{Reconstruction error for three latent orders}{figure.caption.58}{}}
\newlabel{sub@fig:hemisphere_reconstruction_errors}{{b}{90}{Reconstruction error for three latent orders}{figure.caption.58}{}}
\newlabel{fig:sinusoid_variances}{{5.5c}{90}{Learned variances in decreasing order}{figure.caption.58}{}}
\newlabel{sub@fig:sinusoid_variances}{{c}{90}{Learned variances in decreasing order}{figure.caption.58}{}}
\newlabel{fig:sinusoid_reconstruction_errors}{{5.5d}{90}{Reconstruction error for three latent orders}{figure.caption.58}{}}
\newlabel{sub@fig:sinusoid_reconstruction_errors}{{d}{90}{Reconstruction error for three latent orders}{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Learned variances and reconstruction errors for the Hemisphere(5,20) and Sinusoid(5,20) datasets. The plots in the left column show the learned variances in decreasing order for each dataset, while the right column illustrates the average $\ell ^2$ reconstruction error as a function of the number of latent dimensions used. The reconstruction errors are evaluated for three variance-based orders of the latent dimensions: the \textbf  {blue line} (circular markers) represents adding dimensions in decreasing order of variance, the \textbf  {green line} (square markers) for increasing variance, and the \textbf  {red line} (diamond markers) for a random order.\vspace  {-2em} }}{90}{figure.caption.58}\protected@file@percent }
\newlabel{fig:combined_plot}{{5.5}{90}{Learned variances and reconstruction errors for the Hemisphere(5,20) and Sinusoid(5,20) datasets. The plots in the left column show the learned variances in decreasing order for each dataset, while the right column illustrates the average $\ell ^2$ reconstruction error as a function of the number of latent dimensions used. The reconstruction errors are evaluated for three variance-based orders of the latent dimensions: the \textbf {blue line} (circular markers) represents adding dimensions in decreasing order of variance, the \textbf {green line} (square markers) for increasing variance, and the \textbf {red line} (diamond markers) for a random order.\vspace {-2em}}{figure.caption.58}{}}
\@setckpt{Chapter5/chapter5}{
\setcounter{page}{91}
\setcounter{equation}{28}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{7}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{7}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{35}
\setcounter{Hfootnote}{14}
\setcounter{bookmark@seq@number}{70}
\setcounter{theorem}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{1}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALC@unique}{11}
\setcounter{ALC@line}{11}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{6}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{4}
\setcounter{subtable}{0}
}
