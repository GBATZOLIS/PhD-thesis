\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces From left to right: ideal dependencies in the $i^{th}$ autoregressive component. Dual-Glow modeling assumption \cite {Dual-Glow}; information is exchanged only between latent spaces having the same dimension. Our modeling assumption; we retain the dependencies between $L_i$ and the latent spaces of lower dimension.}}{1}{figure.caption.7}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Left: unconditional normalizing flow architecture used to encode conditioning and conditioned images, denoted by $Y_n = Y$ and $W_n = W$ respectively, into a sequence of hierarchical latent variables. Right: design of the conditional transformation $G_{i}^\theta $ that models the $i^{th}$ autoregressive component. The index of the flow $i$ is omitted in both the transformed latent variable $Z_j$ and the intermediate latent variables $Z_j^{\prime }$ for simplicity.}}{2}{figure.caption.8}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Illustration of a multi-scale diffusion model with three scales. Inspired by multi-scale normalizing flows, this approach diffuses different parts of the image tensor (transformed into multi-level Haar coefficients) at varying speeds. High-frequency detail coefficients diffuse progressively faster, with $d_1$ diffusing faster than $d_2$, $d_2$ faster than $d_3$, and so on, ensuring that all coefficients reach the same (very low) signal-to-noise ratio (SNR) at their respective terminal diffusion times: $t = 0.25, 0.5, 0.75$, and $1.0$, respectively. The low-frequency approximation coefficients $a_3$ diffuse the slowest, completing their diffusion at $t = 1.0$. The multi-scale structure reduces the dimensionality of the diffusing tensor at each scale, enabling faster computation. Separate neural networks $S_1, S_2, S_3, S_4$ approximate the score functions at different intervals, leveraging the reduced dimensionality of the intermediate distributions. This hierarchical design mirrors the structure of multi-scale normalizing flows, improving training and sampling efficiency while maintaining high image generation quality.}}{3}{figure.caption.10}%
\contentsline {figure}{\numberline {1.4}{\ignorespaces Results from our conditional multi-speed diffusive estimator.}}{4}{figure.caption.11}%
\contentsline {figure}{\numberline {1.5}{\ignorespaces (Left) Visualization of the score field near the data manifold. (Right) Visualisation of the estimation of the manifold dimension using the trained diffusion model.}}{5}{figure.caption.13}%
\contentsline {figure}{\numberline {1.6}{\ignorespaces Comparison of original and reconstructed images on the FFHQ dataset using our ScoreVAE framework. The left panel presents the original images from the FFHQ dataset, while the right panel displays the corresponding reconstructions generated by ScoreVAE. The results highlight the effectiveness of ScoreVAE in capturing intricate details and preserving high fidelity, overcoming the limitations of traditional VAE models.}}{7}{figure.caption.15}%
\contentsline {figure}{\numberline {1.7}{\ignorespaces Approximate data manifolds learned by the Riemannian autoencoder generated by score-based pullback Riemannian geometry for three datasets. The orange surfaces represent the manifolds learned by the model, while the blue points correspond to the training data. Each manifold provides a convincing low-dimensional representation of the data, isometric to its respective latent space. }}{9}{figure.caption.17}%
