\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\citation{karras2019style,ramesh2021zero,esser2021taming,kong2020diffwave}
\citation{radford2021learning,xu2021videoclip,wang2022image}
\citation{jumper2021highly,stokes2020deep,rives2021biological,zhang2021unified}
\citation{goodfellow2014generative}
\citation{ho2020denoising,song2021score}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{karras2019style}
\citation{esser2021taming}
\citation{kong2020diffwave}
\citation{ramesh2021zero}
\citation{jumper2021highly,baek2021accurate}
\citation{stokes2020deep,gentile2020deep}
\citation{butler2018machine}
\citation{saharia2022image,wang2024sinsr}
\citation{suvorov2022resolution,lugmayr2022repaint}
\citation{chen2020simple}
\citation{devlin2018bert}
\citation{brown2020language}
\citation{hossain2019comprehensive}
\citation{arandjelovic2017look}
\citation{sun2019videobert}
\citation{radford2021learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Generative Modeling}{2}{section.1.1}\protected@file@percent }
\newlabel{intro:generative_modeling}{{1.1}{2}{Generative Modeling}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}The Problem of Generative Modeling}{2}{subsection.1.1.1}\protected@file@percent }
\citation{silverman1986density}
\citation{wasserman2006all}
\citation{scott2015multivariate}
\citation{fefferman2016testing}
\citation{roweis2000nonlinear}
\citation{tenenbaum2000global}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Why Deep Learning for Generative Modeling?}{3}{subsection.1.1.2}\protected@file@percent }
\citation{lecun2015deep}
\citation{lecun1998gradient}
\citation{hochreiter1997long}
\citation{poggio2017theory}
\citation{goodfellow2016deep,kingma2013auto}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Generative Adversarial Networks (GANs)}{4}{subsection.1.1.3}\protected@file@percent }
\newlabel{sec:gans}{{1.1.3}{4}{Generative Adversarial Networks (GANs)}{subsection.1.1.3}{}}
\citation{arjovsky2017wasserstein}
\citation{karras2019stylegan}
\citation{karras2019stylegan}
\citation{arjovsky2017wasserstein}
\citation{arjovsky2017wasserstein}
\citation{mescheder2018which}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Samples generated by the StyleGAN model \cite  {karras2019stylegan}. These high-resolution, realistic images demonstrate the capability of GANs to produce lifelike outputs.}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:stylegan_samples}{{1.1}{5}{Samples generated by the StyleGAN model \cite {karras2019stylegan}. These high-resolution, realistic images demonstrate the capability of GANs to produce lifelike outputs}{figure.caption.1}{}}
\citation{papamakarios2019normalizing}
\citation{ho2020denoising}
\citation{kingma2013auto}
\citation{rezende2014stochastic}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Demonstration of mode collapse in GANs: A vanilla GAN trained on the MNIST dataset is expected to generate samples from all modes of the data distribution (i.e., all digits). However, as training progresses, the generator increasingly focuses on producing convincing samples of a single mode (i.e., the digit 1), illustrating the issue of mode collapse. This highlights the tendency of GANs to fail in capturing the diversity of the data distribution.}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:mode_collapse}{{1.2}{6}{Demonstration of mode collapse in GANs: A vanilla GAN trained on the MNIST dataset is expected to generate samples from all modes of the data distribution (i.e., all digits). However, as training progresses, the generator increasingly focuses on producing convincing samples of a single mode (i.e., the digit 1), illustrating the issue of mode collapse. This highlights the tendency of GANs to fail in capturing the diversity of the data distribution}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Variational Autoencoders (VAEs)}{6}{subsection.1.1.4}\protected@file@percent }
\newlabel{sec:vae}{{1.1.4}{6}{Variational Autoencoders (VAEs)}{subsection.1.1.4}{}}
\newlabel{intro:eq:gaussian_reconstruction}{{1.1.4}{7}{Variational Autoencoders (VAEs)}{equation.1.1.1}{}}
\citation{kingma2013auto}
\citation{higgins2017beta}
\citation{rombach2022high}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces  Comparison of original images (top row) and VAE reconstructions (bottom row), highlighting the \textbf  {blurring effect} caused by the Gaussian decoder assumption, which limits the VAE's ability to capture fine details and complex structures. }}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:vae_blur_reconstructions}{{1.3}{11}{Comparison of original images (top row) and VAE reconstructions (bottom row), highlighting the \textbf {blurring effect} caused by the Gaussian decoder assumption, which limits the VAE's ability to capture fine details and complex structures}{figure.caption.3}{}}
\citation{rezende2015variational}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Normalizing Flows}{12}{subsection.1.1.5}\protected@file@percent }
\newlabel{sec:normalizing_flows}{{1.1.5}{12}{Normalizing Flows}{subsection.1.1.5}{}}
\citation{papamakarios2019normalizing}
\citation{papamakarios2019normalizing}
\citation{behrmann2021understanding,cornish2020relaxing}
\citation{oord2016pixel}
\citation{vanwavenet2016}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Auto-Regressive Models}{14}{subsection.1.1.6}\protected@file@percent }
\newlabel{sec:auto_regressive_models}{{1.1.6}{14}{Auto-Regressive Models}{subsection.1.1.6}{}}
\citation{lecun2006tutorial}
\citation{hinton2002training}
\citation{langevin1908theorie,neal2011mcmc}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.7}Energy-Based Models (EBMs)}{16}{subsection.1.1.7}\protected@file@percent }
\newlabel{sec:energy_based_models}{{1.1.7}{16}{Energy-Based Models (EBMs)}{subsection.1.1.7}{}}
\citation{lecun2006tutorial}
\citation{du2019implicit,grathwohl2020your}
\citation{du2019implicit}
\citation{grathwohl2020your,lecun2006tutorial}
\citation{ho2020denoising}
\citation{song2021sde}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.8}Diffusion Models}{18}{subsection.1.1.8}\protected@file@percent }
\citation{anderson1982reverse_time_sde}
\citation{song2021maximum}
\citation{song2020score}
\citation{song2020denoising}
\citation{rombach2022high}
\citation{salimans2022progressive}
\citation{song2023consistency}
\citation{lecun2021self}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{devlin2019bert}
\citation{radford2019language}
\citation{oord2018representation}
\citation{chen2020simple}
\citation{he2020momentum}
\citation{goyal2021self}
\citation{schneider2019wav2vec}
\citation{baevski2020wav2vec}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Self-Supervised Learning}{26}{section.1.2}\protected@file@percent }
\newlabel{intro:ssl}{{1.2}{26}{Self-Supervised Learning}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Motivation}{26}{subsection.1.2.1}\protected@file@percent }
\citation{radford2021learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Mathematical Formulation of SSL}{27}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Generative vs. Discriminative SSL}{28}{subsection.1.2.3}\protected@file@percent }
\citation{radford2019language}
\citation{oord2016pixel}
\citation{izmailov2021flowgmm}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {subsubsection}{Generative Approaches}{29}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning Latent Representations by Modeling the Full Distribution $p(X)$}{29}{section*.5}\protected@file@percent }
\citation{oord2017neural}
\citation{baevski2020wav2vec}
\citation{razavi2019generating}
\citation{ding2021vqvae}
\citation{karras2019style}
\citation{esser2021taming}
\citation{bao2021beit}
\citation{he2022masked}
\@writefile{toc}{\contentsline {paragraph}{Learning Latent Representations by Modeling the Masked Distribution $p(X_{\text  {masked}}|X_{\text  {context}})$}{30}{section*.6}\protected@file@percent }
\citation{devlin2019bert}
\citation{liu2019roberta}
\citation{tian2022}
\@writefile{toc}{\contentsline {subsubsection}{Discriminative Approaches}{31}{section*.7}\protected@file@percent }
\citation{oord2018representation}
\citation{chen2020simple}
\citation{grill2020bootstrap}
\citation{chen2021exploring}
\citation{zbontar2021barlow}
\citation{bardes2021vicreg}
\citation{caron2018deep}
\citation{caron2020unsupervised}
\citation{gidaris2018unsupervised}
\citation{noroozi2016unsupervised}
\citation{misra2016shuffle}
\citation{karras2019style,karras2020analyzing}
\citation{chen2020simple}
\citation{he2020momentum}
\citation{bao2021beit}
\citation{zhou2022ibot}
\@writefile{toc}{\contentsline {subsubsection}{Choosing Between Generative and Discriminative SSL}{36}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generative SSL is more useful when:}{36}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminative SSL is more useful when:}{36}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Blurring the Lines: Hybrid Approaches}{37}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Utilizing Learned Representations in Downstream Tasks}{37}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Classification}{37}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Other Downstream Tasks}{38}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Challenges and Limitations in Current Methodologies}{38}{subsection.1.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Intrinsic Dimension Estimation}{39}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limited Model Expressiveness in Variational Autoencoders}{39}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Geometric Understanding and Interpretability of the Latent Space}{40}{section*.16}\protected@file@percent }
\@setckpt{introduction/introductionA}{
\setcounter{page}{41}
\setcounter{equation}{2}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{3}
\setcounter{Item}{6}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{16}
\setcounter{theorem}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
}
