%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Introduction *********************************
%*******************************************************************************

\chapter{Introduction}

Over the past decade, we’ve witnessed an extraordinary leap in generative modeling, driven by innovations in Generative Adversarial Networks (GANs) and diffusion models. These breakthroughs have set new standards in sample quality, enabling hyper-realistic image synthesis, lifelike video creation, and compelling audio generation (e.g., voice synthesis, music composition). The impact of these models extends far beyond media, reaching scientific applications like molecular design, where they are invaluable in accelerating drug discovery.

Beyond generating realistic content, these models excel in solving diverse tasks, including inverse problems like super-resolution and inpainting, text-guided generation seen in text-to-image systems, and unpaired image-to-image translation. This versatility has enabled a range of practical applications, from converting black-and-white photos to color and transforming artistic styles (e.g., Monet’s paintings into realistic scenes), to enhancing low-light photos and translating medical scans across modalities (e.g., converting MRI scans to CT-like images) for improved diagnostics.

In this thesis, we explore the intersection of generative modeling and unsupervised representation learning. We start by formally defining the problem of generative modeling and introducing the key frameworks, including GANs, normalizing flows, and diffusion models. We then delve into unsupervised representation learning, presenting methods that extract meaningful latent representations without labeled data. Finally, we discuss how our work bridges these two areas, deepening the connection between generative modeling and unsupervised representation learning, and revealing new insights into the underlying structure of data.

\section{The Problem of Generative Modeling}

Generative modeling sits at the forefront of machine learning, enabling us to capture the underlying structure of complex data and generate new, realistic samples. Whether it's creating lifelike images, composing music, or designing novel molecules, generative models aim to understand and replicate the distribution of data they're trained on.

Formally, let's consider a dataset \( \mathcal{D} = \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N \} \subset \mathbb{R}^d \), comprising \( N \) independent samples drawn from an unknown data-generating distribution \( P \), which we refer to as the \textit{target distribution}. The central goal of generative modeling is to create a model that approximates \( P \) as closely as possible based solely on these samples.

To learn the distribution \( P \), we seek a computationally tractable approximation \( \hat{P} \) based on the dataset \( \mathcal{D} \) and our assumptions about \( P \). We measure how close \( \hat{P} \) is to \( P \) using a divergence \( D(\hat{P}, P) \), such as the Kullback-Leibler divergence, Wasserstein distance, or \( L_p \) norms. Our goal is to minimize this divergence:

\[
\hat{P} = \arg \min_{\hat{P}} D(\hat{P}, P).
\]

Typically, we parameterize \( \hat{P} \) with a finite-dimensional vector \( \theta \in \mathbb{R}^p \), forming a family of models \( \hat{P}_\theta \) and transforming the problem into optimizing over parameters:

\[
\theta^* = \arg \min_\theta D(\hat{P}_\theta, P).
\]

For practicality, \( \hat{P}_\theta \) must be computationally tractable; we should be able to efficiently sample from it or evaluate its probability density function. Different generative modeling approaches balance tractability and fidelity to \( P \), leading to various methods suitable for different applications.

By tackling this problem, we aim not only to reproduce the data we observe but also to uncover the underlying mechanisms that generate it. This deep understanding empowers us to create models that can generate new, unseen data that is indistinguishable from real-world samples, thereby expanding the horizons of what machines can learn and create.

\section{Why Deep Learning for Generative Modelling?}

Non-parametric methods such as Kernel Density Estimation (KDE) have been widely used for modelling probability distributions in low-dimensional settings due to their flexibility in estimating the underlying density without assuming a specific parametric form \cite{silverman1986density}.

However, scaling KDE to high-dimensional data is problematic. The sample complexity of KDE scales exponentially with the number of feature dimensions \( d \), making it impractical for high-dimensional tasks \cite{wasserman2006all}. Specifically, to achieve a target estimation error \( \varepsilon \), the number of samples \( n \) must satisfy:

\[
n \geq \mathcal{O}\left(\varepsilon^{-\frac{4 + d}{4}}\right).
\]
\cite{scott2015multivariate}
This exponential dependence on \( d \) implies that even for moderate dimensions, the required sample size becomes infeasibly large, rendering KDE and similar methods ineffective for generative modelling in high dimensions.

In practice, however, real-world data often does not occupy the entire ambient space but is concentrated around a lower-dimensional manifold, as posited by the \emph{manifold hypothesis} \cite{fefferman2016testing}. This suggests that the data lies on or near a manifold of intrinsic dimension \( K \ll d \), reflecting dependent relationships among features. For instance, in image data, pixels exhibit strong correlations due to spatial continuity, reducing the effective dimensionality \cite{roweis2000nonlinear}.

When the data lies on a \( K \)-dimensional manifold, the sample complexity improves to:

\[
n \geq \mathcal{O}\left(\varepsilon^{-\frac{4 + K}{4}}\right).
\]
\cite{bickel2007local}
Although this reduction from \( d \) to \( K \) offers a substantial improvement, the exponential dependence on \( K \) can still be problematic, especially if \( K \) is not very small \cite{cayton2005algorithms}.

To further improve the sample complexity, we need parametrized models that embed assumptions about the structure of the data manifold. Deep learning architectures, such as Convolutional Neural Networks (CNNs), incorporate inductive biases by utilizing shared weights and local receptive fields, specifically designed to capture spatial hierarchies and local dependencies in the data \cite{lecun1998gradient}. These inductive biases effectively constrain the space of functions the model can represent, reducing the sample complexity and enabling efficient generative modelling even in high-dimensional scenarios \cite{poggio2017theory}. This structural approach explains why deep learning based methods have become indispensable for generative modelling in high dimensions \cite{goodfellow2016deep, kingma2013auto}.

\section{Deep Learning for Generative Modeling}
In the past decade, deep learning has introduced flexible architectures that can effectively learn complex data distributions. This chapter reviews five main types of generative models: Generative Adversarial Networks (GANs), Normalizing Flows, Auto-Regressive Models, Energy-Based Models, and Diffusion Models, each with unique strengths and limitations.

\subsection{Generative Adversarial Networks (GANs)}
Introduced by Goodfellow et al. \cite{goodfellow2014generative}, Generative Adversarial Networks (GANs) consist of two neural networks: a \textit{generator} \( G \) and a \textit{discriminator} \( D \), which are trained simultaneously in a minimax game. The generator \( G \) maps samples \( \mathbf{z} \) from a simple prior distribution \( P_{\mathbf{z}} \) (e.g., a standard normal distribution) to the data space \( \mathbb{R}^d \), producing synthetic data \( G(\mathbf{z}) \). The discriminator \( D \) outputs a probability \( D(\mathbf{x}) \in [0,1] \) indicating whether a sample \( \mathbf{x} \) is real (from the training data) or generated (from \( G \)).

The training objective is formulated as a two-player minimax game with the value function \( V(D, G) \):
\[
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{\mathbf{x} \sim P_{\text{data}}} [\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim P_{\mathbf{z}}} [\log (1 - D(G(\mathbf{z})))]
\]
where \( P_{\text{data}} \) is the data distribution represented by the training set \( \mathcal{D} \).

\paragraph{Training Process}
\begin{itemize}
    \item \textbf{Discriminator Update:} For fixed \( G \), \( D \) is trained to maximize the probability of assigning the correct label to both training examples and samples from \( G \).
    \item \textbf{Generator Update:} For fixed \( D \), \( G \) is trained to minimize \( \log (1 - D(G(\mathbf{z}))) \), which is equivalent to maximizing \( \log D(G(\mathbf{z})) \) in practice for better gradient properties.
    \item \textbf{Optimization:} The generator and discriminator are updated iteratively using stochastic gradient descent or its variants.
\end{itemize}

At equilibrium, the generator produces samples that are indistinguishable from real data, meaning \( P_{G} = P_{\text{data}} \), and the discriminator outputs \( D(\mathbf{x}) = \frac{1}{2} \) for all \( \mathbf{x} \).

\paragraph{Strengths}
\begin{itemize}
    \item \textbf{High-quality samples:} GANs are known for producing visually realistic images and sharp outputs.
    \item \textbf{Efficient sampling:} Once trained, the generator can produce samples quickly by a single forward pass.
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item \textbf{Training instability:} The adversarial setup can lead to issues like mode collapse, vanishing gradients, and oscillatory behavior.
    \item \textbf{No explicit density:} GANs do not provide a likelihood estimate or a probability density function for the generated data.
\end{itemize}

\subsection{Normalizing Flows}\label{sec:normalizing_flows}

Normalizing flows, introduced by Rezende and Mohamed \cite{rezende2015variational}, are generative models that transform a simple base distribution into a complex target distribution through a sequence of invertible and differentiable mappings. Given a latent variable $Z \sim p_Z$ in space $\mathcal{Z}$ and an invertible function $G^\theta: \mathcal{Z} \rightarrow \mathcal{X}$, the model defines $X = G^\theta(Z)$ to approximate the data distribution $p_X$.

\paragraph{Change of Variables}

The probability density function (pdf) of $X$ is computed using the change of variables formula:

\[
p^\theta_X(\mathbf{x}) = p_Z\left(F^\theta(\mathbf{x})\right) \left| \det \left( \frac{\partial F^\theta(\mathbf{x})}{\partial \mathbf{x}} \right) \right|,
\]

\noindent where $F^\theta = (G^\theta)^{-1}$ is the inverse transformation. Taking logarithms:

\[
\log p^\theta_X(\mathbf{x}) = \log p_Z\left(F^\theta(\mathbf{x})\right) + \log \left| \det \left( \frac{\partial F^\theta(\mathbf{x})}{\partial \mathbf{x}} \right) \right|.
\]

\paragraph{Composing Transformations}

$G^\theta$ is often composed of $K$ invertible transformations:

\[
G^\theta = G_K \circ G_{K-1} \circ \dots \circ G_1,
\]

\noindent with inverses $F_k = G_k^{-1}$. For data point $\mathbf{x}$, intermediate variables are defined as $\mathbf{h}_0 = \mathbf{x}$ and $\mathbf{h}_k = F_k(\mathbf{h}_{k-1})$ for $k = 1, \dots, K$. The log-density becomes:

\[
\log p^\theta_X(\mathbf{x}) = \log p_Z(\mathbf{h}_K) + \sum_{k=1}^{K} \log \left| \det \left( \frac{\partial F_k(\mathbf{h}_{k-1})}{\partial \mathbf{h}_{k-1}} \right) \right|.
\]

\paragraph{Training Objective}

Parameters $\theta$ are optimized by maximizing the likelihood over data $\{\mathbf{x}_i\}_{i=1}^N$, minimizing:

\[
\theta^* = \arg \min_{\theta} \sum_{i=1}^N -\log p^\theta_X(\mathbf{x}_i).
\]

\paragraph{Strengths}

\begin{itemize}
    \item \textbf{Exact Density Computation}: Enables exact and tractable computation of likelihoods, facilitating stable training via maximum likelihood estimation \cite{papamakarios2019normalizing}.
    \item \textbf{Efficient Sampling}: Fast sampling by transforming latent samples through $G^\theta$.
\end{itemize}

\paragraph{Weaknesses}

\begin{itemize}
    \item \textbf{Limited expressivity}: Requirements for invertibility and tractable Jacobians limit transformation choices, potentially reducing expressiveness \cite{papamakarios2019normalizing}.
    \item \textbf{Topological limitations and numerical stability issues}: Normalising flows become pathological when used to model targets whose supports have complicated topologies. In such cases, the normalizing flow must become arbitrarily numerically noninvertible in order to approximate the target closely \cite{behrmann2021understanding, cornish2020relaxing}.
\end{itemize}



\subsection{Auto-Regressive Models}
Auto-regressive models, such as PixelCNN and PixelRNN \cite{intro:oord2016pixel}, model the joint distribution \( P(\mathbf{x}) \) as a product of conditional distributions using the chain rule of probability:
\[
P(\mathbf{x}) = \prod_{i=1}^{d} P(x_i | x_{1:i-1})
\]
where \( x_{1:i-1} \) denotes all previous variables in a predefined ordering.

\paragraph{Training Objective}
The model is trained to maximize the likelihood of the data:
\[
\theta^* = \arg \max_{\theta} \sum_{i=1}^{N} \sum_{j=1}^{d} \log P_{\theta}(x_{i,j} | x_{i,1:j-1})
\]
where \( x_{i,j} \) is the \( j \)-th variable of the \( i \)-th sample.

\paragraph{Strengths}
\begin{itemize}
    \item \textbf{Exact likelihood calculation:} The likelihood is computed directly without approximation.
    \item \textbf{Modeling sequential dependencies:} Effective at capturing complex dependencies in data sequences.
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item \textbf{Slow sampling:} Generating a sample requires sequentially sampling each variable, which is time-consuming for high-dimensional data.
    \item \textbf{Limited parallelization:} The sequential nature hinders parallel computation during sampling.
\end{itemize}

\subsection{Energy-Based Models (EBMs)}
Energy-Based Models (EBMs) \cite{intro:lecun2006tutorial} define an unnormalized probability distribution over data using an energy function \( E_{\theta}(\mathbf{x}) \):
\[
P_{\theta}(\mathbf{x}) = \frac{\exp(-E_{\theta}(\mathbf{x}))}{Z_{\theta}}
\]
where \( Z_{\theta} = \int \exp(-E_{\theta}(\mathbf{x})) d\mathbf{x} \) is the partition function, typically intractable to compute.

\paragraph{Training Objective}
The model is trained to minimize the expected energy of data samples and increase the energy of negative samples. The loss function can be formulated using contrastive divergence \cite{intro:hinton2002training}:
\[
\mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}^+ \sim P_{\text{data}}} [E_{\theta}(\mathbf{x}^+)] - \mathbb{E}_{\mathbf{x}^- \sim P_{\theta}} [E_{\theta}(\mathbf{x}^-)]
\]
where \( \mathbf{x}^+ \) are positive samples from the data and \( \mathbf{x}^- \) are negative samples generated from the model.

\paragraph{Training Challenges}
Estimating \( \mathbb{E}_{\mathbf{x}^- \sim P_{\theta}} [E_{\theta}(\mathbf{x}^-)] \) requires sampling from \( P_{\theta} \), often done via Markov Chain Monte Carlo (MCMC) methods like Langevin Dynamics.

\paragraph{Strengths}
\begin{itemize}
    \item \textbf{Flexibility:} EBMs can represent complex distributions without the need for normalization.
    \item \textbf{Compositionality:} Easy to combine with other models and incorporate domain knowledge.
\end{itemize}

\paragraph{Weaknesses}
\begin{itemize}
    \item \textbf{Training difficulty:} MCMC sampling is computationally intensive and can be slow to converge.
\end{itemize}

\subsection{Diffusion and Score-Based Models}

Diffusion models \cite{ho2020denoising} and score-based generative models (SGMs) \cite{song2021sde} are powerful generative frameworks that treat data generation as the process of reversing noise corruption. These models consist of two complementary components: a forward process that gradually perturbs data into a noise-like prior distribution and a reverse process that reconstructs the data by inverting this transformation. The forward process maps a complex data distribution into a simpler, analytically tractable distribution, typically the standard normal distribution \( \mathcal{N}(0, \mathbf{I}) \), while the reverse process transforms noise back into samples from the original data distribution.

In discrete-time models, such as Denoising Diffusion Probabilistic Models (DDPMs), this transformation occurs over a finite sequence of time steps, governed by a Markov chain. In contrast, continuous-time frameworks, such as Score-Based Generative Models (SGMs), model this progression using stochastic differential equations (SDEs). As we will see, these two frameworks are mathematically equivalent in the limit of infinite time steps. The reverse process, which lies at the heart of both frameworks, is the critical mechanism for generating high-quality samples.

\paragraph{Forward Process}

The forward process in diffusion and score-based generative models progressively corrupts the original data into noise, providing the foundation for learning the reverse process that generates data from noise. The design of the forward process ensures that over time, the data distribution transitions smoothly into a simple, analytically tractable distribution $\pi$, such as \( \mathcal{N}(0, \mathbf{I}) \).

\begin{itemize}
    \item \textbf{DDPM (Discrete Time):}
    
    In the Denoising Diffusion Probabilistic Model (DDPM), the forward process is a Markov chain that adds Gaussian noise to the data at discrete time steps \( t = 1, 2, \dots, N \). The transition kernel is given by:
    \[
    q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{\alpha_t} \, \mathbf{x}_{t-1},\, \beta_t \mathbf{I}\right),
    \]
    where:
    \begin{itemize}
        \item \( \alpha_t = 1 - \beta_t \), the scaling factor applied to \( \mathbf{x}_{t-1} \),
        \item \( \beta_t \in (0, 1) \), the predefined variance schedule controlling the amount of noise at step \( t \),
    \end{itemize}
    This formulation allows the process to be expressed in terms of the initial data \( \mathbf{x}_0 \):
    \[
    q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}\left(\mathbf{x}_t;\, \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0,\, (1 - \bar{\alpha}_t) \mathbf{I}\right),
    \]
    where \( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \) is the cumulative product of \( \alpha_s \). This closed-form expression simplifies training and allows direct sampling from any step \( t \).

    \item \textbf{SGM (Continuous Time):} The forward process in Score-Based Generative Models (SGM) is described by a continuous-time Stochastic Differential Equation (SDE) that progressively perturbs the data distribution with infinitesimal noise:

    \[
    d\mathbf{x} = f(\mathbf{x}, t) \, dt + g(t) \, d\mathbf{w},
    \]
    
    where:
    \begin{itemize}
        \item \( \mathbf{w}(t) \) is a standard Wiener process (Brownian motion),
        \item \( f(\mathbf{x}, t) \) is the drift coefficient, defining the deterministic evolution of the data distribution,
        \item \( g(t) \) is the diffusion coefficient, controlling the scale of the stochastic noise.
    \end{itemize}
    
    The drift and diffusion coefficients \( f(\mathbf{x}, t) \) and \( g(t) \) are carefully chosen so that, after a sufficiently large diffusion time \( T \), the data distribution evolves into a distribution which is very close to a simple, analytically tractable distribution, typically the standard normal distribution \( \mathcal{N}(0, \mathbf{I}) \).
    
    Popular choices for the forward SDE include:
    
    \begin{itemize}
        \item \textbf{Variance Preserving (VP) SDE:}
        \[
        f(\mathbf{x}, t) = -\frac{1}{2} \beta(t) \mathbf{x}, \quad g(t) = \sqrt{\beta(t)},
        \]
        where \( \beta(t) \) is a predefined noise schedule that controls the rate of diffusion. This choice preserves the variance of the data distribution throughout the process. The limiting distribution as \( t \to \infty \) is the standard normal distribution. In practice, after sufficient diffusion time $T$, the perturbed distribution $p_T \approx \mathcal{N}(0, \mathbf{I})$.
    
        \item \textbf{Variance Exploding (VE) SDE:}
        \[
        f(\mathbf{x}, t) = \mathbf{0}, \quad g(t) = \sqrt{\frac{d[\sigma^2(t)]}{dt}},
        \]
        where \( \sigma(t) \) is a monotonically increasing function that scales the noise over time. In this formulation, the variance of the data distribution "explodes". After sufficient diffusion time $T$, the perturbed distribution $p_T \approx \mathcal{N}(0, \sigma(T)^2\mathbf{I}).$
    \end{itemize}

\end{itemize}

These formulations ensure a smooth progression from the data distribution to the prior, providing a well-defined framework for the reverse process to reconstruct data from noise.


\paragraph{Reverse Process}

The reverse process reconstructs the original data distribution by inverting the forward process, progressively removing noise to generate data samples:

\begin{itemize}
    \item \textbf{DDPM (Discrete Time):}
    The reverse process in DDPMs is a Markov chain that denoises the data at each step. The conditional distribution for each step is given by:
    \[
    p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}\left(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \beta_t^2 \mathbf{I}\right),
    \]
    where \( \mu_\theta(\mathbf{x}_t, t) \) is a neural network predicting the mean of the denoised distribution based on the noisy input \( \mathbf{x}_t \) and the timestep \( t \). Sampling proceeds iteratively by drawing samples from \( p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \), starting from \( \mathbf{x}_T \sim \pi = \mathcal{N}(0, \mathbf{I}) \).

    \item \textbf{SGM (Continuous Time):}
    In SGMs, the reverse process is governed by the reverse-time SDE, derived from Anderson’s theorem \cite{anderson1982reverse_time_sde}:
    \[
    d\mathbf{x} = \left[f(\mathbf{x}, t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x})\right] dt + g(t) \, d\bar{\mathbf{w}},
    \]
    where:
    \begin{itemize}
        \item \( \nabla_\mathbf{x} \log p_t(\mathbf{x}) \) is the gradient of the logdensity (score function) of the perturbed distribution at diffusion time t. In practice, the score function is approximated by a neural network \( s_\theta(\mathbf{x}, t) \), which we call the score model,
        \item \( \bar{\mathbf{w}}(t) \) is a reverse-time Wiener process.
    \end{itemize}
    Sampling proceeds by solving this reverse-time SDE, starting from \( \mathbf{x}_T \sim \pi \approx p_T(\mathbf{x}) \) (e.g., \( \mathcal{N}(0, \mathbf{I}) \)) and integrating backward from $t=T$ to $t = 0$. This iterative process generates high-quality samples that closely follow the original data distribution.
\end{itemize}

The reverse process lies at the core of diffusion-based generative modeling, enabling the transformation of simple noise into complex data distributions while preserving high fidelity.


\paragraph{Training Objective}

Training involves learning to approximate the reverse process by minimizing specific loss functions:

\begin{itemize}
    \item \textbf{DDPM:} In the DDPM framework, the training objective is formulated to minimize an expression that serves as an upper bound on the negative log-likelihood of the data. This objective is defined as:
    \[
    \mathcal{L}_{\text{DDPM}}(\theta) = \sum_{t=1}^T \mathbb{E}_{\mathbf{x}_0,\, \mathbf{\epsilon}_t} \left[ \left\| \mathbf{\epsilon}_t - \mathbf{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t}\, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\, \mathbf{\epsilon}_t,\, t\right) \right\|^2 \right],
    \]
    where:
    \begin{itemize}
        \item \( \mathbf{\epsilon}_t \sim \mathcal{N}(0, \mathbf{I}) \) is Gaussian noise added at time step \( t \),
        \item \( \mathbf{\epsilon}_\theta(\mathbf{x}_t, t) \) is a neural network that estimates the noise component in \( \mathbf{x}_t \),
        \item \( \bar{\alpha}_t = \prod_{s=1}^t \alpha_s \) is the cumulative product of the noise schedule parameters,
        \item \( \alpha_t = 1 - \beta_t \), and \( \beta_t \) is the variance of the noise added at each time step.
    \end{itemize}

    By minimizing \( \mathcal{L}_{\text{DDPM}}(\theta) \), we effectively minimize an upper bound on the negative log-likelihood, enhancing the model's capacity to generate realistic data samples.

    The mean \( \mu_\theta \) of the reverse process used during sampling is derived from the predicted noise \( \mathbf{\epsilon}_\theta \) as follows:
    \[
    \mu_\theta(\mathbf{x}_t, t) = \frac{\sqrt{\bar{\alpha}_{t-1}}\, \beta_t}{1 - \bar{\alpha}_t}\, \hat{\mathbf{x}}_0 + \frac{\sqrt{\alpha_t}\, (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}\, \mathbf{x}_t,
    \]
    where:
    \begin{itemize}
        \item \( \hat{\mathbf{x}}_0 = \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\, \mathbf{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}} \) is the reconstructed noiseless data point.
    \end{itemize}

    This formulation ensures that the reverse diffusion process effectively removes noise at each step, gradually transforming pure noise into high-quality data samples during generation.


    \item \textbf{SGM:} In the Score-Based Generative Modeling (SGM) framework, the neural network \( s_{\theta} \) is trained to approximate the score function of the perturbed data distribution \( p_t(\mathbf{x}) \). This is achieved by minimizing the \textbf{Weighted Denoising Score Matching Objective}:
        \[
        \mathcal{L}_{\text{DSM}}(\theta) = \frac{1}{2} \int_0^T \lambda(t) \, \mathbb{E}_{\mathbf{x}_0,\, \mathbf{x}_t} \left[ \left\| s_\theta(\mathbf{x}_t, t) - \nabla_{\mathbf{x}_t} \log p_{0t}(\mathbf{x}_t \mid \mathbf{x}_0) \right\|^2 \right] dt,
        \]
        where:
        \begin{itemize}
            \item \( p_{0t}(\mathbf{x}_t \mid \mathbf{x}_0) \) is the perturbation kernel of the forward Stochastic Differential Equation (SDE),
            \item \( \lambda(t) \) is a weighting function,
            \item \( \mathbf{x}_t \) is the diffused data at time \( t \).
        \end{itemize}

        \cite{song2021maximum} showed that for the particular choice \( \lambda(t) = g(t)^2 \), where \( g(t) \) is the diffusion coefficient of the forward SDE, the \( \mathcal{L}_{\text{DSM}} \) objective serves as an upper bound on the negative log-likelihood.

        In practice, a different weighting function—often referred to as the \emph{simple weighting}—is commonly used:
        \[
        \lambda(t) = \sigma(t)^2,
        \]
        where \( \sigma(t)^2 \) denotes the variance of the forward perturbation kernel \( p_{0t}(\mathbf{x}_t \mid \mathbf{x}_0) \). This weighting function has been found to yield superior generative performance, particularly in the image domain where it was initially tested. As a result, it has become the \emph{de facto} standard choice for the weighting function in practical implementations.

\end{itemize}

\paragraph{Equivalence Between DDPM and SGM}

Denoising Diffusion Probabilistic Models (DDPMs) and Score-Based Generative Models (SGMs) are equivalent in the limit as the number of timesteps \( N \to \infty \). Specifically, the forward process in a DDPM converges to the Variance Preserving Stochastic Differential Equation (VP-SDE) used in SGMs:

\[
d\mathbf{x} = -\tfrac{1}{2} \beta(t) \mathbf{x} \, dt + \sqrt{\beta(t)} \, d\mathbf{w},
\]

where \( \beta(t) \) is the continuous noise schedule and \( \mathbf{w} \) is a standard Wiener process. This result is detailed in Song et al.'s paper \textit{Score-Based Generative Modeling through Stochastic Differential Equations} (2020).

\subsubsection{Probability Flow ODE}

An alternative to the stochastic reverse-time SDE is the \textbf{probability flow ODE}, which provides a deterministic mapping between the data distribution and the prior. The probability flow ODE is defined as:
\[
\frac{d\mathbf{x}}{dt} = f(\mathbf{x}, t) - \frac{1}{2} g(t)^2 s_\theta(\mathbf{x}, t),
\]
where:
\begin{itemize}
    \item \( f(\mathbf{x}, t) \) and \( g(t) \) are the drift and diffusion coefficients of the forward SDE,
    \item \( s_\theta(\mathbf{x}, t) \) is the approximation of the score function at time $t$.
\end{itemize}

Under the assumption of a perfectly approximated score function, the probability flow ODE shares the same marginal probability distributions as the reverse-time SDE for all diffusion times. This equivalence arises from considering the Fokker-Planck equations corresponding to both processes, which govern the time evolution of probability densities.

\textit{Computational Efficiency}. In practice, integrating the probability flow ODE has been observed to be computationally more efficient than simulating the reverse-time SDE. Fewer integration steps are typically required to achieve similar sample quality, making it a preferred choice for sampling in diffusion models. This efficiency stems from the deterministic nature of the ODE, which allows for larger step sizes, unlike the stochastic SDE that often requires smaller step sizes to maintain sample fidelity.

\textit{Likelihood Computation with the Probability Flow ODE}. The probability flow ODE enables likelihood computation via the instantaneous change of variables formula. Given the score function \( s_\theta(\mathbf{x}, t) \), the log-likelihood of a data point \( \mathbf{x}_0 \) can be expressed as:

\[
\log p_0(\mathbf{x}_0) = \log p_T(\mathbf{x}_T) + \int_0^T \nabla_{\mathbf{x}} \cdot \mathbf{f}_\theta^{\text{ODE}}(\mathbf{x}, t) \, dt,
\]

where \( \mathbf{f}_\theta^{\text{ODE}}(\mathbf{x}, t) \) is the drift term of the probability flow ODE. Direct computation of the divergence term \( \nabla_{\mathbf{x}} \cdot \mathbf{f}_\theta^{\text{ODE}}(\mathbf{x}, t) \) is often computationally expensive. To address this, the Skilling-Hutchinson trace estimator is used:

\[
\nabla_{\mathbf{x}} \cdot \mathbf{f}_\theta^{\text{ODE}}(\mathbf{x}, t) \approx \mathbb{E}_{\mathbf{\epsilon} \sim \mathcal{N}(0, \mathbf{I})} \big[ \mathbf{\epsilon}^\top \nabla_{\mathbf{x}} \mathbf{f}_\theta^{\text{ODE}}(\mathbf{x}, t) \mathbf{\epsilon} \big],
\]

where \( \nabla_{\mathbf{x}} \mathbf{f}_\theta^{\text{ODE}}(\mathbf{x}, t) \) is the Jacobian of \( \mathbf{f}_\theta^{\text{ODE}} \). The term inside the expectation can be efficiently computed using Jacobian-vector product calculations, making this approach both practical and scalable for accurate likelihood computation.



\paragraph{Strengths and Weaknesses of the Unified Framework}
The unified framework of diffusion and score-based generative models exhibits several notable strengths. Among its strengths, the framework ensures stable training by avoiding issues such as mode collapse and instability that commonly afflict adversarial training methods like Generative Adversarial Networks (GANs). Its flexibility allows it to model complex, high-dimensional, and multi-modal data distributions, making it highly adaptable for various applications. Additionally, it is grounded in stochastic process theory with well-defined objectives, providing a solid theoretical foundation that guides both its development and analysis. However, a significant weakness of this framework is its slow sampling speed. Generating samples typically requires hundreds or even thousands of iterative steps, resulting in significant computational costs. This can make the approach impractical for applications where rapid sample generation is crucial.

\paragraph{Advancements in Sampling Speed}
Significant progress has been made in accelerating the sampling speed of diffusion models, effectively addressing their primary limitation of slow generation due to iterative steps. A critical observation in this area is that the probability flow ODE, which shares the same marginal distribution as the reverse SDE, is generally preferred for faster sampling. The ODE formulation tends to produce better samples than the corresponding SDE in the low discretization regime \cite{song2020score}. Consequently, many methods that accelerate diffusion models focus on learning to efficiently integrate the probability flow ODE, often as a post-training optimization.

Techniques such as \textbf{Denoising Diffusion Implicit Models (DDIM)} \cite{song2020denoising} reformulate the reverse diffusion process by utilizing non-Markovian dynamics, significantly reducing the number of required sampling steps without compromising sample quality. \textbf{Latent Diffusion Models (LDMs)} \cite{rombach2022high} enhance efficiency by performing diffusion in a compressed latent space, drastically decreasing computational demands while preserving high-resolution outputs. \textbf{Progressive Distillation} \cite{salimans2022progressive} further compresses the multi-step diffusion process into models capable of generating high-fidelity samples in significantly fewer steps—sometimes even a single step. Recently, \textbf{Consistency Models} \cite{song2023consistency} have been introduced, directly mapping noise to data in one step while retaining the advantages of iterative methods.

Collectively, these advancements have significantly reduced sampling times and made diffusion models practical for real-time and interactive applications across various domains.

\section{Self-Supervised Representation Learning}

\subsection{Motivation}


In today's data-rich environment—spanning social media, sensor outputs, and extensive collections of images, videos and text—the primary challenge is not the lack of data but our ability to utilize it effectively. Despite the vast amounts of information available, much remains unlabeled and underexploited due to the high costs and impracticalities associated with manual labeling. This situation poses a crucial question: how can we unlock the potential of unlabeled data to develop intelligent systems that learn, adapt, and generalize in a manner akin to human learning?

Self-supervised learning (SSL) emerges as a compelling approach to this challenge. Referred to by Yann LeCun as the "dark matter of intelligence" \citep{lecun2021self}, SSL enables machines to learn from the abundant unlabeled data by exploiting the inherent structures within it. LeCun remarks:

\begin{quote}
    "Common sense helps people learn new skills without requiring massive amounts of teaching for every single task. Self-supervised learning is one of the most promising ways to build such background knowledge and approximate a form of common sense in AI systems."
\end{quote}

Analogous to how a child learns about the world through observation and pattern recognition rather than explicit instruction, SSL allows models to derive meaningful representations from data without manual labels.

The influence of SSL is evident across several domains:

\textbf{Natural Language Processing (NLP):} SSL has transformed NLP by enabling models to understand and generate human language with improved contextual awareness. Early models like Word2Vec \citep{mikolov2013efficient} and GloVe \citep{pennington2014glove} learned semantic relationships between words through context prediction. Advanced models such as BERT \citep{devlin2019bert} and GPT \citep{radford2019language} employed masked language modeling to pretrain on extensive text corpora, enhancing capabilities in tasks like text completion and machine translation.

\textbf{Computer Vision (CV):} In CV, SSL techniques like Contrastive Predictive Coding (CPC) \citep{oord2018representation}, SimCLR \citep{chen2020simple}, and MoCo \citep{he2020momentum} have enabled models to learn visual representations without reliance on labeled datasets. For example, Facebook's SEER model \citep{goyal2021self}, trained on a billion unlabeled images, achieved state-of-the-art performance on ImageNet, illustrating the efficacy of learning from unlabeled visual data.

\textbf{Speech Recognition:} SSL models such as wav2vec \citep{schneider2019wav2vec} and wav2vec 2.0 \citep{baevski2020wav2vec} have significantly advanced speech recognition by learning representations from raw audio data. By predicting future audio samples from past ones, these models capture essential phonetic and linguistic features, improving transcription accuracy and the handling of diverse accents and intonations.

\textbf{Multimodal Learning:} SSL has facilitated the integration of multiple data modalities. Models like CLIP \citep{radford2021learning} learn joint representations from images and text, enabling tasks such as image captioning and text-based image retrieval, thereby bridging the gap between visual and textual data.

\textbf{Reinforcement Learning (RL):} In RL, SSL techniques have been applied to learn representations of states and actions without explicit reward signals, enhancing sample efficiency. By predicting future states or actions, these models improve planning and decision-making processes in complex environments \citep{schwarzer2021pretraining}.

These advancements are not merely incremental; they signify a fundamental shift in machine learning methodologies. By leveraging unlabeled data, SSL enables models to develop a form of common sense, understanding context and patterns previously inaccessible through supervised learning alone.

The potential applications of SSL are extensive. It opens pathways to developing AI systems capable of understanding and generating human-like language and vision, and learning new tasks with minimal supervision. Self-supervised learning represents a significant step toward creating AI that learns and adapts more like humans, enhancing intelligence and adaptability in artificial systems.



\subsection{Mathematical Formulation of SSL}

Formally, let \( \mathcal{D} = \{ \mathbf{x}_i \}_{i=1}^N \subset \mathbb{R}^d \) be a dataset of unlabeled samples drawn from an unknown distribution \( P_{\mathbf{x}} \). The objective of self-supervised representation learning is to learn an encoder function \( f_{\theta} : \mathbb{R}^d \rightarrow \mathbb{R}^k \), where \( k \leq d \), that maps input data to a latent space, producing meaningful representations \( \mathbf{h}_i = f_{\theta}(\mathbf{x}_i) \).

The encoder \( f_{\theta} \) is parameterized by \( \theta \) and is trained by minimizing a self-supervised loss function \( \mathcal{L}(\theta; \mathcal{D}) \) designed to capture essential features of the data:

\[
\theta^* = \arg \min_{\theta} \mathbb{E}_{\mathbf{x}_i \sim P_{\mathbf{x}}} [\mathcal{L}(\mathbf{x}_i; \theta)].
\]

The design of \( \mathcal{L} \) depends on the specific self-supervised learning approach and aims to enforce properties such as invariance to data augmentations, ability to predict contextual information, or alignment with semantic structures.

\subsection{Generative vs. Discriminative SSL}


Self-supervised learning methods broadly fall into two categories:

\textbf{Generative Approaches}: Generative self-supervised learning methods aim to model the underlying data distribution by reconstructing input data, predicting missing components, or generating new samples. These methods extract meaningful representations by recovering the structure and dependencies in the data. Examples include:

\begin{itemize}
    \item \emph{Auto-Regressive (AR) Models}: These models factorize the joint probability of data into a product of conditionals and generate each element sequentially based on its context. Examples include:
    \begin{itemize}
        \item \emph{GPT and GPT-2} \citep{radford2019language}: Learn language representations by predicting the next token given preceding context, enabling robust text generation and understanding.
        \item \emph{PixelRNN and PixelCNN} \citep{oord2016pixel}: Model pixel dependencies in images for self-supervised representation learning, useful for tasks like image completion.
    \end{itemize}



\item \emph{Normalizing Flows}: These models transform a simple latent distribution into a complex target distribution using a sequence of invertible mappings, preserving exact likelihoods. Examples include:
\begin{itemize}
    \item \emph{FlowGMM} \citep{izmailov2021flowgmm}: Combines normalizing flows with a Gaussian mixture model in the latent space to extract representations useful for tasks such as text classification, tabular data analysis, and semi-supervised image classification.
\end{itemize}

    
    \item \emph{Auto-Encoding (AE) Models}: Autoencoders reconstruct data from latent representations, learning robust embeddings for various downstream tasks. Examples include:
    \begin{itemize}
        \item \emph{Denoising Autoencoders (DAEs)} \citep{vincent2008extracting}: Learn robust representations by reconstructing corrupted inputs, foundational for models like \emph{BERT} \citep{devlin2019bert}, which predicts masked tokens in text.
        \item \emph{Variational Autoencoders (VAEs)} \citep{kingma2013auto}: Introduce probabilistic latent spaces, learning smooth representations useful for clustering and semi-supervised learning.
        \item \emph{VQ-VAE} \citep{oord2017neural}: Extends VAEs to discrete latent spaces, providing meaningful representations that have been widely applied in downstream tasks:
        \begin{itemize}
            \begin{itemize}
            \item In \textit{audio processing}, VQ-VAE representations have been used in \textit{wav2vec} \citep{baevski2020wav2vec} for unsupervised pretraining on speech data, enabling improvements in downstream speech recognition tasks by learning phonetic and linguistic features from raw audio.
            \item In \textit{vision}, the latent codes are inputs to autoregressive models for image generation, inpainting, and enhancement tasks \citep{razavi2019generating}.
            \item In \textit{multimodal learning}, VQ-VAE representations bridge modalities such as text and images, aiding in tasks like image captioning and cross-modal retrieval \citep{ding2021vqvae}.
        \end{itemize}
        \end{itemize}
        \item \emph{Masked Autoencoders}: These models learn representations by reconstructing masked portions of the input data, forcing the model to capture the context and structure of the data. This approach has been particularly impactful in self-supervised learning for computer vision. Key methods include \textbf{Context Encoders} \citep{pathak2016context}, \textbf{BEiT} \citep{bao2021beit}, \textbf{Masked Autoencoders (MAE)} \citep{he2022masked}, and \textbf{SimMIM} \citep{xie2022simmim}. %More recent models like \textbf{iBOT} \citep{zhou2022ibot} and \textbf{DINOv2} \citep{oquab2023dinov2} combine elements of both generative and discriminative approaches by using masked image modeling for context reconstruction while employing self-distillation to align the learned latent representations with teacher networks. This hybrid strategy leverages the strengths of generative modeling for capturing data structure and discriminative methods for robust feature learning, achieving superior performance across a wide range of vision tasks.


    \end{itemize}


\textbf{Discriminative Approaches}: These methods focus on learning robust feature representations by solving pretext tasks or leveraging contrastive objectives without explicitly modeling the data distribution. The main motivation is to learn representations that are useful for downstream tasks by capturing essential features and invariances in the data. Key techniques include:

    \begin{itemize}
\item \textbf{Contrastive Learning}:

\emph{Motivation and Logic}: Contrastive learning aims to learn robust feature representations by maximizing agreement between similar pairs (positive pairs) and minimizing agreement between dissimilar pairs (negative pairs). The central idea is to structure the latent space such that representations of similar samples are closer, while representations of different samples are farther apart.

\emph{Generalized Contrastive Loss}: \cite{tian2022} unified contrastive losses under a general framework, where the loss is expressed as:

\[
\mathcal{L}_{\phi,\psi}(\theta) = \sum_{i=1}^{N} \phi \left( \sum_{j \neq i} \psi\left(\|z_i - z_j\|_2^2 - \|z_i - z_i'\|_2^2\right) \right),
\]

where:
\begin{itemize}
    \item \(z_i\) and \(z_i'\) are the representations of a sample \(i\) and its positive pair (augmented view of the same sample).
    \item \(\|z_i - z_j\|_2^2\) is the squared distance between the representations of sample \(i\) and a negative sample \(j\).
    \item \(\phi\) and \(\psi\) are monotonically increasing and differentiable scalar functions that control how positive and negative distances are weighted.
\end{itemize}

Specific choices of \(\phi\) and \(\psi\) lead to well-known contrastive learning methods:
\begin{itemize}
    \item \textbf{InfoNCE Loss} \citep{oord2018representation}: Setting \(\phi(x) = \tau \log(\epsilon + x)\) and \(\psi(x) = \exp(x / \tau)\) results in:
    \[
    \mathcal{L}_{\text{InfoNCE}} = -\tau \sum_{i=1}^{N} \log \frac{\exp\left(-\|z_i - z_i'\|_2^2 / \tau\right)}{\epsilon \exp\left(-\|z_i - z_i'\|_2^2 / \tau\right) + \sum_{j \neq i} \exp\left(-\|z_i - z_j\|_2^2 / \tau\right)}.
    \]

    \item \textbf{SimCLR Loss} \citep{chen2020simple}: Simplifies InfoNCE by setting \(\epsilon = 0\), removing the stabilization term in the denominator.
\end{itemize}


        \item \textbf{Self-Distillation}:

        \emph{Motivation and Logic}: These methods aim to learn representations without the need for negative samples, addressing issues like the need for large batch sizes or memory banks in contrastive learning. The model learns by predicting its own representations under different augmentations, promoting consistency.

        \emph{Training Objective}: The model minimizes a prediction loss between representations of different augmented views. For instance, in BYOL, the loss is:

        \[
        \mathcal{L}_{\text{BYOL}} = \left\| \mathbf{q}_i - \text{stopgrad}(\mathbf{z}_i^+) \right\|_2^2,
        \]

        where:
        \begin{itemize}
            \item \( \mathbf{q}_i = f_{\theta}(\mathbf{x}_i) \) is the output of the online network.
            \item \( \mathbf{z}_i^+ = f_{\theta'}(\mathbf{x}_i^+) \) is the output of the target network.
            \item \( \text{stopgrad} \) indicates that gradients are not backpropagated through this path.
            \item \( \theta' \) is an exponential moving average of the parameters \( \theta \).
        \end{itemize}

        \emph{Examples}:
        \begin{itemize}
            \item \emph{BYOL} \citep{grill2020bootstrap}: Uses an online and a target network to predict one view from another, with the target network updated via an exponential moving average.
            \item \emph{SimSiam} \citep{chen2021exploring}: Simplifies BYOL by removing the momentum encoder and employing a stop-gradient operation to prevent representational collapse.
        \end{itemize}

        \item \textbf{Feature Decorrelation Methods}:

\emph{Motivation and Logic}: Feature decorrelation methods aim to learn rich and diverse representations by reducing redundancy among feature dimensions. Inspired by Canonical Correlation Analysis (CCA), these approaches encourage each component of the embedding to capture unique information, promoting uncorrelated and informative features. By focusing on decorrelating feature dimensions, these methods prevent representational collapse and enhance the quality of learned representations without relying on negative samples or complex training schemes.

\emph{Training Objective}: The central idea is to make embeddings from different augmented views of the same data similar (invariance) while ensuring that different feature dimensions are uncorrelated (decorrelation). This is achieved by combining an invariance term with a decorrelation term in the loss function, encouraging both similarity across views and diversity among features.

\emph{Examples}:
\begin{itemize}
    \item \emph{Barlow Twins} \citep{zbontar2021barlow} employs two identical networks to process different augmented views of the same image. It computes a cross-correlation matrix \( \mathbf{C} \) between the embeddings from the two networks. The loss function encourages the diagonal elements of \( \mathbf{C} \) to be close to one (promoting invariance) and the off-diagonal elements to be close to zero (reducing redundancy):

    \[
    \mathcal{L}_{\text{BT}} = \sum_{i=1}^d (1 - \mathbf{C}_{ii})^2 + \lambda \sum_{i \neq j} \mathbf{C}_{ij}^2.
    \]

    This approach enables the learning of features that are invariant to augmentations while ensuring each dimension captures distinct information.

    \item \emph{VICReg} \citep{bardes2021vicreg} introduces a loss function with three components. The invariance term minimizes the mean squared error between embeddings from different views. The variance term ensures the standard deviation of each feature dimension exceeds a threshold to prevent collapse. The covariance term penalizes off-diagonal covariance elements to reduce redundancy. The loss is expressed as:

    \[
    \mathcal{L}_{\text{VICReg}} = \underbrace{\frac{1}{B} \sum_{b=1}^B \| \mathbf{z}_b^{(1)} - \mathbf{z}_b^{(2)} \|_2^2}_{\text{Invariance}} + \gamma \underbrace{\sum_{j=1}^d \max(0, s - \sigma(\mathbf{z})_j)}_{\text{Variance}} + \mu \underbrace{\sum_{i \neq j} \text{Cov}(\mathbf{z})_{ij}^2}_{\text{Covariance}},
    \]

    where \( \gamma \) and \( \mu \) balance the terms, \( s \) is the variance threshold, and \( \text{Cov}(\mathbf{z})_{ij} \) is the off-diagonal element of the covariance matrix of \( \mathbf{z} \).
\end{itemize}

        \item \textbf{Clustering-Based Methods}:

        \emph{Motivation and Logic}: These approaches leverage unsupervised clustering to group similar data points, learning representations that capture group-level semantic structures. The model alternates between clustering the data in the representation space and updating the encoder to produce features that align with these clusters.

        \emph{Training Objective}: The encoder is trained to predict pseudo-labels derived from cluster assignments. The loss function typically involves cross-entropy between the predicted labels and the pseudo-labels:

        \[
        \mathcal{L}_{\text{cluster}} = \sum_{i=1}^N \ell_{\text{CE}}(f_{\theta}(\mathbf{x}_i), y_i),
        \]

        where \( y_i \) is the cluster assignment for \( \mathbf{x}_i \) obtained via a clustering algorithm (e.g., k-means), and \( \ell_{\text{CE}} \) denotes the cross-entropy loss.

        \emph{Examples}:
        \begin{itemize}
            \item \emph{DeepCluster} \citep{caron2018deep}: Performs k-means clustering on the learned features and uses the cluster assignments as pseudo-labels for training.
            \item \emph{SwAV} \citep{caron2020unsupervised}: SwAV uses siamese networks to generate embeddings for two augmented views of an image and aligns them with trainable prototypes. It computes cluster assignments online using the Sinkhorn-Knopp algorithm and minimizes cross-entropy between these assignments and predicted probabilities, enabling efficient training without pairwise comparisons.
        \end{itemize}
    \end{itemize}





\subsection{Utilizing Learned Representations in Downstream Tasks}

Once the representation function \( f_{\theta} \) is learned, the representations \( \mathbf{h}_i = f_{\theta}(\mathbf{x}_i) \) can be utilized in various downstream tasks. These representations capture essential features and structures in the data, making them valuable inputs for different applications.

\subsubsection{Classification}

In classification problems, the goal is to assign input data to one of several predefined categories. The learned representations \( \mathbf{h}_i \) serve as informative features that can be used to train a classifier. Specifically, we can define a classifier \( g_{\phi} : \mathbb{R}^k \rightarrow \mathbb{R}^C \), where \( C \) is the number of classes, and \( \phi \) represents the parameters of the classifier (e.g., weights of a fully connected layer).

The classifier is trained on a labeled dataset \( \{ (\mathbf{x}_j, y_j) \}_{j=1}^M \), where \( y_j \in \{1, 2, \dots, C\} \) is the class label for sample \( \mathbf{x}_j \). The training involves minimizing a supervised loss function, typically the cross-entropy loss:

\[
\phi^* = \arg \min_{\phi} \frac{1}{M} \sum_{j=1}^M \ell_{\text{CE}}(g_{\phi}(\mathbf{h}_j), y_j),
\]

where \( \mathbf{h}_j = f_{\theta}(\mathbf{x}_j) \) and \( \ell_{\text{CE}} \) denotes the cross-entropy loss function.

The representations \( \mathbf{h}_i \) often capture high-level semantic features and are invariant to variations in the input data that are irrelevant to the classification task, such as changes in lighting, orientation, or background. This invariance leads to better generalization and robustness, improving classification performance even when labeled data is limited.

In practice, the representations can be utilized in two ways:

\begin{itemize}
    \item \textbf{Fixed Feature Extraction}: The encoder \( f_{\theta} \) is kept frozen, and only the classifier \( g_{\phi} \) is trained on the labeled data. This approach is useful when computational resources are limited or when the labeled dataset is small.
    \item \textbf{Fine-Tuning}: Both the encoder \( f_{\theta} \) and the classifier \( g_{\phi} \) are trained (or fine-tuned) jointly on the labeled data. This allows the model to adapt the learned representations to the specific nuances of the downstream task, often leading to better performance.
\end{itemize}

\subsubsection{Other Downstream Tasks}

Similar logic can be applied to other downstream tasks such as object detection, semantic segmentation, instance segmentation, image captioning, and more. In these tasks, the extracted representations \( \mathbf{h}_i \) serve as inputs to task-specific prediction heads. For example, in object detection, the representations can be used to extract feature maps that help in localizing and classifying objects within images. In semantic segmentation, the representations provide spatial and contextual information necessary for assigning class labels to each pixel in an image. For image captioning, the representations capture visual features that can be input to sequence models to generate descriptive captions for images.

In all these tasks, the pretrained encoder \( f_{\theta} \) acts as a feature extractor, and the task-specific prediction heads are trained to perform the desired task using the representations as input. Similar to classification, the encoder \( f_{\theta} \) can be kept fixed or fine-tuned along with the prediction heads. This flexibility allows the model to either leverage the general representations learned during self-supervised pretraining or adapt them to the specific nuances of the downstream task. This approach leverages the strengths of self-supervised learning to enhance performance across various domains and applications.


\subsection{Challenges and Weaknesses in Current Methodologies}

Despite significant advancements, current unsupervised representation learning methods face several challenges:

\begin{itemize}
    \item \textbf{Limited Model Expressiveness}: Generative models like standard VAEs struggle to capture complex data distributions due to limited decoder capacity and restrictive assumptions (e.g., Gaussianity). This limits their applicability to high-dimensional and complex data.

    \item \textbf{Intrinsic Dimension Estimation}: Many methods lack mechanisms to estimate the intrinsic dimensionality of the data manifold, which is crucial for understanding the true complexity and structure of the data.

    \item \textbf{Geometric Understanding}: Existing methods often fail to exploit the geometry of the data manifold effectively. Without leveraging geometric insights, models may learn suboptimal representations that do not reflect the true relationships in the data.

\end{itemize}

\subsection{Addressing Weaknesses Through Our Work}

In this thesis, we propose novel approaches to address these challenges in self-supervised representation learning:

\paragraph{ScoreVAE}

We introduce \emph{ScoreVAE}, a model that integrates variational inference with pre-trained diffusion models to overcome the limitations of traditional VAEs. By relaxing the Gaussian assumption in the reconstruction distribution and incorporating more expressive score-based models, ScoreVAE achieves higher model expressiveness, enabling it to capture complex data distributions and produce clearer reconstructions.

\paragraph{Intrinsic Dimension Estimation Using Diffusion Models}

We propose a novel framework that leverages diffusion models to estimate the intrinsic dimensionality of data. By analyzing the diffusion process and its interaction with the data manifold, our method captures the latent structure and dimensionality, offering a principled way to understand complex datasets.

\paragraph{Score-Based Pullback Riemannian Geometry}

Our work introduces a method to learn the geometry of data manifolds by adapting a score-based Riemannian metric. This approach enables dimensionality reduction and geometric understanding while preserving local manifold properties. By leveraging the pullback metric induced by the score function, we address intrinsic dimension estimation and effectively exploit the underlying geometry in representation learning.

These contributions aim to enhance the expressiveness of generative models, provide tools for intrinsic dimension estimation, and integrate geometric insights into self-supervised learning, thereby advancing the field toward more robust and intelligent systems.
