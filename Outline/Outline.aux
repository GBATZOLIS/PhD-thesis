\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\citation{Dual-Glow}
\citation{Dual-Glow}
\citation{batzolis2024caflow}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Thesis Outline and Contributions}{37}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}CAFLOW: Conditional Autoregressive Flows}{37}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces From left to right: ideal dependencies in the $i^{th}$ autoregressive component. Dual-Glow modeling assumption \cite  {Dual-Glow}; information is exchanged only between latent spaces having the same dimension. Our modeling assumption; we retain the dependencies between $L_i$ and the latent spaces of lower dimension.}}{37}{figure.caption.24}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dependencies}{{2.1}{37}{From left to right: ideal dependencies in the $i^{th}$ autoregressive component. Dual-Glow modeling assumption \cite {Dual-Glow}; information is exchanged only between latent spaces having the same dimension. Our modeling assumption; we retain the dependencies between $L_i$ and the latent spaces of lower dimension}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Left: unconditional normalizing flow architecture used to encode conditioning and conditioned images, denoted by $Y_n = Y$ and $W_n = W$ respectively, into a sequence of hierarchical latent variables. Right: design of the conditional transformation $G_{i}^\theta $ that models the $i^{th}$ autoregressive component. The index of the flow $i$ is omitted in both the transformed latent variable $Z_j$ and the intermediate latent variables $Z_j^{\prime }$ for simplicity.}}{38}{figure.caption.25}\protected@file@percent }
\newlabel{fig:high_level_design_conditional}{{2.2}{38}{Left: unconditional normalizing flow architecture used to encode conditioning and conditioned images, denoted by $Y_n = Y$ and $W_n = W$ respectively, into a sequence of hierarchical latent variables. Right: design of the conditional transformation $G_{i}^\theta $ that models the $i^{th}$ autoregressive component. The index of the flow $i$ is omitted in both the transformed latent variable $Z_j$ and the intermediate latent variables $Z_j^{\prime }$ for simplicity}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Non-Uniform Diffusion Models}{39}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Illustration of a multi-scale diffusion model with three scales. Inspired by multi-scale normalizing flows, this approach diffuses different parts of the image tensor (transformed into multi-level Haar coefficients) at varying speeds. High-frequency detail coefficients diffuse progressively faster, with $d_1$ diffusing faster than $d_2$, $d_2$ faster than $d_3$, and so on, ensuring that all coefficients reach the same (very low) signal-to-noise ratio (SNR) at their respective terminal diffusion times: $t = 0.25, 0.5, 0.75$, and $1.0$, respectively. The low-frequency approximation coefficients $a_3$ diffuse the slowest, completing their diffusion at $t = 1.0$. The multi-scale structure reduces the dimensionality of the diffusing tensor at each scale, enabling faster computation. Separate neural networks $S_1, S_2, S_3, S_4$ approximate the score functions at different intervals, leveraging the reduced dimensionality of the intermediate distributions. This hierarchical design mirrors the structure of multi-scale normalizing flows, improving training and sampling efficiency while maintaining high image generation quality.}}{39}{figure.caption.27}\protected@file@percent }
\newlabel{fig:Multiscale model}{{2.3}{39}{Illustration of a multi-scale diffusion model with three scales. Inspired by multi-scale normalizing flows, this approach diffuses different parts of the image tensor (transformed into multi-level Haar coefficients) at varying speeds. High-frequency detail coefficients diffuse progressively faster, with $d_1$ diffusing faster than $d_2$, $d_2$ faster than $d_3$, and so on, ensuring that all coefficients reach the same (very low) signal-to-noise ratio (SNR) at their respective terminal diffusion times: $t = 0.25, 0.5, 0.75$, and $1.0$, respectively. The low-frequency approximation coefficients $a_3$ diffuse the slowest, completing their diffusion at $t = 1.0$. The multi-scale structure reduces the dimensionality of the diffusing tensor at each scale, enabling faster computation. Separate neural networks $S_1, S_2, S_3, S_4$ approximate the score functions at different intervals, leveraging the reduced dimensionality of the intermediate distributions. This hierarchical design mirrors the structure of multi-scale normalizing flows, improving training and sampling efficiency while maintaining high image generation quality}{figure.caption.27}{}}
\citation{batzolis2022non_uniform}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Results from our conditional multi-speed diffusive estimator.}}{40}{figure.caption.28}\protected@file@percent }
\newlabel{fig: teaser}{{2.4}{40}{Results from our conditional multi-speed diffusive estimator}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Diffusion Models Encode the Intrinsic Dimension of Data Manifolds}{41}{section.2.3}\protected@file@percent }
\newlabel{fig:score_field}{{2.5a}{41}{The data manifold (in blue) and the neural approximation of the score field $\nabla _\textbf {x} \ln p_{t_0}(\textbf {x})$ obtained from a diffusion model. Near the manifold, the score field is perpendicular to the manifold surface}{figure.caption.30}{}}
\newlabel{sub@fig:score_field}{{a}{41}{The data manifold (in blue) and the neural approximation of the score field $\nabla _\textbf {x} \ln p_{t_0}(\textbf {x})$ obtained from a diffusion model. Near the manifold, the score field is perpendicular to the manifold surface}{figure.caption.30}{}}
\newlabel{fig:zoom}{{2.5b}{41}{The red dot shows a point $\textbf {x}_0$ on the data manifold where we wish to estimate the dimension. We sample $K$ blue points $\textbf {x}_t^{(i)}$ in a close neighborhood of the red point and evaluate the score field. The resulting vectors $s_\theta (\textbf {x}_\epsilon ^{(i)}, \epsilon )$ point in the normal direction. We put the vectors into a matrix and perform SVD to detect the dimension of the normal space. The dimension of the manifold equals the number of (almost) vanishing singular values}{figure.caption.30}{}}
\newlabel{sub@fig:zoom}{{b}{41}{The red dot shows a point $\textbf {x}_0$ on the data manifold where we wish to estimate the dimension. We sample $K$ blue points $\textbf {x}_t^{(i)}$ in a close neighborhood of the red point and evaluate the score field. The resulting vectors $s_\theta (\textbf {x}_\epsilon ^{(i)}, \epsilon )$ point in the normal direction. We put the vectors into a matrix and perform SVD to detect the dimension of the normal space. The dimension of the manifold equals the number of (almost) vanishing singular values}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces (Left) Visualization of the score field near the data manifold. (Right) Visualisation of the estimation of the manifold dimension using the trained diffusion model.}}{41}{figure.caption.30}\protected@file@percent }
\newlabel{fig:score_estimation}{{2.5}{41}{(Left) Visualization of the score field near the data manifold. (Right) Visualisation of the estimation of the manifold dimension using the trained diffusion model}{figure.caption.30}{}}
\citation{pmlr-v235-stanczuk24a}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Variational Diffusion Auto-encoder: Latent Space Extraction from Pre-Trained Diffusion Models}{43}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Comparison of original and reconstructed images on the FFHQ dataset using our ScoreVAE framework. The left panel presents the original images from the FFHQ dataset, while the right panel displays the corresponding reconstructions generated by ScoreVAE. The results highlight the effectiveness of ScoreVAE in capturing intricate details and preserving high fidelity, overcoming the limitations of traditional VAE models.}}{43}{figure.caption.32}\protected@file@percent }
\newlabel{fig:ffhq}{{2.6}{43}{Comparison of original and reconstructed images on the FFHQ dataset using our ScoreVAE framework. The left panel presents the original images from the FFHQ dataset, while the right panel displays the corresponding reconstructions generated by ScoreVAE. The results highlight the effectiveness of ScoreVAE in capturing intricate details and preserving high fidelity, overcoming the limitations of traditional VAE models}{figure.caption.32}{}}
\citation{batzolis2023variational}
\citation{song2021maximum}
\citation{diepeveen2024score}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Score-Based Pullback Riemannian Geometry}{44}{section.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Approximate data manifolds learned by the Riemannian autoencoder generated by score-based pullback Riemannian geometry for three datasets. The orange surfaces represent the manifolds learned by the model, while the blue points correspond to the training data. Each manifold provides a convincing low-dimensional representation of the data, isometric to its respective latent space. }}{45}{figure.caption.34}\protected@file@percent }
\newlabel{fig:learned_charts}{{2.7}{45}{Approximate data manifolds learned by the Riemannian autoencoder generated by score-based pullback Riemannian geometry for three datasets. The orange surfaces represent the manifolds learned by the model, while the blue points correspond to the training data. Each manifold provides a convincing low-dimensional representation of the data, isometric to its respective latent space}{figure.caption.34}{}}
\@setckpt{Outline/Outline}{
\setcounter{page}{47}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{7}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{6}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{25}
\setcounter{theorem}{0}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{caption@flags}{6}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{3}
\setcounter{subtable}{0}
}
